{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Ordeq!","text":"<p>Ordeq is a framework for developing data pipelines. It simplifies IO and modularizes pipeline logic. Ordeq elevates your proof-of-concept to a production-grade pipelines. See the introduction for an easy-to-follow example of how Ordeq can help.</p>"},{"location":"#installation","title":"Installation","text":"<p>Ordeq is available under MIT license. Please refer to the license and notice for more details.</p> <p>To install Ordeq, run:</p> <pre><code>uv pip install ordeq\n</code></pre> <p>Ordeq supports various IO packages for reading and writing data. You can install them as needed. For example, for reading and writing CSV files with Pandas, install the <code>ordeq-pandas</code> package:</p> <pre><code>uv pip install ordeq-pandas\n</code></pre> <p>Have a look at the API reference for a list of available packages and extensions.</p>"},{"location":"#why-consider-ordeq","title":"Why consider Ordeq?","text":"<ul> <li>Ordeq is the GenAI companion: it gives your project structure and consistency, such that GenAI can thrive</li> <li>It offers seamless integrations with existing data &amp; ML tooling, such as Spark, Pandas, Pydantic and PyMuPDF, and     adding new integrations is trivial</li> <li>It's actively developed and trusted by data scientists, engineers, analysts and machine learning engineers at     ING</li> </ul>"},{"location":"#learning-ordeq","title":"Learning Ordeq","text":"<p>To learn more about Ordeq, check out the following resources:</p> <ul> <li>See how Ordeq can help your project in the introduction</li> <li>Check out the core concepts to learn how to use Ordeq</li> </ul>"},{"location":"CONTRIBUTING/","title":"Contributing","text":"<p>Thanks for considering to contribute to Ordeq! All contributions are welcome, whether it's reporting issues, suggesting features, or submitting code changes. This should get you started:</p> <ul> <li>Have a look at the open issues</li> <li>Have a look at our guidelines below</li> <li>Set up your local environment as instructed below</li> </ul>"},{"location":"CONTRIBUTING/#guidelines","title":"Guidelines","text":"<p>Feel free to challenge the below contributing guidelines. We are young project and still figuring out how we can collaborate best.</p>"},{"location":"CONTRIBUTING/#development-testing","title":"Development &amp; testing","text":"<p>The <code>just</code> command runner tool is used for common tasks in the project. After installing it, you can run <code>just</code> to see the available commands:</p> <pre><code>Available recipes:\n    localsetup                # Local installation\n    ruff                      # Linting and formatting with ruff\n    mdformat                  # Formatting with mdformat\n    mdformat-fix              # Fix formatting with mdformat\n    doccmd-ruff-format        # Formatting with ruff via doccmd\n    doccmd-ruff-lint          # Linting with ruff via doccmd\n    doccmd-fix                # Combine doccmd with ruff for linting and formatting\n    lint                      # Linting with ruff\n    lint-fix                  # Fix linting issues with ruff\n    format                    # Formatting with ruff\n    format-fix                # Fix formatting with ruff\n    ty                        # Type checking with ty\n    list                      # List all packages\n    mypy                      # Type checking with mypy\n    mypy-packages             # Mypy check all package directories\n    mypy-examples             # Mypy check all example directories\n    sa                        # Static analysis (lint + type checking)\n    fix                       # Format code and apply lint fixes with ruff and mdformat\n    test *PACKAGES            # or `just test ordeq ordeq-cli-runner` (Run tests in the 'ordeq' and 'ordeq-cli-runner' packages)\n    test_package PACKAGE      # Test a single package\n    test_all                  # Run tests for all packages with coverage\n    generate-api-docs         # Generate API documentation pages\n    generate-package-overview # Generate package overview documentation page\n    docs-build                # Build the documentation\n    docs-serve                # Build and serve the documentation locally\n    docs-publish              # Publish the documentation to GitHub Pages\n    precommit                 # Run pre-commit hooks\n    precommit_install         # Install pre-commit hooks\n    install                   # Install development dependencies\n    upgrade                   # Upgrade (pre-commit only)\n    build PACKAGE             # Build a package\n    publish PACKAGE           # You need an API token from PyPI to run this command.\n    lock                      # Lock dependencies\n    bump *ARGS                # Bump version\n    delete-snapshots          # Delete all .snapshot.md files anywhere in the repository\n    capture-snapshots         # Recompute snapshots by running only those tests for all packages\n</code></pre> <p>Tip: install support for <code>just</code> in your IDE, e.g. just for PyCharm.</p> <p>Install Ordeq locally in editable mode:</p> <pre><code>just localsetup\n</code></pre> <p>(In case of any issues, check out the troubleshooting section below)</p> <p>Install the pre-commit hooks:</p> <pre><code>uv run pre-commit install\n</code></pre> <ul> <li>When you start on a work item, create a new branch.</li> <li>The CI pipeline will be triggered when you create a pull request.</li> <li>Pull requests should merge your branch into <code>main</code>.</li> <li>You are encouraged to open and share draft PRs for work that is pending.</li> <li>The merge type must be squash commit.</li> <li>The pull request title and labels will be used to generate the release notes.</li> <li>There is a policy check on the PR which ensures that, before merge:<ul> <li>the build has succeeded (formatting, linters &amp; tests pass)</li> <li>open comments are resolved</li> <li>at least one person besides the author has approved</li> </ul> </li> </ul>"},{"location":"CONTRIBUTING/#releases","title":"Releases","text":"<ul> <li>We use semantic versioning for the release tags.</li> <li>Releases should be done for each package individually, e.g. <code>ordeq</code>,<code>ordeq-spark</code></li> <li>Releases are managed via GitHub releases.</li> <li>To create releases:<ul> <li>Ensure you are on the <code>main</code> branch and have pulled the latest changes.</li> <li>Run the release script <code>just generate-draft-releases</code>.</li> <li>This will create draft releases for all packages that have changes since the last release.</li> <li>Go to the \"Releases\" section of the GitHub repository.</li> <li>Find the draft release for the package you want to publish.</li> <li>Review the release notes and make any necessary edits.</li> <li>Untick the \"Set as the latest release\" checkbox if you are not releasing the <code>ordeq</code> package.</li> <li>Click \"Publish release\"</li> <li>The CI will automatically build the package and upload it to Pypi.</li> </ul> </li> </ul>"},{"location":"CONTRIBUTING/#publishing-to-pypi-for-the-first-time","title":"Publishing to PyPi for the first time","text":"<p>GitHub Actions cannot publish a new package to PyPi until GitHub is added as a Trusted Publisher for the project. To enable automated publishing, you must first configure the Trusted Publisher settings:</p> <ul> <li>Add the new package as pending trusted publisher:<ul> <li>Go to https://pypi.org/manage/account/publishing/</li> <li>Click \"Add a new trusted publisher\"</li> <li>Enter the package name (e.g. <code>ordeq_spark</code>) as PyPi project name</li> <li>Owner/Organization: <code>ing-bank</code></li> <li>Repository: <code>ordeq</code></li> <li>Workflow: <code>release.yml</code></li> <li>Environment: <code>pypi</code></li> </ul> </li> <li>After completing these steps, future tags pushed to the repository will trigger automated publishing via GitHub Actions.</li> </ul>"},{"location":"CONTRIBUTING/#troubleshooting","title":"Troubleshooting","text":""},{"location":"CONTRIBUTING/#locked-dependencies","title":"Locked dependencies","text":"<p>If you get an error saying: <code>error: Failed to parse 'uv.lock'</code> or <code>The lockfile at 'uv.lock' needs to be updated</code>, this usually indicates that the dependencies were altered in the <code>pyproject.toml</code> or <code>uv.lock</code>.</p> <ul> <li>Please check if you have accidentally altered <code>pyproject.toml</code> or <code>uv.lock</code></li> <li>Use <code>uv add</code> instead of (<code>uv</code>) <code>pip install</code>. More info here.</li> </ul>"},{"location":"CONTRIBUTING/#non-pip-dependencies","title":"Non-pip dependencies","text":"<p>If you receive the following error installing <code>pymssql</code> on Mac, you need to install FreeTDS to get the required C-headers: <code>brew install freetds</code>.</p> <pre><code>  \u00d7 Failed to build `pymssql==2.3.7`\n  \u251c\u2500\u25b6 The build backend returned an error\n  \u2570\u2500\u25b6 Call to `setuptools.build_meta:__legacy__.build_wheel` failed (exit status: 1)\n</code></pre>"},{"location":"CONTRIBUTING/#docker-backed-tests","title":"Docker-backed Tests","text":"<p>Some of the unit tests rely on Docker via the <code>testcontainers</code> PyPI package. If you're using Docker Desktop on macOS, these tests will fail in the default configuration:</p> <pre><code>ERROR tests/.../test_xxx.py::TestFile::test_function - docker.errors.DockerException: Error while fetching server API version: ('Connection aborted.', FileNotFoundError(2, 'No such file or directory'))\n</code></pre> <p>This can be remedied by changing the configuration of Docker Desktop for macOS:</p> <ol> <li>Open Docker Desktop, go to Settings \u21d2 Advanced</li> <li>Enable \"Docker CLI Tools System \u21d2 (requires password)\"</li> <li>Enable \"Allow default Docker socket (requires password)\"</li> <li>Click \"Apply &amp; Restart\"</li> </ol>"},{"location":"CONTRIBUTING/#spark-java","title":"Spark &amp; Java","text":"<p>The unit tests for <code>ordeq-spark</code> run Spark on your host system. This means that Java must be installed on your laptop, and your default Java VM must not be newer than JDK 17, because newer versions remove some deprecated functions that Spark still relies on:</p> <pre><code>E                   py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\nE                   : java.lang.ExceptionInInitializerError\nE                       at org.apache.spark.unsafe.array.ByteArrayMethods.&lt;clinit&gt;(ByteArrayMethods.java:56)\n                        ...\nE                   Caused by: java.lang.NoSuchMethodException: java.nio.DirectByteBuffer.&lt;init&gt;(long,int)\n                        ...\n</code></pre> <p>If you use SdkMan! to manage your Java installations:</p> <pre><code>sdk list java | fgrep 17 | fgrep tem\nsdk install java 17.0.12-tem   # replace 12 by whatever is current\nsdk default java 17.0.12-tem\n</code></pre> <p>If you use another tool to manage your JDKs, run the equivalent tasks to make sure your <code>JAVA_HOME</code> is set correctly.</p>"},{"location":"packages/","title":"Package overview","text":"<p>This page lists all public packages in the <code>ordeq</code> project.</p>"},{"location":"packages/#framework-packages","title":"Framework packages","text":"<p>Libraries that provide framework functionality for Ordeq.</p> Name Description API Docs Ordeq is a framework that simplifies IO and modularizes pipeline logic API Docs Manifests for Ordeq projects API Docs Visualization for Ordeq project API Docs"},{"location":"packages/#command-line-interfaces","title":"Command line interfaces","text":"<p>Command line tools for interacting with Ordeq.</p> Name Description API Docs Command line interface for the Ordeq runner API Docs Command line interface for Ordeq visualization API Docs"},{"location":"packages/#io-packages","title":"IO packages","text":"<p>Packages that provide implementations of inputs and outputs.</p> Name Description API Docs Altair integration for Ordeq API Docs Argparse and environment variables integration for Ordeq API Docs Boto3 integration for Ordeq API Docs Frequently used IOs and Hooks for Ordeq projects API Docs DuckDB integration for Ordeq API Docs Faiss integration for Ordeq API Docs A collection of file IOs for Ordeq (e.g. Text, CSV, JSON, Pickle) API Docs HuggingFace datasets integration for Ordeq API Docs Ibis integration for Ordeq API Docs Joblib integration for Ordeq API Docs Matplotlib integration for Ordeq API Docs NetworkX integration for Ordeq API Docs NumPy integration for Ordeq API Docs Pandas integration for Ordeq API Docs Polars integration for Ordeq API Docs Pydantic integration for Ordeq API Docs PyMuPDF (PDF) integration for Ordeq API Docs Pyproject.toml IO for Ordeq API Docs Requests integration for Ordeq API Docs Sentence Transformers integration for Ordeq API Docs Apache Spark integration for Ordeq API Docs TOML integration for Ordeq API Docs YAML integration for Ordeq API Docs"},{"location":"api/ordeq/","title":"ordeq","text":""},{"location":"api/ordeq/_catalog/","title":"_catalog.py","text":""},{"location":"api/ordeq/_catalog/#ordeq._catalog.check_catalogs_are_consistent","title":"<code>check_catalogs_are_consistent(a, b, *others)</code>","text":"<p>Utility method to checks if two (or more) catalogs are consistent, i.e. if they define the same keys.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>ModuleType</code> <p>First catalog to compare.</p> required <code>b</code> <code>ModuleType</code> <p>Second catalog to compare.</p> required <code>*others</code> <code>ModuleType</code> <p>Additional catalogs to compare.</p> <code>()</code> <p>Raises:</p> Type Description <code>CatalogError</code> <p>If the catalogs are inconsistent, i.e. if they define different keys.</p>"},{"location":"api/ordeq/_fqn/","title":"_fqn.py","text":"<p>Object references to fully qualified names (FQNs) conversion utilities.</p> <p>Object references are represented as strings in the format \"module:name\", while fully qualified names (FQNs) are represented as tuples of the form (module, name).</p>"},{"location":"api/ordeq/_fqn/#ordeq._fqn.fqn_to_str","title":"<code>fqn_to_str(name)</code>","text":"<p>Convert a fully qualified name (FQN) to a string representation.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>FQN</code> <p>A tuple representing the fully qualified name (module, name).</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string in the format \"module:name\".</p>"},{"location":"api/ordeq/_fqn/#ordeq._fqn.str_to_fqn","title":"<code>str_to_fqn(name)</code>","text":"<p>Convert a string representation to a fully qualified name (FQN).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>A string in the format \"module:name\".</p> required <p>Returns:</p> Type Description <code>FQN</code> <p>A tuple representing the fully qualified name (module, name).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input string is not in the expected format.</p>"},{"location":"api/ordeq/_graph/","title":"_graph.py","text":""},{"location":"api/ordeq/_graph/#ordeq._graph.NodeGraph","title":"<code>NodeGraph</code>","text":""},{"location":"api/ordeq/_graph/#ordeq._graph.NodeGraph.nodes","title":"<code>nodes</code>  <code>property</code>","text":"<p>Returns the set of all nodes in this graph.</p> <p>Returns:</p> Type Description <code>set[Node]</code> <p>all nodes in this graph</p>"},{"location":"api/ordeq/_graph/#ordeq._graph.NodeGraph.sink_nodes","title":"<code>sink_nodes</code>  <code>property</code>","text":"<p>Finds the sink nodes, i.e., nodes without successors.</p> <p>Returns:</p> Type Description <code>set[Node]</code> <p>set of the sink nodes</p>"},{"location":"api/ordeq/_hook/","title":"_hook.py","text":""},{"location":"api/ordeq/_hook/#ordeq._hook.InputHook","title":"<code>InputHook</code>","text":"<p>               Bases: <code>Protocol[T]</code></p> <p>Hook used to inject custom logic that will be executed before and after an input is loaded.</p>"},{"location":"api/ordeq/_hook/#ordeq._hook.InputHook.after_input_load","title":"<code>after_input_load(io, data)</code>","text":"<p>Hook that is executed after an input is loaded.</p> <p>Parameters:</p> Name Type Description Default <code>io</code> <code>Input[T]</code> <p>the input object.</p> required <code>data</code> <code>T</code> <p>the loaded data.</p> required"},{"location":"api/ordeq/_hook/#ordeq._hook.InputHook.before_input_load","title":"<code>before_input_load(io)</code>","text":"<p>Hook that is executed before an input is loaded.</p> <p>Parameters:</p> Name Type Description Default <code>io</code> <code>Input[T]</code> <p>the input object.</p> required"},{"location":"api/ordeq/_hook/#ordeq._hook.NodeHook","title":"<code>NodeHook</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Hook used to inject custom logic that will be executed when a node is run.</p>"},{"location":"api/ordeq/_hook/#ordeq._hook.NodeHook.on_node_call_error","title":"<code>on_node_call_error(node, error)</code>","text":"<p>Triggered when an exception is raised when calling the node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>the node object</p> required <code>error</code> <code>Exception</code> <p>the error</p> required"},{"location":"api/ordeq/_hook/#ordeq._hook.OutputHook","title":"<code>OutputHook</code>","text":"<p>               Bases: <code>Protocol[T]</code></p> <p>Hook used to inject custom logic that will be executed before and after an output is saved.</p>"},{"location":"api/ordeq/_hook/#ordeq._hook.OutputHook.after_output_save","title":"<code>after_output_save(io, data)</code>","text":"<p>Hook that is executed after an output is saved.</p> <p>Parameters:</p> Name Type Description Default <code>io</code> <code>Output[T]</code> <p>the input object.</p> required <code>data</code> <code>T</code> <p>the data that has been saved.</p> required"},{"location":"api/ordeq/_hook/#ordeq._hook.OutputHook.before_output_save","title":"<code>before_output_save(io, data)</code>","text":"<p>Hook that is executed before an output is saved.</p> <p>Parameters:</p> Name Type Description Default <code>io</code> <code>Output[T]</code> <p>the input object.</p> required <code>data</code> <code>T</code> <p>the data to be saved.</p> required"},{"location":"api/ordeq/_hook/#ordeq._hook.RunHook","title":"<code>RunHook</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Hook used to inject custom logic that will be executed when a graph is run.</p>"},{"location":"api/ordeq/_hook/#ordeq._hook.RunHook.after_run","title":"<code>after_run(graph)</code>","text":"<p>Triggered after the graph is run.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>NodeGraph</code> <p>the graph object</p> required"},{"location":"api/ordeq/_hook/#ordeq._hook.RunHook.before_run","title":"<code>before_run(graph)</code>","text":"<p>Triggered before the graph is run.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>NodeGraph</code> <p>the graph object</p> required"},{"location":"api/ordeq/_io/","title":"_io.py","text":""},{"location":"api/ordeq/_io/#ordeq._io.IO","title":"<code>IO</code>","text":"<p>               Bases: <code>Input[T]</code>, <code>Output[T]</code></p> <p>Base class for all IOs in Ordeq. An <code>IO</code> is a class that can both load and save data. See the Ordeq IO packages for some out-of-the-box implementations (e.g., <code>YAML</code>, <code>StringBuffer</code>, etc.).</p> <p><code>IO</code> can also be used directly as placeholder. This can be useful when you want to pass data from one node to another, but you do not want to save the data in between:</p> <pre><code>&gt;&gt;&gt; from ordeq import Input, node\n&gt;&gt;&gt; from ordeq_common import StringBuffer, Literal\n\n&gt;&gt;&gt; hello = StringBuffer(\"hi\")\n&gt;&gt;&gt; name = Literal(\"Bob\")\n&gt;&gt;&gt; greeting = IO[str]()\n&gt;&gt;&gt; greeting_capitalized = StringBuffer()\n\n&gt;&gt;&gt; @node(\n...     inputs=[hello, name],\n...     outputs=greeting\n... )\n... def greet(greeting: str, name: str) -&gt; str:\n...     return f\"{greeting}, {name}!\"\n\n&gt;&gt;&gt; @node(\n...     inputs=greeting,\n...     outputs=greeting_capitalized\n... )\n... def capitalize(s: str) -&gt; str:\n...     return s.capitalize()\n</code></pre> <p>In the example above, <code>greeting</code> represents the placeholder output to the node <code>greet</code>, as well as the placeholder input to <code>capitalize</code>.</p> <p>When you run the nodes <code>greeting</code> and <code>capitalize</code> the result of <code>greeting</code> will be passed along unaffected to <code>capitalize</code>, much like a cache:</p> <pre><code>&gt;&gt;&gt; from ordeq import run\n&gt;&gt;&gt; result = run(greet, capitalize)\n&gt;&gt;&gt; result[greeting]\n'hi, Bob!'\n</code></pre>"},{"location":"api/ordeq/_io/#ordeq._io.IOException","title":"<code>IOException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised by IO implementations in case of load/save failure. IO implementations should provide instructive information.</p>"},{"location":"api/ordeq/_io/#ordeq._io.Input","title":"<code>Input</code>","text":"<p>               Bases: <code>_InputOptions[Tin]</code>, <code>_InputHooks[Tin]</code>, <code>_InputReferences[Tin]</code>, <code>_InputCache[Tin]</code>, <code>_InputException[Tin]</code>, <code>Generic[Tin]</code></p> <p>Base class for all inputs in Ordeq. An <code>Input</code> is a class that loads data. All <code>Input</code> classes should implement a load method. By default, loading an input raises a <code>NotImplementedError</code>. See the Ordeq IO packages for some out-of-the-box implementations (e.g., <code>Literal</code>, <code>StringBuffer</code>, etc.).</p> <p><code>Input</code> can also be used directly as placeholder. This can be useful when you are defining a node, but you do not want to provide an actual input yet. In this case, you can:</p> <pre><code>&gt;&gt;&gt; from ordeq import Input, node\n&gt;&gt;&gt; from ordeq_common import StringBuffer\n\n&gt;&gt;&gt; name = Input[str]()\n&gt;&gt;&gt; greeting = StringBuffer()\n\n&gt;&gt;&gt; @node(\n...     inputs=name,\n...     outputs=greeting\n... )\n... def greet(name: str) -&gt; str:\n...     return f\"Hello, {name}!\"\n</code></pre> <p>In the example above, <code>name</code> represents the placeholder input to the node <code>greet</code>. Running the node greet as-is will raise a <code>NotImplementedError</code>:</p> <pre><code>&gt;&gt;&gt; from ordeq import run\n&gt;&gt;&gt; run(greet) # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\nNotImplementedError:\n</code></pre> <p>To use the <code>greet</code> node, we need to provide an actual input. For instance:</p> <pre><code>&gt;&gt;&gt; from ordeq_common import Literal\n&gt;&gt;&gt; result = run(greet, io={name: Literal(\"Alice\")})\n&gt;&gt;&gt; result[greeting]\n'Hello, Alice!'\n</code></pre>"},{"location":"api/ordeq/_io/#ordeq._io.Output","title":"<code>Output</code>","text":"<p>               Bases: <code>_OutputOptions[Tout]</code>, <code>_OutputHooks[Tout]</code>, <code>_OutputReferences[Tout]</code>, <code>_OutputException[Tout]</code>, <code>Generic[Tout]</code></p> <p>Base class for all outputs in Ordeq. An <code>Output</code> is a class that saves data. All <code>Output</code> classes should implement a save method. By default, saving an output does nothing. See the Ordeq IO packages for some out-of-the-box implementations (e.g., <code>YAML</code>, <code>StringBuffer</code>, etc.).</p> <p><code>Output</code> can also be used directly as placeholder. This can be useful when you are defining a node, but you do not want to provide an actual output. In this case, you can:</p> <pre><code>&gt;&gt;&gt; from ordeq import Output, node\n&gt;&gt;&gt; from ordeq_common import StringBuffer\n\n&gt;&gt;&gt; greeting = StringBuffer(\"hello\")\n&gt;&gt;&gt; greeting_upper = Output[str]()\n\n&gt;&gt;&gt; @node(\n...     inputs=greeting,\n...     outputs=greeting_upper\n... )\n... def uppercase(greeting: str) -&gt; str:\n...     return greeting.upper()\n</code></pre> <p>In the example above, <code>greeting_upper</code> represents the placeholder output to the node <code>uppercase</code>. When you run the node <code>uppercase</code>, its result can be retrieved from the <code>greeting_upper</code> output. For instance:</p> <pre><code>&gt;&gt;&gt; from ordeq import run\n&gt;&gt;&gt; result = run(uppercase)\n&gt;&gt;&gt; result[greeting_upper]\n'HELLO'\n</code></pre>"},{"location":"api/ordeq/_nodes/","title":"_nodes.py","text":""},{"location":"api/ordeq/_nodes/#ordeq._nodes.Node","title":"<code>Node</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[FuncParams, FuncReturns]</code></p>"},{"location":"api/ordeq/_nodes/#ordeq._nodes.Node.views","title":"<code>views</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the views used as input to this node.</p>"},{"location":"api/ordeq/_nodes/#ordeq._nodes.Node.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Nodes always have to be hashable</p>"},{"location":"api/ordeq/_nodes/#ordeq._nodes.Node.validate","title":"<code>validate()</code>","text":"<p>These checks are performed before the node is run.</p>"},{"location":"api/ordeq/_nodes/#ordeq._nodes.View","title":"<code>View</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Node[FuncParams, FuncReturns]</code></p>"},{"location":"api/ordeq/_nodes/#ordeq._nodes.create_node","title":"<code>create_node(func, *, name=None, inputs=None, outputs=None, attributes=None)</code>","text":"<pre><code>create_node(\n    func: Callable[FuncParams, FuncReturns],\n    *,\n    name: str | None = None,\n    inputs: Sequence[Input | Callable]\n    | Input\n    | Callable\n    | None = None,\n    outputs: Sequence[Output] | Output | None = None,\n    attributes: dict[str, Any] | None = None,\n) -&gt; Node[FuncParams, FuncReturns]\n</code></pre><pre><code>create_node(\n    func: Callable[FuncParams, FuncReturns],\n    *,\n    name: str | None = None,\n    inputs: Sequence[Input | Callable]\n    | Input\n    | Callable\n    | None = None,\n    outputs: None = None,\n    attributes: dict[str, Any] | None = None,\n) -&gt; View[FuncParams, FuncReturns]\n</code></pre> <p>Creates a Node instance.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[FuncParams, FuncReturns]</code> <p>The function to be executed by the node.</p> required <code>name</code> <code>str | None</code> <p>name for the node. If not provided, inferred from func.</p> <code>None</code> <code>inputs</code> <code>Sequence[Input | Callable] | Input | Callable | None</code> <p>The inputs to the node.</p> <code>None</code> <code>outputs</code> <code>Sequence[Output] | Output | None</code> <p>The outputs from the node.</p> <code>None</code> <code>attributes</code> <code>dict[str, Any] | None</code> <p>Optional attributes for the node.</p> <code>None</code> <p>Returns:</p> Type Description <code>Node[FuncParams, FuncReturns] | View[FuncParams, FuncReturns]</code> <p>A Node instance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if any of the inputs is a callable that is not a view</p>"},{"location":"api/ordeq/_nodes/#ordeq._nodes.get_node","title":"<code>get_node(func)</code>","text":"<p>Gets the node from a callable created with the <code>@node</code> decorator.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>a callable created with the <code>@node</code> decorator</p> required <p>Returns:</p> Type Description <code>Node</code> <p>the node associated with the callable</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the callable was not created with the <code>@node</code> decorator</p>"},{"location":"api/ordeq/_nodes/#ordeq._nodes.infer_node_name_from_func","title":"<code>infer_node_name_from_func(func)</code>","text":"<p>Infers a node name from a function, including its module.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., Any]</code> <p>The function to infer the name from.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The inferred name.</p>"},{"location":"api/ordeq/_nodes/#ordeq._nodes.node","title":"<code>node(func=None, *, inputs=None, outputs=None, **attributes)</code>","text":"<pre><code>node(\n    func: Callable[FuncParams, FuncReturns],\n    *,\n    inputs: Sequence[Input | Callable]\n    | Input\n    | Callable\n    | None = None,\n    outputs: Sequence[Output] | Output | None = None,\n    **attributes: Any,\n) -&gt; Callable[FuncParams, FuncReturns]\n</code></pre><pre><code>node(\n    *,\n    inputs: Sequence[Input | Callable]\n    | Input\n    | Callable\n    | None = None,\n    outputs: Sequence[Output] | Output | None = None,\n    **attributes: Any,\n) -&gt; Callable[\n    [Callable[FuncParams, FuncReturns]],\n    Callable[FuncParams, FuncReturns],\n]\n</code></pre> <p>Decorator that creates a node from a function. When a node is run, the inputs are loaded and passed to the function. The returned values are saved to the outputs.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import DataFrame\n&gt;&gt;&gt; @node(\n...     inputs=CSV(path=\"path/to.csv\"),\n...     outputs=Table(table=\"db.table\")\n... )\n... def transformation(csv: DataFrame) -&gt; DataFrame:\n...     return csv.select(\"someColumn\")\n</code></pre> <p>Nodes can also take a variable number of inputs:</p> <pre><code>&gt;&gt;&gt; @node(\n...     inputs=[\n...         CSV(path=\"path/to/fst.csv\"),\n...         # ...\n...         CSV(path=\"path/to/nth.csv\")\n...     ],\n...     outputs=Table(table=\"db.all_appended\")\n... )\n... def append_dfs(*args: DataFrame) -&gt; DataFrame:\n...     df = args[0]\n...     for arg in args[1:]:\n...         df = df.unionByName(arg)\n...     return df\n</code></pre> <p>Node can also be created from existing functions:</p> <pre><code>&gt;&gt;&gt; def remove_header(data: list[str]) -&gt; list[str]:\n...     return data[1:]\n&gt;&gt;&gt; fst = node(remove_header, inputs=CSV(path=\"path/to/fst.csv\"), ...)\n&gt;&gt;&gt; snd = node(remove_header, inputs=CSV(path=\"path/to/snd.csv\"), ...)\n&gt;&gt;&gt; ...\n</code></pre> <p>You can assign attributes to a node, which can be used for filtering or grouping nodes later:</p> <pre><code>&gt;&gt;&gt; @node(inputs=..., outputs=..., group=\"group1\", retries=3)\n... def func(...): -&gt; ...\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[FuncParams, FuncReturns] | None</code> <p>function of the node</p> <code>None</code> <code>inputs</code> <code>Sequence[Input | Callable] | Input | Callable | None</code> <p>sequence of inputs</p> <code>None</code> <code>outputs</code> <code>Sequence[Output] | Output | None</code> <p>sequence of outputs</p> <code>None</code> <code>attributes</code> <code>Any</code> <p>additional attributes to assign to the node</p> <code>{}</code> <p>Returns:</p> Type Description <code>Callable[[Callable[FuncParams, FuncReturns]], Callable[FuncParams, FuncReturns]] | Callable[FuncParams, FuncReturns]</code> <p>a node</p>"},{"location":"api/ordeq/_resolve/","title":"_resolve.py","text":"<p>Resolve packages and modules to nodes and IOs.</p>"},{"location":"api/ordeq/_runner/","title":"_runner.py","text":""},{"location":"api/ordeq/_runner/#ordeq._runner.run","title":"<code>run(*runnables, hooks=(), save='all', verbose=False, io=None)</code>","text":"<p>Runs nodes in topological order.</p> <p>Parameters:</p> Name Type Description Default <code>runnables</code> <code>Runnable</code> <p>the nodes to run, or modules or packages containing nodes</p> <code>()</code> <code>hooks</code> <code>Sequence[RunnerHook | str]</code> <p>hooks to apply</p> <code>()</code> <code>save</code> <code>SaveMode</code> <p>'all' | 'sinks'. If 'sinks', only saves the sink outputs</p> <code>'all'</code> <code>verbose</code> <code>bool</code> <p>whether to print the node graph</p> <code>False</code> <code>io</code> <code>dict[Input[T] | Output[T], Input[T] | Output[T]] | None</code> <p>mapping of IO objects to their replacements</p> <code>None</code>"},{"location":"api/ordeq/types/","title":"types.py","text":""},{"location":"api/ordeq/types/#ordeq.types.GlobPath","title":"<code>GlobPath</code>","text":"<p>               Bases: <code>PathLike</code>, <code>Protocol</code></p> <p>Protocol for accepting any PathLike object that has a <code>glob</code> method</p>"},{"location":"api/ordeq/types/#ordeq.types.PathLike","title":"<code>PathLike</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Utility protocol used for the type of the <code>path</code> attribute of file-like datasets. Example types that adhere to this protocol are <code>pathlib.Path</code> and <code>cloudpathlib.CloudPath</code>:</p> <pre><code>&gt;&gt;&gt; from ordeq.types import PathLike\n&gt;&gt;&gt; from cloudpathlib import CloudPath\n&gt;&gt;&gt; isinstance(CloudPath, PathLike)\nTrue\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; isinstance(Path, PathLike)\nTrue\n</code></pre> <p><code>pathlib.Path</code> provides a consistent file addressing mechanism across different operating systems. Moreover, <code>cloudpathlib.CloudPath</code> allows users to interact with files on cloud storage through (most of the) <code>pathlib.Path</code> API.  The <code>PathLike</code> protocol allows users to use either one interchangebly in the dataset definition.</p> <p>Note that datasets that rely on the IO implementation of an external library may expect a type different from <code>PathLike</code> for its <code>path</code> (e.g., <code>spark.read_csv</code>).</p>"},{"location":"api/ordeq_altair/","title":"ordeq_altair","text":""},{"location":"api/ordeq_altair/chart/","title":"chart.py","text":""},{"location":"api/ordeq_altair/chart/#ordeq_altair.chart.AltairChart","title":"<code>AltairChart</code>","text":"<p>               Bases: <code>Output[Chart]</code></p> <p>IO to save altair Charts.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from ordeq_altair import AltairChart\n&gt;&gt;&gt; my_chart = AltairChart(\n...     path=Path(\"path/figure.pdf\")\n... )\n</code></pre>"},{"location":"api/ordeq_altair/chart/#ordeq_altair.chart.AltairChart.save","title":"<code>save(chart, **save_options)</code>","text":"<p>Saves the Altair chart to a specified path in HTML format.</p> <p>Parameters:</p> Name Type Description Default <code>chart</code> <code>Chart</code> <p>The Altair chart to save.</p> required <code>**save_options</code> <code>Any</code> <p>Additional options to pass to the <code>save</code> method.</p> <code>{}</code>"},{"location":"api/ordeq_args/","title":"ordeq_args","text":""},{"location":"api/ordeq_args/command_line_arg/","title":"command_line_arg.py","text":""},{"location":"api/ordeq_args/command_line_arg/#ordeq_args.command_line_arg.CommandLineArg","title":"<code>CommandLineArg</code>","text":"<p>               Bases: <code>Input[T]</code></p> <p>Dataset that represents a command line argument as node input. Useful for parameterization of node logic based on arguments in the run command.</p> <p>Parses the argument from <code>sys.argv</code> on load. See argparse for more information.</p> <p>Example:</p> main.py<pre><code>from ordeq import node, run\nfrom ordeq_spark import SparkHiveTable\nimport pyspark.sql.functions as F\nfrom pyspark.sql import DataFrame\n\n\n@node(\n    inputs=[SparkHiveTable(table=\"my.table\"), CommandLineArg(\"--value\")],\n    outputs=SparkHiveTable(table=\"my.output\"),\n)\ndef transform(df: DataFrame, value: str) -&gt; DataFrame:\n    return df.where(F.col(\"col\") == value)\n\n\nif __name__ == \"__main__\":\n    run(transform)\n</code></pre> <p>When you run <code>transform</code> through the CLI as follows:</p> <pre><code>python main.py --value MyValue\n</code></pre> <p><code>MyValue</code> will be used as <code>value</code> in <code>transform</code>.</p> <p>By default, the command line arguments are parsed as string. You can parse as different type using built-in type converters, for instance:</p> <pre><code>import pathlib\nimport datetime\n\nk = CommandLineArg(\"--k\", type=int)\nthreshold = CommandLineArg(\"--threshold\", type=float)\naddress = CommandLineArg(\"--address\", type=ascii)\npath = CommandLineArg(\"--path\", type=pathlib.Path)\ndate_time = CommandLineArg(\"--date\", type=datetime.date.fromisoformat)\n</code></pre> <p>Alternatively, you can parse using a user-defined function, e.g.:</p> <pre><code>def hyphenated(string: str) -&gt; str:\n    return \"-\".join([w[:4] for w in string.casefold().split()])\n\n\ntitle = CommandLineArg(\"--title\", type=hyphenated)\n</code></pre> <p>When using multiple <code>CommandLineArg</code> IOs in a node, then you can link them to the same argument parser:</p> <pre><code>import argparse\n\nparser = argparse.ArgumentParser()\narg1 = CommandLineArg(\"--arg1\", parser=parser)\narg2 = CommandLineArg(\"--arg2\", parser=parser)\n</code></pre> <p>Parsing command line arguments as <code>argparse.FileType</code> is discouraged as it has been deprecated from Python 3.14.</p> <p>More info on parsing types here</p>"},{"location":"api/ordeq_args/environment_variable/","title":"environment_variable.py","text":""},{"location":"api/ordeq_args/environment_variable/#ordeq_args.environment_variable.EnvironmentVariable","title":"<code>EnvironmentVariable</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[str]</code></p> <p>IO used to load and save environment variables. Use:     - as input, to parameterize the node logic     - as output, to set an environment variable based on node logic</p> <p>Gets and sets <code>os.environ</code> on load and save. See the Python docs for more information.</p> <p>Example in a node:</p> main.py<pre><code>from ordeq import run, node\nfrom ordeq_spark import SparkHiveTable\nimport pyspark.sql.functions as F\nfrom pyspark.sql import DataFrame\n\n\n@node(\n    inputs=[\n        SparkHiveTable(table=\"my.table\"),\n        EnvironmentVariable(\"KEY\", default=\"DEFAULT\"),\n    ],\n    outputs=SparkHiveTable(table=\"my.output\"),\n)\ndef transform(df: DataFrame, value: str) -&gt; DataFrame:\n    return df.where(F.col(\"col\") == value)\n\n\nif __name__ == \"__main__\":\n    run(transform)\n</code></pre> <p>When you run <code>transform</code> through the CLI as follows:</p> <pre><code>export KEY=MyValue\npython main.py transform\n</code></pre> <p><code>MyValue</code> will be used as <code>value</code> in <code>transform</code>.</p>"},{"location":"api/ordeq_boto3/","title":"ordeq_boto3","text":""},{"location":"api/ordeq_boto3/s3_connector/","title":"s3_connector.py","text":""},{"location":"api/ordeq_boto3/s3_connector/#ordeq_boto3.s3_connector.S3Connector","title":"<code>S3Connector</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[BaseClient | ServiceResource]</code></p> <p>IO object representing the S3 client or S3 resource.</p> <p>Example:</p> <p>Using a Boto3 S3 Client:</p> <pre><code>&gt;&gt;&gt; from ordeq_boto3 import S3Connector\n&gt;&gt;&gt; import boto3\n&gt;&gt;&gt; s3_connector = S3Connector(\n...     connector=boto3.client('s3')\n... )\n&gt;&gt;&gt; client = s3_connector.load()  # doctest: +SKIP\n</code></pre> <p>Using a Boto3 S3 Resource:</p> <pre><code>&gt;&gt;&gt; from ordeq_boto3 import S3Connector\n&gt;&gt;&gt; import boto3\n&gt;&gt;&gt; s3_connector = S3Connector(\n...     connector=boto3.resource('s3')\n... )\n&gt;&gt;&gt; resource = s3_connector.load()  # doctest: +SKIP\n</code></pre>"},{"location":"api/ordeq_boto3/s3_connector/#ordeq_boto3.s3_connector.S3Connector.load","title":"<code>load()</code>","text":"<p>Gets the S3 Client or S3 Resource</p> <p>Returns:</p> Type Description <code>BaseClient | ServiceResource</code> <p>The S3 Client or Resource</p>"},{"location":"api/ordeq_boto3/s3_object/","title":"s3_object.py","text":""},{"location":"api/ordeq_boto3/s3_object/#ordeq_boto3.s3_object.S3Object","title":"<code>S3Object</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[bytes]</code></p> <p>IO for loading and saving objects from S3 using boto3.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from ordeq_boto3 import S3Object\n&gt;&gt;&gt; import boto3\n\n&gt;&gt;&gt; s3_object = S3Object(\n...     bucket=\"my-bucket\",\n...     key=\"path/to/my_object\",\n...     client=boto3.client('s3')\n... )\n&gt;&gt;&gt; data = s3_object.load()  # doctest: +SKIP\n</code></pre> <p>Extra parameters can be passed to the <code>load</code> and <code>save</code> methods, such as:</p> <pre><code>&gt;&gt;&gt; from datetime import datetime\n&gt;&gt;&gt; data = s3_object.load(\n...     IfModifiedSince=datetime(2015, 1, 1)\n... )  # doctest: +SKIP\n&gt;&gt;&gt; s3_object.save(ACL=\"authenticated-read\")  # doctest: +SKIP\n</code></pre> <p>When <code>client</code> is not provided, it will be created using <code>boto3.client(\"s3\")</code>:</p> <pre><code>&gt;&gt;&gt; s3_object = S3Object(\n...     bucket=\"my-bucket\",\n...     key=\"path/to/my_object\",\n... )\n</code></pre>"},{"location":"api/ordeq_common/","title":"ordeq_common","text":""},{"location":"api/ordeq_common/hooks/logger/","title":"logger.py","text":""},{"location":"api/ordeq_common/hooks/logger/#ordeq_common.hooks.logger.LoggerHook","title":"<code>LoggerHook</code>","text":"<p>               Bases: <code>InputHook</code>, <code>OutputHook</code>, <code>NodeHook</code></p> <p>Hook that prints the calls to the methods. Typically only used for test purposes.</p>"},{"location":"api/ordeq_common/hooks/spy/","title":"spy.py","text":""},{"location":"api/ordeq_common/hooks/spy/#ordeq_common.hooks.spy.SpyHook","title":"<code>SpyHook</code>","text":"<p>               Bases: <code>InputHook</code>, <code>OutputHook</code>, <code>NodeHook</code></p> <p>Hook that stores the arguments it is called with in a list. Typically only used for test purposes.</p>"},{"location":"api/ordeq_common/io/bytes_buffer/","title":"bytes_buffer.py","text":""},{"location":"api/ordeq_common/io/bytes_buffer/#ordeq_common.io.bytes_buffer.BytesBuffer","title":"<code>BytesBuffer</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[bytes]</code></p> <p>IO that uses an in-memory bytes buffer to load and save data. Useful for buffering data across nodes without writing to disk.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from ordeq_common import BytesBuffer\n&gt;&gt;&gt; buffer = BytesBuffer()\n&gt;&gt;&gt; buffer.load()\nb''\n</code></pre> <p>The buffer is initially empty, unless provided with initial data:</p> <pre><code>&gt;&gt;&gt; buffer = BytesBuffer(b\"Initial data\")\n&gt;&gt;&gt; buffer.load()\nb'Initial data'\n</code></pre> <p>Saving to the buffer appends data to the existing content:</p> <pre><code>&gt;&gt;&gt; buffer.save(b\"New data\")\n&gt;&gt;&gt; buffer.load()\nb'Initial dataNew data'\n</code></pre> <p>Example in a node:</p> <pre><code>&gt;&gt;&gt; from ordeq_args import CommandLineArg\n&gt;&gt;&gt; from ordeq_common import BytesBuffer, Literal\n&gt;&gt;&gt; from ordeq import node, run\n&gt;&gt;&gt; result = BytesBuffer()\n&gt;&gt;&gt; @node(\n...     inputs=[BytesBuffer(b\"Hello\"), Literal(b\"you\")], outputs=result\n... )\n... def greet(greeting: bytes, name: bytes) -&gt; bytes:\n...     return greeting + b\" to \" + name + b\"!\"\n&gt;&gt;&gt; _ = run(greet)\n&gt;&gt;&gt; result.load()\nb'Hello to you!'\n</code></pre>"},{"location":"api/ordeq_common/io/dataclass/","title":"dataclass.py","text":""},{"location":"api/ordeq_common/io/dataclass/#ordeq_common.io.dataclass.Dataclass","title":"<code>Dataclass</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Input['DataclassInstance']</code></p> <p>IO that parses data as Python dataclass on load.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from ordeq_common import Dataclass\n&gt;&gt;&gt; from ordeq_files import JSON\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; ValidJSON = JSON(path=Path(\"to/valid.json\"))\n&gt;&gt;&gt; ValidJSON.load()  # doctest: +SKIP\n{\"name\": \"banana\", \"colour\": \"yellow\"}\n\n&gt;&gt;&gt; @dataclass\n... class Fruit:\n...     name: str\n...     colour: str\n&gt;&gt;&gt; Dataclass(ValidJSON, Fruit).load()  # doctest: +SKIP\nFruit(name=\"banana\", colour=\"yellow\")\n\n&gt;&gt;&gt; InvalidJSON = JSON(path=Path(\"to/invalid.json\"))\n&gt;&gt;&gt; InvalidJSON.load()  # doctest: +SKIP\n{\"name\": \"banana\", \"weight_gr\": \"100\"}\n&gt;&gt;&gt; Dataclass(InvalidJSON, Fruit).load()  # doctest: +SKIP\nTypeError: Fruit.__init__() got an unexpected keyword argument 'weight_gr'\n</code></pre> <p>For nested models, or other more sophisticated parsing requirements consider using <code>ordeq-pydantic</code> instead.</p>"},{"location":"api/ordeq_common/io/iterate/","title":"iterate.py","text":""},{"location":"api/ordeq_common/io/iterate/#ordeq_common.io.iterate.Iterate","title":"<code>Iterate(*ios)</code>","text":"<p>IO for loading and saving iteratively. This can be useful when processing multiple IOs using the same node, while only requiring to have one of them in memory at the same time.</p> <p>Examples:</p> <p>The load function returns a generator:</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from ordeq_files import Text, JSON\n&gt;&gt;&gt; from ordeq_common import Iterate\n&gt;&gt;&gt; paths = [Path(\"hello.txt\"), Path(\"world.txt\")]\n&gt;&gt;&gt; text_ios = Iterate(*[Text(path=path) for path in paths])\n&gt;&gt;&gt; text_ios.load()  # doctest: +SKIP\n&lt;generator object Iterate._load at 0x104946f60&gt;\n</code></pre> <p>The load function returns the contents of the files in this case:</p> <pre><code>&gt;&gt;&gt; list(text_ios.load())  # doctest: +SKIP\n['hello', 'world']\n</code></pre> <p>By iterating over the contents, each file will be loaded and saved without the need to keep multiple files in memory at the same time:</p> <pre><code>&gt;&gt;&gt; for idx, content in enumerate(text_ios.load()):   # doctest: +SKIP\n...    JSON(\n...        path=paths[idx].with_suffix(\".json\")\n...    ).save({\"content\": content})\n</code></pre> <p>We can achieve the same by passing a generator to the <code>Iterate.save</code> method:</p> <pre><code>&gt;&gt;&gt; json_dataset = Iterate(\n...     *[\n...         JSON(path=path.with_suffix(\".json\"))\n...         for path in paths\n...     ]\n... )\n&gt;&gt;&gt; json_dataset.save(\n...    ({\"content\": content} for content in text_ios.load())\n... )   # doctest: +SKIP\n&gt;&gt;&gt; from collections.abc import Iterable\n&gt;&gt;&gt; def generate_json_contents(\n...     contents: Iterable[str]\n... ) -&gt; Iterable[dict[str, str]]:\n...     for content in contents:\n...         yield {\"content\": content}\n&gt;&gt;&gt; json_dataset.save(generate_json_contents(text_ios.load()))   # doctest: +SKIP\n</code></pre> <p>Returns:</p> Type Description <code>_Iterate[T]</code> <p>_Iterate</p>"},{"location":"api/ordeq_common/io/literal/","title":"literal.py","text":""},{"location":"api/ordeq_common/io/literal/#ordeq_common.io.literal.Literal","title":"<code>Literal</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Input[T]</code></p> <p>IO that returns a pre-defined value on load. Mostly useful for testing purposes.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from ordeq_common import Literal\n&gt;&gt;&gt; value = Literal(\"someValue\")\n&gt;&gt;&gt; value.load()\n'someValue'\n&gt;&gt;&gt; print(value)\nLiteral('someValue')\n</code></pre>"},{"location":"api/ordeq_common/io/match/","title":"match.py","text":""},{"location":"api/ordeq_common/io/match/#ordeq_common.io.match.Match","title":"<code>Match(io=None)</code>","text":"<pre><code>Match(\n    io: Input[Tkey] | IO[Tkey],\n) -&gt; MatchOnLoad[Tval, Tkey]\n</code></pre><pre><code>Match() -&gt; MatchOnSave[Tval, Tkey]\n</code></pre> <p>Utility IO that allows dynamic switching between IO, like the match-case statement in Python.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from ordeq_common import Literal, Match\n&gt;&gt;&gt; from ordeq_args import EnvironmentVariable\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; Country = (\n...     Match(EnvironmentVariable(\"COUNTRY\"))\n...     .Case(\"NL\", Literal(\"Netherlands\"))\n...     .Case(\"BE\", Literal(\"Belgium\"))\n...     .Default(Literal(\"Unknown\"))\n... )\n&gt;&gt;&gt; os.environ[\"COUNTRY\"] = \"NL\"\n&gt;&gt;&gt; Country.load()\n'Netherlands'\n</code></pre> <p>If a default is provided, it will be used when no cases match:</p> <pre><code>&gt;&gt;&gt; os.environ[\"COUNTRY\"] = \"DE\"\n&gt;&gt;&gt; Country.load()\n'Unknown'\n</code></pre> <p>Otherwise, it raises an error when none of the provided cases are matched:</p> <pre><code>&gt;&gt;&gt; Match(EnvironmentVariable(\"COUNTRY\")).load()  # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\n...\nordeq.IOException: Failed to load\nUnsupported case 'DE'\n</code></pre> <p>Match on save works as follows:</p> <pre><code>&gt;&gt;&gt; SmallOrLarge = (\n...     Match()\n...     .Case(\"S\", EnvironmentVariable(\"SMALL\"))\n...     .Case(\"L\", EnvironmentVariable(\"LARGE\"))\n...     .Default(EnvironmentVariable(\"UNKNOWN\"))\n... )\n&gt;&gt;&gt; SmallOrLarge.save((\"S\", \"Andorra\"))\n&gt;&gt;&gt; SmallOrLarge.save((\"L\", \"Russia\"))\n&gt;&gt;&gt; SmallOrLarge.save((\"XXL\", \"Mars\"))\n&gt;&gt;&gt; os.environ[\"SMALL\"]\n'Andorra'\n&gt;&gt;&gt; os.environ.get(\"LARGE\")\n'Russia'\n&gt;&gt;&gt; os.environ.get(\"UNKNOWN\")\n'Mars'\n</code></pre> <p>Example in a node:</p> <pre><code>&gt;&gt;&gt; from ordeq import node\n&gt;&gt;&gt; from ordeq_files import JSON\n&gt;&gt;&gt; from ordeq_args import CommandLineArg\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; TestOrTrain = (\n...     Match(CommandLineArg(\"--split\"))\n...     .Case(\"test\", JSON(path=Path(\"to/test.json\")))\n...     .Case(\"train\", JSON(path=Path(\"to/train.json\")))\n... )\n&gt;&gt;&gt; @node(\n...     inputs=TestOrTrain,\n... )\n... def evaluate(data: dict) -&gt; dict:\n...     ...\n</code></pre> <p>Returns:</p> Type Description <code>MatchOnLoad | MatchOnSave</code> <p>MatchOnLoad or MatchOnSave</p>"},{"location":"api/ordeq_common/io/printer/","title":"printer.py","text":""},{"location":"api/ordeq_common/io/printer/#ordeq_common.io.printer.Print","title":"<code>Print</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Output[Any]</code></p> <p>Output that prints data on save. Mostly useful for debugging purposes. The difference between other utilities like <code>StringBuffer</code> and <code>Pass</code> is that <code>Print</code> shows the output of the node directly on the console.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from ordeq_common import Print, Literal\n&gt;&gt;&gt; from ordeq import node, run\n&gt;&gt;&gt; @node(\n...     inputs=Literal(\"hello, world!\"),\n...     outputs=Print()\n... )\n... def print_message(message: str) -&gt; str:\n...     return message.capitalize()\n\n&gt;&gt;&gt; result = run(print_message)\nHello, world!\n</code></pre>"},{"location":"api/ordeq_common/io/string_buffer/","title":"string_buffer.py","text":""},{"location":"api/ordeq_common/io/string_buffer/#ordeq_common.io.string_buffer.StringBuffer","title":"<code>StringBuffer</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[str]</code></p> <p>IO that uses an in-memory string buffer to load and save data. Useful for buffering data across nodes without writing to disk.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from ordeq_common import StringBuffer\n&gt;&gt;&gt; buffer = StringBuffer()\n&gt;&gt;&gt; buffer.load()\n''\n</code></pre> <p>The buffer is initially empty, unless provided with initial data:</p> <pre><code>&gt;&gt;&gt; buffer = StringBuffer(\"Initial data\")\n&gt;&gt;&gt; buffer.load()\n'Initial data'\n</code></pre> <p>Saving to the buffer appends data to the existing content:</p> <pre><code>&gt;&gt;&gt; buffer.save(\"New data\")\n&gt;&gt;&gt; buffer.load()\n'Initial dataNew data'\n</code></pre> <p>Example in a node:</p> <pre><code>&gt;&gt;&gt; from ordeq_args import CommandLineArg\n&gt;&gt;&gt; from ordeq_common import StringBuffer, Literal\n&gt;&gt;&gt; from ordeq import node, run\n&gt;&gt;&gt; result = StringBuffer()\n&gt;&gt;&gt; @node(\n...     inputs=[StringBuffer(\"Hello\"), Literal(\"you\")],\n...     outputs=result\n... )\n... def greet(greeting: str, name: str) -&gt; str:\n...     return f\"{greeting} to {name}!\"\n&gt;&gt;&gt; _ = run(greet)\n&gt;&gt;&gt; result.load()\n'Hello to you!'\n</code></pre>"},{"location":"api/ordeq_dev_tools/","title":"ordeq_dev_tools","text":""},{"location":"api/ordeq_dev_tools/paths/","title":"paths.py","text":""},{"location":"api/ordeq_dev_tools/utils/","title":"utils.py","text":""},{"location":"api/ordeq_dev_tools/utils/#ordeq_dev_tools.utils.run_command","title":"<code>run_command(command)</code>","text":"<p>Runs a shell command and returns its output.</p> <p>Parameters:</p> Name Type Description Default <code>command</code> <code>list[str]</code> <p>List of command arguments</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>Output of the command as a string</p>"},{"location":"api/ordeq_dev_tools/pipelines/docs_package_overview/","title":"docs_package_overview.py","text":"<p>Generate a markdown table overview of all packages in the packages/ directory. The resulting markdown is written to docs/packages.md.</p>"},{"location":"api/ordeq_dev_tools/pipelines/docs_package_overview/#ordeq_dev_tools.pipelines.docs_package_overview.get_pypi_name_description_group_logo","title":"<code>get_pypi_name_description_group_logo(pyproject_path)</code>","text":"<p>Extract the relevant attributes for the package pyproject.tomls, including logo_url from [tool.ordeq-dev].</p> <p>Parameters:</p> Name Type Description Default <code>pyproject_path</code> <code>Path</code> <p>The path to the pyproject.toml file.</p> required <p>Returns:</p> Type Description <code>tuple[str, str, str | None, str | None]</code> <p>A tuple containing the package name, description, group (or None), and logo_url (or None).</p>"},{"location":"api/ordeq_dev_tools/pipelines/docs_package_overview/#ordeq_dev_tools.pipelines.docs_package_overview.groups","title":"<code>groups(packages)</code>","text":"<p>Generate HTML table row data for each package directory, including logo (if present).</p> <p>Parameters:</p> Name Type Description Default <code>packages</code> <code>list[str]</code> <p>A list of package directory paths.</p> required <p>Returns:</p> Type Description <code>dict[str, list[dict]]</code> <p>A mapping group names to lists of package dicts.</p>"},{"location":"api/ordeq_dev_tools/pipelines/docs_package_overview/#ordeq_dev_tools.pipelines.docs_package_overview.write_html_table_by_group","title":"<code>write_html_table_by_group(groups)</code>","text":"<p>Write the grouped HTML tables to the given output_path, with logo column and proper sizing.</p> <p>Parameters:</p> Name Type Description Default <code>groups</code> <code>dict[str, list[dict]]</code> <p>A dictionary mapping group names to lists of package dicts.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The generated markdown content as a string.</p>"},{"location":"api/ordeq_dev_tools/pipelines/docs_update_just/","title":"docs_update_just.py","text":"<p>Pipeline to update documentation with the latest <code>just</code> command list.</p>"},{"location":"api/ordeq_dev_tools/pipelines/docs_update_just/#ordeq_dev_tools.pipelines.docs_update_just.just_output","title":"<code>just_output()</code>","text":"<p>Run <code>just --list</code> command and return its output as a string.</p> <p>Returns:</p> Type Description <code>str</code> <p>Output of <code>just --list</code> command.</p>"},{"location":"api/ordeq_dev_tools/pipelines/docs_update_just/#ordeq_dev_tools.pipelines.docs_update_just.update_docs_with_just_section","title":"<code>update_docs_with_just_section(docs_file, just_section)</code>","text":"<p>Update the documentation file with the latest <code>just</code> command list.</p> <p>Parameters:</p> Name Type Description Default <code>docs_file</code> <code>str</code> <p>The current content of the documentation file.</p> required <code>just_section</code> <code>str</code> <p>The formatted <code>just</code> command list section.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The updated documentation content.</p>"},{"location":"api/ordeq_dev_tools/pipelines/generate_draft_releases/","title":"generate_draft_releases.py","text":"<p>Automated release pipeline</p>"},{"location":"api/ordeq_dev_tools/pipelines/generate_draft_releases/#ordeq_dev_tools.pipelines.generate_draft_releases.bump_version","title":"<code>bump_version(version, bump)</code>","text":"<p>Bump the version based on the specified type.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>Version</code> <p>A tuple of integers representing the current version, e.g. (1, 2, 3).</p> required <code>bump</code> <code>Literal['major', 'minor', 'patch']</code> <p>The type of bump to apply, one of 'major', 'minor', or 'patch'.</p> required <p>Returns:</p> Type Description <code>tuple[int, int, int]</code> <p>A tuple of integers representing the new version after the bump.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the bump type is unknown.</p>"},{"location":"api/ordeq_dev_tools/pipelines/generate_draft_releases/#ordeq_dev_tools.pipelines.generate_draft_releases.compute_bump","title":"<code>compute_bump(labels)</code>","text":"<p>Compute the version bump type based on PR labels.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>list[str]</code> <p>List of PR labels</p> required <p>Returns:</p> Type Description <code>Literal['major', 'minor', 'patch'] | None</code> <p>Bump type if found, else None</p>"},{"location":"api/ordeq_dev_tools/pipelines/generate_draft_releases/#ordeq_dev_tools.pipelines.generate_draft_releases.create_draft_github_release","title":"<code>create_draft_github_release(package, new_tag, release_notes)</code>","text":"<p>Create a draft GitHub release for the new tag.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>str</code> <p>Package name</p> required <code>new_tag</code> <code>str</code> <p>New tag name</p> required <code>release_notes</code> <code>str</code> <p>Changes dictionary</p> required"},{"location":"api/ordeq_dev_tools/pipelines/generate_draft_releases/#ordeq_dev_tools.pipelines.generate_draft_releases.delete_draft_github_release","title":"<code>delete_draft_github_release(tag)</code>","text":"<p>Delete a draft GitHub release for the given tag.</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>Tag name</p> required"},{"location":"api/ordeq_dev_tools/pipelines/generate_draft_releases/#ordeq_dev_tools.pipelines.generate_draft_releases.filter_commits_by_package","title":"<code>filter_commits_by_package(commits, changes_per_commit, package)</code>","text":"<p>Filters commits to only include those that have changes in the package.</p> <p>Parameters:</p> Name Type Description Default <code>commits</code> <code>list[str]</code> <p>List of commit hashes</p> required <code>changes_per_commit</code> <code>dict[str, list[str]]</code> <p>List of commits to filter</p> required <code>package</code> <code>str</code> <p>Package name to filter by</p> required <p>Returns:</p> Type Description <code>list[dict[str, str]]</code> <p>Filtered list of commits with added 'changed_files' information</p>"},{"location":"api/ordeq_dev_tools/pipelines/generate_draft_releases/#ordeq_dev_tools.pipelines.generate_draft_releases.filter_commits_by_package_node","title":"<code>filter_commits_by_package_node(commit_per_package, changes_per_commit)</code>","text":"<p>Filters commits for each package to only include those that have changes</p> <p>Parameters:</p> Name Type Description Default <code>commit_per_package</code> <code>dict[str, list[str]]</code> <p>Mapping of package names to their commits</p> required <code>changes_per_commit</code> <code>dict[str, list[str]]</code> <p>Mapping of commit hashes to their changed files</p> required <p>Returns:</p> Type Description <code>dict[str, list[dict[str, str]]]</code> <p>Mapping of package names to their filtered commits</p>"},{"location":"api/ordeq_dev_tools/pipelines/generate_draft_releases/#ordeq_dev_tools.pipelines.generate_draft_releases.get_all_commits","title":"<code>get_all_commits(tags)</code>","text":"<p>Gets all commits since the latest tag for all packages.</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>dict[str, str]</code> <p>Mapping of package names to their latest tag</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, list[str]], dict[str, list[str]]]</code> <p>Mapping of package names to their commits and commit messages</p>"},{"location":"api/ordeq_dev_tools/pipelines/generate_draft_releases/#ordeq_dev_tools.pipelines.generate_draft_releases.get_all_latest_tags","title":"<code>get_all_latest_tags(package_tags)</code>","text":"<p>Gets the latest tag for all packages.</p> <p>Parameters:</p> Name Type Description Default <code>package_tags</code> <code>dict[str, list[str]]</code> <p>Mapping of package names to their tags</p> required <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Mapping of package names to their latest tag</p>"},{"location":"api/ordeq_dev_tools/pipelines/generate_draft_releases/#ordeq_dev_tools.pipelines.generate_draft_releases.get_commit_changed_files","title":"<code>get_commit_changed_files(commits_per_package)</code>","text":"<p>Gets the list of changed files for each commit.</p> <p>Parameters:</p> Name Type Description Default <code>commits_per_package</code> <code>dict[str, list[str]]</code> <p>Mapping of package names to their commits</p> required <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>Mapping of commit hashes to their changed files</p>"},{"location":"api/ordeq_dev_tools/pipelines/generate_draft_releases/#ordeq_dev_tools.pipelines.generate_draft_releases.get_commits_since_tag","title":"<code>get_commits_since_tag(tag)</code>","text":"<p>Gets a list of commits between the specified tag and HEAD.</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>Tag to compare against</p> required <p>Returns:</p> Type Description <code>list[dict[str, str]]</code> <p>Commits with hash</p>"},{"location":"api/ordeq_dev_tools/pipelines/generate_draft_releases/#ordeq_dev_tools.pipelines.generate_draft_releases.get_draft_releases","title":"<code>get_draft_releases()</code>","text":"<p>Get existing draft GitHub releases.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of tag names for draft releases</p>"},{"location":"api/ordeq_dev_tools/pipelines/generate_draft_releases/#ordeq_dev_tools.pipelines.generate_draft_releases.get_github_pr_by_sha","title":"<code>get_github_pr_by_sha(commit)</code>","text":"<p>Get the GitHub PR URL associated with a commit SHA.</p> <p>Parameters:</p> Name Type Description Default <code>commit</code> <code>str</code> <p>Commit SHA</p> required <p>Returns:</p> Type Description <code>dict[str, Any] | None</code> <p>PR URL if found, else None</p>"},{"location":"api/ordeq_dev_tools/pipelines/generate_draft_releases/#ordeq_dev_tools.pipelines.generate_draft_releases.get_latest_tag","title":"<code>get_latest_tag(package, tags)</code>","text":"<p>Gets the latest tag for a package in the format '[package]/vX.Y.Z'.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>str</code> <p>Name of the package</p> required <code>tags</code> <code>list[str]</code> <p>Name of the package</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>Tag if found, else None</p>"},{"location":"api/ordeq_dev_tools/pipelines/generate_draft_releases/#ordeq_dev_tools.pipelines.generate_draft_releases.get_relevant_prs","title":"<code>get_relevant_prs(commits)</code>","text":"<p>Gets relevant PRs for each package based on filtered commits.</p> <p>Parameters:</p> Name Type Description Default <code>commits</code> <code>dict[str, list[dict[str, str]]]</code> <p>Mapping of package names to their filtered commits</p> required <p>Returns:</p> Type Description <code>dict[str, list[dict[str, Any]]]</code> <p>Mapping of package names to their relevant PRs</p>"},{"location":"api/ordeq_dev_tools/pipelines/generate_draft_releases/#ordeq_dev_tools.pipelines.generate_draft_releases.get_tags","title":"<code>get_tags(package)</code>","text":"<p>Get all git tags for a given package.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>str</code> <p>The package name, e.g. 'ordeq'.</p> required <p>Returns:</p> Type Description <code>list[str] | None</code> <p>A list of tags for the package.</p>"},{"location":"api/ordeq_dev_tools/pipelines/generate_draft_releases/#ordeq_dev_tools.pipelines.generate_draft_releases.get_version_from_tag","title":"<code>get_version_from_tag(tag, package)</code>","text":"<p>Extracts the version from a tag in the format '[package]/vX.Y.Z'.</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>Tag string</p> required <code>package</code> <code>str</code> <p>Package name</p> required <p>Returns:</p> Type Description <code>Version</code> <p>Version object</p>"},{"location":"api/ordeq_dev_tools/pipelines/generate_draft_releases/#ordeq_dev_tools.pipelines.generate_draft_releases.package_tags","title":"<code>package_tags(packages)</code>","text":"<p>Gets all git tags for all packages.</p> <p>Parameters:</p> Name Type Description Default <code>packages</code> <code>list[str]</code> <p>Package names</p> required <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Mapping package names to their tags</p>"},{"location":"api/ordeq_dev_tools/pipelines/list_changed_packages/","title":"list_changed_packages.py","text":"<p>Pipeline to list changed packages compared to main branch.</p>"},{"location":"api/ordeq_dev_tools/pipelines/list_changed_packages/#ordeq_dev_tools.pipelines.list_changed_packages.changed_files","title":"<code>changed_files()</code>","text":"<p>Get list of changed files compared to main branch.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of changed file paths relative to repo root</p>"},{"location":"api/ordeq_dev_tools/pipelines/list_changed_packages/#ordeq_dev_tools.pipelines.list_changed_packages.extract_changed_packages","title":"<code>extract_changed_packages(changed_files)</code>","text":"<p>Extract unique package names from changed files.</p> <p>Parameters:</p> Name Type Description Default <code>changed_files</code> <code>list[str]</code> <p>List of changed file paths</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of package names that have changes</p>"},{"location":"api/ordeq_dev_tools/pipelines/list_dependencies/","title":"list_dependencies.py","text":"<p>Pipeline to list dependencies between packages.</p>"},{"location":"api/ordeq_dev_tools/pipelines/list_dependencies/#ordeq_dev_tools.pipelines.list_dependencies.compute_affected_dependencies","title":"<code>compute_affected_dependencies(deps_by_package)</code>","text":"<p>Compute which packages are (recursively) affected by changes in a given package. This can be used to determine which packages need to be retested.</p> <p>Parameters:</p> Name Type Description Default <code>deps_by_package</code> <code>dict[str, list[str]]</code> <p>mapping of package names to their dependencies</p> required <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>A dictionary mapping package names to the list of packages affected by</p> <code>dict[str, list[str]]</code> <p>changes in that package.</p>"},{"location":"api/ordeq_dev_tools/pipelines/list_dependencies/#ordeq_dev_tools.pipelines.list_dependencies.generate_mermaid_diagram","title":"<code>generate_mermaid_diagram(deps_by_package)</code>","text":"<p>Generate a Mermaid diagram of package dependencies.</p> <p>Parameters:</p> Name Type Description Default <code>deps_by_package</code> <code>dict[str, list[str]]</code> <p>mapping of package names to their dependencies</p> required <p>Returns:</p> Type Description <code>str</code> <p>The Mermaid diagram</p>"},{"location":"api/ordeq_dev_tools/pipelines/list_dependencies/#ordeq_dev_tools.pipelines.list_dependencies.parse_dependencies","title":"<code>parse_dependencies(lock_data)</code>","text":"<p>Parse dependencies from uv.lock.</p> <p>Parameters:</p> Name Type Description Default <code>lock_data</code> <code>dict[str, Any]</code> <p>data from the uv.lock file</p> required <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>A dictionary mapping package names to their dependencies</p>"},{"location":"api/ordeq_dev_tools/pipelines/relevant_packages/","title":"relevant_packages.py","text":"<p>Pipeline that extracts relevant ordeq packages based on changed packages and affected dependencies.</p>"},{"location":"api/ordeq_dev_tools/pipelines/relevant_packages/#ordeq_dev_tools.pipelines.relevant_packages.extract_relevant_packages","title":"<code>extract_relevant_packages(changed_packages, affected_deps)</code>","text":"<p>Extract relevant ordeq packages based on changed packages and affected dependencies.</p> <p>Parameters:</p> Name Type Description Default <code>changed_packages</code> <code>list[str]</code> <p>List of changed package names.</p> required <code>affected_deps</code> <code>dict[str, Any]</code> <p>Dictionary mapping package names to their affected dependencies.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of relevant package names.</p>"},{"location":"api/ordeq_dev_tools/pipelines/shared/","title":"shared.py","text":""},{"location":"api/ordeq_dev_tools/pipelines/shared/#ordeq_dev_tools.pipelines.shared.packages","title":"<code>packages()</code>","text":"<p>Gets a list of package names from the packages directory.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>Package names</p>"},{"location":"api/ordeq_dev_tools/pipelines/viz_self/","title":"viz_self.py","text":"<p>Pipeline to visualize the development tools.</p>"},{"location":"api/ordeq_dev_tools/pipelines/viz_self/#ordeq_dev_tools.pipelines.viz_self.visualize_ordeq_dev_tools","title":"<code>visualize_ordeq_dev_tools()</code>","text":"<p>Visualize the development tools using a mermaid diagram.</p> <p>Returns:</p> Type Description <code>str</code> <p>The mermaid diagram representation of the development tools.</p>"},{"location":"api/ordeq_duckdb/","title":"ordeq_duckdb","text":""},{"location":"api/ordeq_duckdb/connection/","title":"connection.py","text":""},{"location":"api/ordeq_duckdb/connection/#ordeq_duckdb.connection.DuckDBConnection","title":"<code>DuckDBConnection</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Input[DuckDBPyConnection]</code></p> <p>Input that loads a DuckDB connection.</p>"},{"location":"api/ordeq_duckdb/connection/#ordeq_duckdb.connection.DuckDBConnection.load","title":"<code>load(**kwargs)</code>","text":"<p>Loads a DuckDB connection.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional kwargs to pass to <code>duckdb.connect</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DuckDBPyConnection</code> <p>The DuckDB connection.</p>"},{"location":"api/ordeq_duckdb/csv/","title":"csv.py","text":""},{"location":"api/ordeq_duckdb/csv/#ordeq_duckdb.csv.DuckDBCSV","title":"<code>DuckDBCSV</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[DuckDBPyRelation]</code></p> <p>IO to load and save CSV files using DuckDB.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from ordeq import node, run\n&gt;&gt;&gt; from ordeq_duckdb import DuckDBCSV\n&gt;&gt;&gt; csv = DuckDBCSV(path=\"data.csv\")\n&gt;&gt;&gt; csv.save(duckdb.values([1, \"a\"]))\n&gt;&gt;&gt; data = csv.load()\n&gt;&gt;&gt; data.describe()\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  aggr   \u2502  col0  \u2502  col1   \u2502\n\u2502 varchar \u2502 double \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 count   \u2502    1.0 \u2502 1       \u2502\n\u2502 mean    \u2502    1.0 \u2502 NULL    \u2502\n\u2502 stddev  \u2502   NULL \u2502 NULL    \u2502\n\u2502 min     \u2502    1.0 \u2502 a       \u2502\n\u2502 max     \u2502    1.0 \u2502 a       \u2502\n\u2502 median  \u2502    1.0 \u2502 NULL    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n&lt;BLANKLINE&gt;\n</code></pre>"},{"location":"api/ordeq_duckdb/csv/#ordeq_duckdb.csv.DuckDBCSV.load","title":"<code>load(**kwargs)</code>","text":"<p>Load a CSV file into a DuckDB relation.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional options to pass to duckdb.read_csv.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DuckDBPyRelation</code> <p>The DuckDB relation representing the loaded CSV data.</p>"},{"location":"api/ordeq_duckdb/csv/#ordeq_duckdb.csv.DuckDBCSV.save","title":"<code>save(relation, **kwargs)</code>","text":"<p>Save a DuckDB relation to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>relation</code> <code>DuckDBPyRelation</code> <p>The relation to save.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional options to pass to <code>relation.to_csv</code></p> <code>{}</code>"},{"location":"api/ordeq_duckdb/table/","title":"table.py","text":""},{"location":"api/ordeq_duckdb/table/#ordeq_duckdb.table.DuckDBTable","title":"<code>DuckDBTable</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[DuckDBPyRelation]</code></p> <p>IO to load from and save to a DuckDB table.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import duckdb\n&gt;&gt;&gt; from ordeq_duckdb import DuckDBTable\n&gt;&gt;&gt; connection = duckdb.connect(\":memory:\")\n&gt;&gt;&gt; table = DuckDBTable(\n...     table=\"my_table\",\n...     connection=connection\n... )\n&gt;&gt;&gt; table.save(\n...     connection.values([123, \"abc\"])\n... )\n&gt;&gt;&gt; connection.sql(\"SELECT * FROM my_table\").show()\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 col0  \u2502  col1   \u2502\n\u2502 int32 \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   123 \u2502 abc     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n&lt;BLANKLINE&gt;\n</code></pre> <p>Example in a node:</p> <pre><code>&gt;&gt;&gt; from ordeq import node, run\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; connection = duckdb.connect(\":memory:\")\n&gt;&gt;&gt; table = DuckDBTable(\n...     table=\"my_data\",\n...     connection=connection,\n... )\n&gt;&gt;&gt; @node(outputs=table)\n... def convert_to_duckdb_relation() -&gt; duckdb.DuckDBPyRelation:\n...     return connection.values([2, \"b\"])\n&gt;&gt;&gt; result = run(convert_to_duckdb_relation)\n&gt;&gt;&gt; connection.table(\"my_data\").show()\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 col0  \u2502  col1   \u2502\n\u2502 int32 \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502     2 \u2502 b       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n&lt;BLANKLINE&gt;\n</code></pre>"},{"location":"api/ordeq_duckdb/table/#ordeq_duckdb.table.DuckDBTable.load","title":"<code>load()</code>","text":"<p>Load the DuckDB table into a DuckDB relation.</p> <p>Returns:</p> Type Description <code>DuckDBPyRelation</code> <p>A relation representing the loaded table.</p>"},{"location":"api/ordeq_duckdb/table/#ordeq_duckdb.table.DuckDBTable.save","title":"<code>save(relation, mode='create')</code>","text":"<p>Save a relation to the DuckDB table.</p> <p>Parameters:</p> Name Type Description Default <code>relation</code> <code>DuckDBPyRelation</code> <p>The relation to save.</p> required <code>mode</code> <code>Literal['create', 'insert']</code> <p>The save mode. \"create\" will create the table, \"insert\" will insert into the table if it exists, or create it if it doesn't.</p> <code>'create'</code> <p>Raises:</p> Type Description <code>CatalogException</code> <p>If the table already exists and mode is \"create\".</p>"},{"location":"api/ordeq_duckdb/view/","title":"view.py","text":""},{"location":"api/ordeq_duckdb/view/#ordeq_duckdb.view.DuckDBView","title":"<code>DuckDBView</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[DuckDBPyRelation]</code></p> <p>IO to load and save a DuckDB view.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import duckdb\n&gt;&gt;&gt; from ordeq_duckdb import DuckDBView\n&gt;&gt;&gt; connection = duckdb.connect(\":memory:\")\n&gt;&gt;&gt; view = DuckDBView(\n...     view=\"fruits\",\n...     connection=connection\n... )\n&gt;&gt;&gt; data = connection.values([1, \"apples\", \"red\"])\n&gt;&gt;&gt; view.save(data)\n&gt;&gt;&gt; view.load()\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 col0  \u2502  col1   \u2502  col2   \u2502\n\u2502 int32 \u2502 varchar \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502     1 \u2502 apples  \u2502 red     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n&lt;BLANKLINE&gt;\n</code></pre> <p>By default, the view will be replaced if it already exists. To change this, pass <code>replace=False</code> to the <code>save</code> method:</p> <pre><code>&gt;&gt;&gt; view = view.with_save_options(replace=False)\n&gt;&gt;&gt; view.save(data) # doctest: +SKIP\nIOException('Failed to save DuckDBView(view='fruits', ...\n</code></pre> <p>Example in a node:</p> <pre><code>&gt;&gt;&gt; from ordeq import node\n&gt;&gt;&gt; from ordeq_duckdb import DuckDBTable\n&gt;&gt;&gt; import duckdb\n&gt;&gt;&gt; connection = duckdb.connect(\":memory:\")\n&gt;&gt;&gt; fruits = DuckDBTable(\n...     table=\"fruits\",\n...     connection=connection,\n... )\n&gt;&gt;&gt; fruits_filtered = DuckDBView(\n...     view=\"fruits_filtered\",\n...     connection=connection,\n... )\n&gt;&gt;&gt; @node(inputs=fruits, outputs=fruits_filtered)\n... def filter_fruits(\n...     fruits: duckdb.DuckDBPyRelation\n... ) -&gt; duckdb.DuckDBPyRelation:\n...     return fruits.filter(\"color = 'red'\")\n</code></pre>"},{"location":"api/ordeq_duckdb/view/#ordeq_duckdb.view.DuckDBView.load","title":"<code>load()</code>","text":"<p>Loads a DuckDB view.</p> <p>Returns:</p> Type Description <code>DuckDBPyRelation</code> <p>The DuckDB view.</p>"},{"location":"api/ordeq_duckdb/view/#ordeq_duckdb.view.DuckDBView.save","title":"<code>save(relation, replace=True)</code>","text":"<p>Saves a DuckDB relation to a DuckDB view.</p> <p>Parameters:</p> Name Type Description Default <code>relation</code> <code>DuckDBPyRelation</code> <p>The DuckDB relation to save.</p> required <code>replace</code> <code>bool</code> <p>Whether to replace the view if it already exists.</p> <code>True</code>"},{"location":"api/ordeq_faiss/","title":"ordeq_faiss","text":""},{"location":"api/ordeq_faiss/index_module/","title":"index.py","text":""},{"location":"api/ordeq_faiss/index_module/#ordeq_faiss.index.FaissIndex","title":"<code>FaissIndex</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[Index]</code></p> <p>IO to load from and save index data using Faiss. Calls <code>faiss.read_index</code> and <code>faiss.write_index</code> under the hood.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from ordeq_faiss import FaissIndex\n&gt;&gt;&gt; my_index = FaissIndex(\n...     path=Path(\"path/to.index\")\n... )\n</code></pre>"},{"location":"api/ordeq_files/","title":"ordeq_files","text":""},{"location":"api/ordeq_files/bytes/","title":"bytes.py","text":""},{"location":"api/ordeq_files/bytes/#ordeq_files.bytes.Bytes","title":"<code>Bytes</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[bytes]</code></p> <p>IO representing bytes.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from ordeq_files import Bytes\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; my_png = Bytes(\n...     path=Path(\"path/to.png\")\n... )\n</code></pre>"},{"location":"api/ordeq_files/csv/","title":"csv.py","text":""},{"location":"api/ordeq_files/csv/#ordeq_files.csv.CSV","title":"<code>CSV</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[Iterable[Iterable[Any]]]</code></p> <p>IO representing a CSV file.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from ordeq_files import CSV\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; computer_sales = CSV(\n...     path=Path(\"path/to/computer_sales.csv\")\n... )\n</code></pre> <p>Example in a node:</p> <pre><code>&gt;&gt;&gt; from ordeq import node\n&gt;&gt;&gt; computer_sales_in_nl = CSV(path=Path(\"computer_sales_nl.csv\"))\n&gt;&gt;&gt; @node(\n...     inputs=computer_sales,\n...     outputs=computer_sales_in_nl\n... )\n... def filter_computer_sales(computer_sales: list) -&gt; list:\n...     return [row for row in computer_sales if row[1] == \"NL\"]\n</code></pre> <p>Loading and saving can be configured with additional parameters, e.g:</p> <pre><code>&gt;&gt;&gt; computer_sales.load(quotechar='\"', delimiter=',')  # doctest: +SKIP\n&gt;&gt;&gt; computer_sales.with_load_options(dialect='excel').load()  # doctest: +SKIP\n&gt;&gt;&gt; data = [[\"NL\", \"2023-10-01\", 1000], [\"BE\", \"2023-10-02\", 1500]]\n&gt;&gt;&gt; computer_sales.save(data, quoting=csv.QUOTE_MINIMAL)  # doctest: +SKIP\n</code></pre> <p>Refer to 1 for more details on the available options.</p>"},{"location":"api/ordeq_files/glob/","title":"glob.py","text":""},{"location":"api/ordeq_files/glob/#ordeq_files.glob.Glob","title":"<code>Glob</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Input[Generator[PathLike, None, None]]</code></p> <p>IO class that loads all paths provided a pattern. Although this class can be used as dataset in your nodes, for most cases it would be more suitable to inherit from this class and extend the <code>load</code> method, for example:</p> <pre><code>&gt;&gt;&gt; class LoadPartitions(Glob):\n...     def load(self):\n...         paths = super().load()\n...         for path in paths:\n...             yield my_load_func(path)\n</code></pre>"},{"location":"api/ordeq_files/json/","title":"json.py","text":""},{"location":"api/ordeq_files/json/#ordeq_files.json.JSON","title":"<code>JSON</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[dict[str, Any]]</code></p> <p>IO representing a JSON.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from ordeq_files import JSON\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; my_json = JSON(\n...     path=Path(\"path/to.json\")\n... )\n</code></pre>"},{"location":"api/ordeq_files/pickle/","title":"pickle.py","text":""},{"location":"api/ordeq_files/pickle/#ordeq_files.pickle.Pickle","title":"<code>Pickle</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[T]</code></p> <p><code>IO</code> that loads and saves a Pickle files.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from ordeq_files import Pickle\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; my_pickle = Pickle(\n...     path=Path(\"path/to.pkl\")\n... )\n</code></pre>"},{"location":"api/ordeq_files/text/","title":"text.py","text":""},{"location":"api/ordeq_files/text/#ordeq_files.text.Text","title":"<code>Text</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[str]</code></p> <p>IO representing a plain-text file.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ordeq_files import Text\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; my_text = Text(\n...     path=Path(\"path/to.txt\")\n... )\n</code></pre>"},{"location":"api/ordeq_huggingface/","title":"ordeq_huggingface","text":""},{"location":"api/ordeq_huggingface/dataset/","title":"dataset.py","text":""},{"location":"api/ordeq_huggingface/dataset/#ordeq_huggingface.dataset.HuggingfaceDataset","title":"<code>HuggingfaceDataset</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Input[Dataset | DatasetDict | IterableDatasetDict | IterableDataset]</code></p> <p>Load a dataset from the Huggingface datasets library.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from ordeq_huggingface import HuggingfaceDataset\n&gt;&gt;&gt; ds = HuggingfaceDataset(path=\"imdb\")\n&gt;&gt;&gt; data = ds.load(split=\"train[:10%]\")  # doctest: +SKIP\n&gt;&gt;&gt; len(data)  # doctest: +SKIP\n</code></pre>"},{"location":"api/ordeq_huggingface/disk_dataset/","title":"disk_dataset.py","text":""},{"location":"api/ordeq_huggingface/disk_dataset/#ordeq_huggingface.disk_dataset.HuggingfaceDiskDataset","title":"<code>HuggingfaceDiskDataset</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[Dataset | DatasetDict]</code></p> <p>Load and save a dataset from/to disk using the Huggingface datasets library.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from ordeq_huggingface import HuggingfaceDiskDataset\n&gt;&gt;&gt; ds = HuggingfaceDiskDataset(path=\"path/to/dataset\")  # doctest: +SKIP\n&gt;&gt;&gt; data = ds.load()  # doctest: +SKIP\n</code></pre>"},{"location":"api/ordeq_ibis/","title":"ordeq_ibis","text":""},{"location":"api/ordeq_ibis/io/parquet/","title":"parquet.py","text":""},{"location":"api/ordeq_ibis/io/parquet/#ordeq_ibis.io.parquet.IbisParquet","title":"<code>IbisParquet</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[Table]</code></p> <p>IO to load from and save to PARQUET data using Ibis.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from ordeq_ibis import IbisParquet\n&gt;&gt;&gt; MyParquetUsingPolars = IbisParquet(\n...     path=Path(\"path/to.parquet\"),\n...     resource=\"polars://\"\n... )\n\n&gt;&gt;&gt; MyParquetUsingDuckDB = IbisParquet(\n...     path=Path(\"path/to.parquet\"),\n...     resource=\"duckdb://\"\n... )\n</code></pre> <p>See 1 on how to configure the <code>resource</code>.</p>"},{"location":"api/ordeq_joblib/","title":"ordeq_joblib","text":""},{"location":"api/ordeq_joblib/joblib/","title":"joblib.py","text":""},{"location":"api/ordeq_joblib/joblib/#ordeq_joblib.joblib.Joblib","title":"<code>Joblib</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[T]</code></p> <p>IO for joblib objects.</p> <p>Example of stand-alone use:</p> <pre><code>&gt;&gt;&gt; from ordeq_joblib import Joblib\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; Model = Joblib(path=Path(\"model.pkl\"))\n&gt;&gt;&gt; model = Model.load()  # doctest: +SKIP\n&gt;&gt;&gt; Model.save(model)  # doctest: +SKIP\n</code></pre> <p>Example in a node:</p> <pre><code>&gt;&gt;&gt; from ordeq_joblib import Joblib\n&gt;&gt;&gt; from ordeq_pandas import PandasExcel\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from ordeq import node\n&gt;&gt;&gt; @node(\n...     inputs=[\n...         Joblib(path=Path(\"model.pkl\")),\n...         PandasExcel(path=Path(\"iris.xlsx\"))\n...     ],\n...     outputs=Joblib(path=Path(\"model-trained.pkl\"))\n... )\n... def train(model, df):\n...     X, y = df.drop('label'), df['label']\n...     model.fit(X, y)\n...     return model\n</code></pre>"},{"location":"api/ordeq_manifest/","title":"ordeq_manifest","text":""},{"location":"api/ordeq_manifest/manifest/","title":"manifest.py","text":""},{"location":"api/ordeq_manifest/manifest/#ordeq_manifest.manifest.create_manifest","title":"<code>create_manifest(package)</code>","text":"<p>Creates an in-memory manifest for the given package or module.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>ModuleType</code> <p>The package or module to create the manifest for.</p> required <p>Returns:</p> Name Type Description <code>ProjectModel</code> <code>ProjectModel</code> <p>the manifest of the package or module.</p>"},{"location":"api/ordeq_manifest/manifest/#ordeq_manifest.manifest.create_manifest_json","title":"<code>create_manifest_json(package, output=None, indent=2, **json_options)</code>","text":"<pre><code>create_manifest_json(\n    package: ModuleType,\n    output: None = None,\n    indent: int = 2,\n    **json_options: Any,\n) -&gt; str\n</code></pre><pre><code>create_manifest_json(\n    package: ModuleType,\n    output: Path,\n    indent: int = 2,\n    **json_options: Any,\n) -&gt; None\n</code></pre> <p>Creates a JSON manifest for the given package or module.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>ModuleType</code> <p>The package or module to create a manifest for.</p> required <code>indent</code> <code>int</code> <p>The number of spaces to use for indentation in the JSON output.</p> <code>2</code> <code>output</code> <code>Path | None</code> <p>path to the JSON file. Will be created if it does not exist.</p> <code>None</code> <code>**json_options</code> <code>Any</code> <p>Additional options to pass to the <code>model_dump_json</code> method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str | None</code> <p>The JSON manifest of the package or module.</p>"},{"location":"api/ordeq_manifest/models/","title":"models.py","text":"<p>Ordeq project data models</p>"},{"location":"api/ordeq_manifest/models/#ordeq_manifest.models.IOModel","title":"<code>IOModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model representing an IO in a project.</p>"},{"location":"api/ordeq_manifest/models/#ordeq_manifest.models.NodeModel","title":"<code>NodeModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model representing a node in a project.</p>"},{"location":"api/ordeq_manifest/models/#ordeq_manifest.models.ProjectModel","title":"<code>ProjectModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model representing a project.</p>"},{"location":"api/ordeq_manifest/models/#ordeq_manifest.models.ProjectModel.from_nodes_and_ios","title":"<code>from_nodes_and_ios(name, nodes, ios)</code>  <code>classmethod</code>","text":"<p>Create a ProjectModel from nodes and ios dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the project.</p> required <code>nodes</code> <code>set[Node]</code> <p>A dictionary of NodeModel instances.</p> required <code>ios</code> <code>Catalog</code> <p>A dictionary of IOModel instances.</p> required <p>Returns:</p> Type Description <code>ProjectModel</code> <p>A ProjectModel instance.</p>"},{"location":"api/ordeq_matplotlib/","title":"ordeq_matplotlib","text":""},{"location":"api/ordeq_matplotlib/figure/","title":"figure.py","text":""},{"location":"api/ordeq_matplotlib/figure/#ordeq_matplotlib.figure.MatplotlibFigure","title":"<code>MatplotlibFigure</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Output[Figure]</code></p> <p>IO to save matplotlib Figures.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from ordeq_matplotlib import MatplotlibFigure\n&gt;&gt;&gt; my_figure = MatplotlibFigure(\n...     path=Path(\"path/figure.pdf\")\n... )\n</code></pre>"},{"location":"api/ordeq_networkx/","title":"ordeq_networkx","text":""},{"location":"api/ordeq_networkx/networkx_gml/","title":"networkx_gml.py","text":""},{"location":"api/ordeq_networkx/networkx_gml/#ordeq_networkx.networkx_gml.NetworkxGML","title":"<code>NetworkxGML</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[Graph]</code></p> <p>IO to load from and save graph data using NetworkX's GML support. Calls <code>networkx.read_gml</code> and <code>networkx.write_gml</code> under the hood.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; import networkx as nx\n&gt;&gt;&gt; from ordeq_networkx import NetworkxGML\n&gt;&gt;&gt; random_graph = nx.erdos_renyi_graph(10, 0.5)\n&gt;&gt;&gt; my_graph = NetworkxGML(\n...     path=Path(\"graph.gml\")\n... )\n&gt;&gt;&gt; my_graph.save(random_graph)  # doctest: +SKIP\n</code></pre>"},{"location":"api/ordeq_networkx/networkx_gml/#ordeq_networkx.networkx_gml.NetworkxGML.load","title":"<code>load(**load_options)</code>","text":"<p>Load a graph from the GML file at the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>**load_options</code> <code>Any</code> <p>Additional keyword arguments passed to <code>networkx.read_gml</code>. These can be used to control how the GML file is parsed (e.g., <code>label</code>, <code>destringizer</code>).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Graph</code> <p>The loaded NetworkX graph.</p>"},{"location":"api/ordeq_networkx/networkx_gml/#ordeq_networkx.networkx_gml.NetworkxGML.save","title":"<code>save(graph, **save_options)</code>","text":"<p>Save a NetworkX graph to the GML file at the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>Graph</code> <p>The NetworkX graph to save.</p> required <code>**save_options</code> <code>Any</code> <p>Additional keyword arguments passed to <code>networkx.write_gml</code>. These can be used to control how the GML file is written (e.g., <code>stringizer</code>, <code>prettyprint</code>).</p> <code>{}</code>"},{"location":"api/ordeq_networkx/networkx_graphml/","title":"networkx_graphml.py","text":""},{"location":"api/ordeq_networkx/networkx_graphml/#ordeq_networkx.networkx_graphml.NetworkxGraphML","title":"<code>NetworkxGraphML</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[Graph]</code></p> <p>IO to load from and save graph data using NetworkX's GraphML support. Calls <code>networkx.read_graphml</code> and <code>networkx.write_graphml</code> under the hood.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; import networkx as nx\n&gt;&gt;&gt; from ordeq_networkx import NetworkxGraphML\n&gt;&gt;&gt; random_graph = nx.erdos_renyi_graph(10, 0.5)\n&gt;&gt;&gt; my_graph = NetworkxGraphML(\n...     path=Path(\"graph.graphml\")\n... )\n&gt;&gt;&gt; my_graph.save(random_graph)  # doctest: +SKIP\n</code></pre>"},{"location":"api/ordeq_networkx/networkx_graphml/#ordeq_networkx.networkx_graphml.NetworkxGraphML.load","title":"<code>load(**load_options)</code>","text":"<p>Load a graph from the GraphML file at the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>**load_options</code> <code>Any</code> <p>Additional keyword arguments passed to <code>networkx.read_graphml</code>. These can be used to control how the GraphML file is parsed.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Graph</code> <p>The loaded NetworkX graph.</p>"},{"location":"api/ordeq_networkx/networkx_graphml/#ordeq_networkx.networkx_graphml.NetworkxGraphML.save","title":"<code>save(graph, **save_options)</code>","text":"<p>Save a NetworkX graph to the GraphML file at the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>Graph</code> <p>The NetworkX graph to save.</p> required <code>**save_options</code> <code>Any</code> <p>Additional keyword arguments passed to <code>networkx.write_graphml</code>. These can be used to control how the GraphML file is written.</p> <code>{}</code>"},{"location":"api/ordeq_networkx/networkx_json/","title":"networkx_json.py","text":""},{"location":"api/ordeq_networkx/networkx_json/#ordeq_networkx.networkx_json.NetworkxJSON","title":"<code>NetworkxJSON</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[Graph]</code></p> <p>IO to load from and save graph data using NetworkX's JSON support. Calls <code>networkx.node_link_graph</code> and <code>networkx.node_link_data</code> under the hood.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; import networkx as nx\n&gt;&gt;&gt; from ordeq_networkx import NetworkxJSON\n&gt;&gt;&gt; random_graph = nx.erdos_renyi_graph(10, 0.5)\n&gt;&gt;&gt; my_graph = NetworkxJSON(\n...     path=Path(\"graph.json\")\n... )\n&gt;&gt;&gt; my_graph.save(random_graph)  # doctest: +SKIP\n</code></pre>"},{"location":"api/ordeq_networkx/networkx_json/#ordeq_networkx.networkx_json.NetworkxJSON.load","title":"<code>load(**load_options)</code>","text":"<p>Load a NetworkX graph from a JSON file using node-link format.</p> <p>Parameters:</p> Name Type Description Default <code>**load_options</code> <code>Any</code> <p>Additional keyword arguments passed to <code>json.load</code>. These can be used to control JSON decoding options.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Graph</code> <p>The loaded NetworkX graph.</p>"},{"location":"api/ordeq_networkx/networkx_json/#ordeq_networkx.networkx_json.NetworkxJSON.save","title":"<code>save(graph, **save_options)</code>","text":"<p>Save a NetworkX graph to a JSON file using node-link format.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>Graph</code> <p>The NetworkX graph to save.</p> required <code>**save_options</code> <code>Any</code> <p>Additional keyword arguments passed to <code>json.dump</code>. These can be used to control JSON encoding options.</p> <code>{}</code>"},{"location":"api/ordeq_numpy/","title":"ordeq_numpy","text":""},{"location":"api/ordeq_numpy/binary_array/","title":"binary_array.py","text":""},{"location":"api/ordeq_numpy/binary_array/#ordeq_numpy.binary_array.NumpyBinary","title":"<code>NumpyBinary</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[ndarray]</code></p> <p>IO to load from and save binary numpy arrays.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from ordeq_numpy import NumpyBinary\n&gt;&gt;&gt; MyArray = NumpyBinary(\n...     path=Path(\"path/to.npy\")\n... )\n</code></pre>"},{"location":"api/ordeq_numpy/binary_array/#ordeq_numpy.binary_array.NumpyBinary.load","title":"<code>load(**load_options)</code>","text":"<p>Load numpy array with optional parameters.</p> <p>Parameters:</p> Name Type Description Default <code>**load_options</code> <code>Any</code> <p>Arguments passed to np.load() (e.g., mmap_mode, allow_pickle, max_header_size)</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Numpy array</p>"},{"location":"api/ordeq_numpy/binary_array/#ordeq_numpy.binary_array.NumpyBinary.save","title":"<code>save(array, **save_options)</code>","text":"<p>Save numpy array with optional parameters.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>The array to save</p> required <code>**save_options</code> <code>Any</code> <p>Arguments passed to np.save() (e.g., allow_pickle, fix_imports)</p> <code>{}</code>"},{"location":"api/ordeq_numpy/text_array/","title":"text_array.py","text":""},{"location":"api/ordeq_numpy/text_array/#ordeq_numpy.text_array.NumpyText","title":"<code>NumpyText</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[ndarray]</code></p> <p>IO to load from and save plain text numpy arrays.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from ordeq_numpy import NumpyText\n&gt;&gt;&gt; MyArray = NumpyText(\n...     path=Path(\"path/to.txt\")\n... )\n</code></pre>"},{"location":"api/ordeq_numpy/text_array/#ordeq_numpy.text_array.NumpyText.load","title":"<code>load(**load_options)</code>","text":"<p>Load numpy array with optional parameters.</p> <p>Parameters:</p> Name Type Description Default <code>**load_options</code> <code>Any</code> <p>Arguments passed to np.loadtxt() (e.g., dtype, delimiter, skiprows, max_rows)</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Numpy array</p>"},{"location":"api/ordeq_numpy/text_array/#ordeq_numpy.text_array.NumpyText.save","title":"<code>save(array, **save_options)</code>","text":"<p>Save numpy array with optional parameters.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>The array to save</p> required <code>**save_options</code> <code>Any</code> <p>Arguments passed to np.savetxt() (e.g., fmt, delimiter, header, footer)</p> <code>{}</code>"},{"location":"api/ordeq_pandas/","title":"ordeq_pandas","text":""},{"location":"api/ordeq_pandas/csv/","title":"csv.py","text":""},{"location":"api/ordeq_pandas/csv/#ordeq_pandas.csv.PandasCSV","title":"<code>PandasCSV</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[DataFrame]</code></p> <p>IO to load from and save to CSV data using Pandas. Calls <code>pd.read_csv</code> and <code>pd.write_csv</code> under the hood.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from ordeq_pandas import PandasCSV\n&gt;&gt;&gt; csv = PandasCSV(\n...     path=Path(\"path/to.csv\")\n... ).load(header=\"infer\")  # doctest: +SKIP\n</code></pre> <p>Load behaviour is configured by <code>with_load_options</code>:</p> <pre><code>&gt;&gt;&gt; csv = PandasCSV(\n...     path=Path(\"path/to.csv\")\n... ).with_load_options(header=\"infer\")\n</code></pre> <p>Save behaviour is configured by <code>with_save_options</code>:</p> <pre><code>&gt;&gt;&gt; csv = PandasCSV(\n...     path=Path(\"path/to.csv\"),\n... ).with_save_options(header=True)\n</code></pre>"},{"location":"api/ordeq_pandas/excel/","title":"excel.py","text":""},{"location":"api/ordeq_pandas/excel/#ordeq_pandas.excel.PandasExcel","title":"<code>PandasExcel</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[DataFrame]</code></p> <p>IO to load from and save to Excel data using Pandas. Calls <code>pd.read_excel</code> and <code>pd.to_excel</code> under the hood.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from ordeq_pandas import PandasExcel\n&gt;&gt;&gt; xlsx = PandasExcel(\n...     path=Path(\"path/to.xlsx\")\n... ).load(usecols=\"A:C\")  # doctest: +SKIP\n</code></pre> <p>Load behaviour is configured by <code>with_load_options</code>:</p> <pre><code>&gt;&gt;&gt; xlsx = (\n...     PandasExcel(\n...         path=Path(\"path/to.xlsx\")\n...     )\n...     .with_load_options(usecols=\"A:C\")\n... )\n</code></pre>"},{"location":"api/ordeq_pandas/parquet/","title":"parquet.py","text":""},{"location":"api/ordeq_pandas/parquet/#ordeq_pandas.parquet.PandasParquet","title":"<code>PandasParquet</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[DataFrame]</code></p> <p>IO to load from and save to PARQUET data using Pandas. Calls <code>pd.read_parquet</code> and <code>pd.write_parquet</code> under the hood.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from ordeq_pandas import PandasParquet\n&gt;&gt;&gt; parquet = PandasParquet(\n...     path=Path(\"path/to.parquet\")\n... )\n</code></pre>"},{"location":"api/ordeq_polars/","title":"ordeq_polars","text":""},{"location":"api/ordeq_polars/eager/csv/","title":"csv.py","text":""},{"location":"api/ordeq_polars/eager/csv/#ordeq_polars.eager.csv.PolarsEagerCSV","title":"<code>PolarsEagerCSV</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[DataFrame]</code></p> <p>IO for loading and saving CSV using Polars.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from ordeq_polars import PolarsEagerCSV\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; csv = PolarsEagerCSV(\n...     path=Path(\"to.csv\")\n... ).with_load_options(\n...     has_header=True\n... )\n</code></pre>"},{"location":"api/ordeq_polars/eager/excel/","title":"excel.py","text":""},{"location":"api/ordeq_polars/eager/excel/#ordeq_polars.eager.excel.PolarsEagerExcel","title":"<code>PolarsEagerExcel</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[DataFrame]</code></p> <p>IO for loading and saving Excel using Polars.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from ordeq_polars import PolarsEagerExcel\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; xlsx = PolarsEagerExcel(\n...     path=Path(\"to.xlsx\")\n... ).with_load_options(\n...     has_header=True\n... )\n</code></pre>"},{"location":"api/ordeq_polars/eager/parquet/","title":"parquet.py","text":""},{"location":"api/ordeq_polars/eager/parquet/#ordeq_polars.eager.parquet.PolarsEagerParquet","title":"<code>PolarsEagerParquet</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[DataFrame]</code></p> <p>IO for loading and saving Parquet using Polars.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from ordeq_polars import PolarsEagerParquet\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; parquet = PolarsEagerParquet(\n...     path=Path(\"to.parquet\")\n... ).with_load_options(\n...     n_rows=1_000\n... )\n</code></pre>"},{"location":"api/ordeq_polars/lazy/csv/","title":"csv.py","text":""},{"location":"api/ordeq_polars/lazy/csv/#ordeq_polars.lazy.csv.PolarsLazyCSV","title":"<code>PolarsLazyCSV</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[LazyFrame]</code></p> <p>IO for loading and saving CSV lazily using Polars.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from ordeq_polars import PolarsLazyCSV\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; csv = PolarsLazyCSV(\n...     path=Path(\"to.csv\")\n... ).with_load_options(\n...     has_header=True\n... )\n</code></pre>"},{"location":"api/ordeq_polars/lazy/parquet/","title":"parquet.py","text":""},{"location":"api/ordeq_polars/lazy/parquet/#ordeq_polars.lazy.parquet.PolarsLazyParquet","title":"<code>PolarsLazyParquet</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[LazyFrame]</code></p> <p>IO for loading and saving Parquet lazily using Polars.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from ordeq_polars import PolarsLazyParquet\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; parquet = PolarsLazyParquet(\n...     path=Path(\"to.parquet\")\n... ).with_load_options(\n...     n_rows=1_000\n... )\n</code></pre>"},{"location":"api/ordeq_pydantic/","title":"ordeq_pydantic","text":""},{"location":"api/ordeq_pydantic/json/","title":"json.py","text":""},{"location":"api/ordeq_pydantic/json/#ordeq_pydantic.json.PydanticJSON","title":"<code>PydanticJSON</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[BaseModel]</code></p> <p>IO to load and save Pydantic models to JSON</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from ordeq_pydantic import PydanticJSON\n&gt;&gt;&gt; from pydantic import BaseModel\n\n&gt;&gt;&gt; class MyModel(BaseModel):\n...     hello: str\n...     world: str\n\n&gt;&gt;&gt; dataset = PydanticJSON(\n...     path=Path(\"path/to.json\"),\n...     model_type=MyModel\n... )\n</code></pre>"},{"location":"api/ordeq_pydantic/json/#ordeq_pydantic.json.PydanticJSON.load","title":"<code>load()</code>","text":"<p>Load the Pydantic model from the JSON file.</p> <p>Returns:</p> Type Description <code>BaseModel</code> <p>The loaded Pydantic model.</p>"},{"location":"api/ordeq_pydantic/json/#ordeq_pydantic.json.PydanticJSON.save","title":"<code>save(model)</code>","text":"<p>Save the Pydantic model to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseModel</code> <p>The Pydantic model to save.</p> required"},{"location":"api/ordeq_pydantic/model/","title":"model.py","text":""},{"location":"api/ordeq_pydantic/model/#ordeq_pydantic.model.PydanticModel","title":"<code>PydanticModel</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[BaseModel]</code></p> <p>IO to load and save Pydantic models from/to any IO that handles     dictionaries.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from ordeq_pydantic import PydanticModel\n&gt;&gt;&gt; from pydantic import BaseModel\n&gt;&gt;&gt; from ordeq_yaml import YAML\n\n&gt;&gt;&gt; class MyModel(BaseModel):\n...     hello: str\n...     world: str\n\n&gt;&gt;&gt; dataset = PydanticModel(\n...     io=YAML(path=Path(\"path/to.yaml\")),\n...     model_type=MyModel\n... )\n</code></pre> <p>Instead of using:</p> <pre><code>from pathlib import Path\nfrom ordeq_pydantic import PydanticModel\nfrom ordeq_json import JSON\n\nmy_model = PydanticModel(io=JSON(path=Path(...)), model_type=MyModel)\n</code></pre> <p>you can also use:</p> <pre><code>from pathlib import Path\nfrom ordeq_pydantic import PydanticJSON\n\nPydanticJSON(path=Path(...), model_type=MyModel)\n</code></pre> <p>This uses the Pydantic JSON implementation which is more efficient for JSON files.</p>"},{"location":"api/ordeq_pydantic/model/#ordeq_pydantic.model.PydanticModel.load","title":"<code>load(**load_options)</code>","text":"<p>Load the Pydantic model from the underlying IO.</p> <p>Parameters:</p> Name Type Description Default <code>**load_options</code> <code>Any</code> <p>Options to pass to the Pydantic model validation.</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseModel</code> <p>The loaded Pydantic model.</p>"},{"location":"api/ordeq_pydantic/model/#ordeq_pydantic.model.PydanticModel.save","title":"<code>save(model, **save_options)</code>","text":"<p>Save the Pydantic model to the underlying IO.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseModel</code> <p>The Pydantic model to save.</p> required <code>**save_options</code> <code>Any</code> <p>Options to pass to the Pydantic model dump.</p> <code>{}</code>"},{"location":"api/ordeq_pymupdf/","title":"ordeq_pymupdf","text":""},{"location":"api/ordeq_pymupdf/pdf_file/","title":"pdf_file.py","text":""},{"location":"api/ordeq_pymupdf/pdf_file/#ordeq_pymupdf.pdf_file.PymupdfFile","title":"<code>PymupdfFile</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[Document]</code></p> <p>IO to load from and save PDF files using Pymupdf.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from ordeq_pymupdf import PymupdfFile\n&gt;&gt;&gt; MyPDF = PymupdfFile(\n...     path=Path(\"path/to.index\")\n... )\n</code></pre>"},{"location":"api/ordeq_pymupdf/pdf_file/#ordeq_pymupdf.pdf_file.PymupdfFile.load","title":"<code>load()</code>","text":"<p>Read a PDF file and open the pymypdf Document.</p> <p>Returns:</p> Type Description <code>Document</code> <p><code>pymupdf</code> Document</p>"},{"location":"api/ordeq_pymupdf/pdf_file/#ordeq_pymupdf.pdf_file.PymupdfFile.save","title":"<code>save(document)</code>","text":"<p>Save a pymypdf Document to a file.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>Document</code> <p>document to save</p> required"},{"location":"api/ordeq_pyproject/","title":"ordeq_pyproject","text":""},{"location":"api/ordeq_pyproject/pyproject/","title":"pyproject.py","text":""},{"location":"api/ordeq_pyproject/pyproject/#ordeq_pyproject.pyproject.Pyproject","title":"<code>Pyproject</code>  <code>dataclass</code>","text":"<p>               Bases: <code>TOMLInput</code></p> <p>IO for loading a pyproject.toml section.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from ordeq_pyproject import Pyproject\n&gt;&gt;&gt; pyproject = Pyproject(\n...     path=Path(\"pyproject.toml\"), section=\"tool.my_tool\"\n... )\n&gt;&gt;&gt; data = pyproject.load() # doctest: +SKIP\n</code></pre>"},{"location":"api/ordeq_pyproject/pyproject/#ordeq_pyproject.pyproject.Pyproject.load","title":"<code>load(**load_options)</code>","text":"<p>Load the specified section from the pyproject.toml file.</p> <p>Parameters:</p> Name Type Description Default <code>**load_options</code> <code>Any</code> <p>Additional options to pass to the TOML loader.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>A dictionary representing the contents of the specified section in the pyproject.toml file.</p>"},{"location":"api/ordeq_requests/","title":"ordeq_requests","text":""},{"location":"api/ordeq_requests/io/response/","title":"response.py","text":""},{"location":"api/ordeq_requests/io/response/#ordeq_requests.io.response.Response","title":"<code>Response</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Input[T]</code></p> <p>IO to load data from an API using <code>requests</code>.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from ordeq_requests import Response\n&gt;&gt;&gt; User = ResponseContent(\n...     url=\"https://jsonplaceholder.typicode.com/users/1\"\n... )\n&gt;&gt;&gt; User.load()  # doctest: +SKIP\nb'{\"id\":1,\"name\":\"Leanne Graham\", ...}'  # type bytes\n</code></pre> <p>Use <code>ResponseText</code> or <code>ResponseJSON</code> to parse as <code>str</code> or <code>dict</code>:</p> <pre><code>&gt;&gt;&gt; from ordeq_requests import ResponseText\n&gt;&gt;&gt; UserText = ResponseText(\n...     url=\"https://jsonplaceholder.typicode.com/users/1\"\n... )\n&gt;&gt;&gt; UserText.load()  # doctest: +SKIP\n'{\"id\":1,\"name\":\"Leanne Graham\", ...}'  # type str\n&gt;&gt;&gt; from ordeq_requests import ResponseJSON\n&gt;&gt;&gt; UserJSON = ResponseJSON(\n...     url=\"https://jsonplaceholder.typicode.com/users/1\"\n... )\n&gt;&gt;&gt; UserJSON.load()  # doctest: +SKIP\n{'id': 1, 'name': 'Leanne Graham', ...}  # type dict\n</code></pre> <p>Example in a node:</p> <pre><code>&gt;&gt;&gt; from ordeq import node\n&gt;&gt;&gt; from ordeq_files import JSON\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; @node(\n...     inputs=UserJSON,\n...     outputs=JSON(path=Path(\"location.json\"))\n... )\n... def parse_user_location(user: dict) -&gt; dict:\n...     return {\n...         \"id\": user[\"id\"],\n...         \"lat\": user[\"geo\"][\"lat\"],\n...         \"lng\": user[\"geo\"][\"lng\"]\n...     }\n</code></pre> <p>By default, each <code>Response</code> instance will create a new <code>requests.Session</code>. To reuse a session across multiple datasets, pass it as attribute on init:</p> <pre><code>&gt;&gt;&gt; from requests.auth import HTTPBasicAuth\n&gt;&gt;&gt; session = Session()\n&gt;&gt;&gt; RequestWithCookies = ResponseJSON(\n...     url=\"https://httpbin.org/cookies/set/cookie/123\",\n...     method=\"GET\",\n...     session=session\n... )\n&gt;&gt;&gt; RequestWithCookies.load()  # doctest: +SKIP\n{'cookies': {'cookie': '123'}}\n&gt;&gt;&gt; session.cookies.items()  # doctest: +SKIP\n[('cookie', '123')]\n&gt;&gt;&gt; NextRequestWithCookies = ResponseJSON(\n...     url=\"https://jsonplaceholder.typicode.com/users/2\",\n...     method=\"GET\",\n...     session=session  # reuse session and cookies in next request\n... )\n</code></pre> <p>Authentication can also be set on the <code>session</code> attribute:</p> <pre><code>&gt;&gt;&gt; from requests.auth import HTTPBasicAuth\n&gt;&gt;&gt; session = Session()\n&gt;&gt;&gt; session.auth = HTTPBasicAuth('user', 'password')\n&gt;&gt;&gt; RequestWithAuth = ResponseText(\n...     url=\"https://httpbin.org/headers\",\n...     method=\"GET\",\n...     session=session\n... )\n&gt;&gt;&gt; RequestWithAuth.load()  # doctest: +SKIP\n{\"headers\":{\"Accept\":\"*/*\", ..., \"Authorization\": \"Basic ******\"}}\n</code></pre> <p>Similar patterns apply to other configuration like certificates and proxies. See 1 for more info.</p>"},{"location":"api/ordeq_sentence_transformers/","title":"ordeq_sentence_transformers","text":""},{"location":"api/ordeq_sentence_transformers/sentence_transformer/","title":"sentence_transformer.py","text":""},{"location":"api/ordeq_sentence_transformers/sentence_transformer/#ordeq_sentence_transformers.sentence_transformer.SentenceTransformer","title":"<code>SentenceTransformer</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Input[SentenceTransformer]</code></p> <p>Input for a SentenceTransformer model from the <code>sentence-transformers</code> library.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from ordeq_sentence_transformers import SentenceTransformer\n&gt;&gt;&gt; huggingface_token = \"your_hf_token_here\"\n&gt;&gt;&gt; model = SentenceTransformer(\n...     'all-mpnet-base-v2'\n... ).with_load_options(\n...     token=huggingface_token\n... )\n&gt;&gt;&gt; st_model = model.load()  # doctest: +SKIP\n</code></pre> <p>For more details, see the <code>sentence-transformers</code> documentation.</p>"},{"location":"api/ordeq_spark/","title":"ordeq_spark","text":""},{"location":"api/ordeq_spark/utils/","title":"utils.py","text":""},{"location":"api/ordeq_spark/utils/#ordeq_spark.utils.apply_schema","title":"<code>apply_schema(df, schema)</code>","text":"<p>Applies a schema to the DataFrame, meaning: - select only those columns in the schema from the DataFrame - cast these to the types specified in the schema</p> <p>Note that this method does not check for nullability in the schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame</p> required <code>schema</code> <code>StructType</code> <p>StructType (schema)</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>the DataFrame with schema applied</p>"},{"location":"api/ordeq_spark/utils/#ordeq_spark.utils.create_schema","title":"<code>create_schema(schema)</code>","text":"<p>Utility method that creates a Spark StructType (or, schema) from tuple inputs, without need to import and write StructType &amp; StructField</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>tuple[tuple[str, DataType, bool], ...]</code> <p>tuple of schema inputs</p> required <p>Returns:</p> Type Description <code>StructType</code> <p>StructType</p>"},{"location":"api/ordeq_spark/utils/#ordeq_spark.utils.get_spark_session","title":"<code>get_spark_session()</code>","text":"<p>Helper to get the SparkSession</p> <p>Returns:</p> Type Description <code>SparkSession</code> <p>the spark session object</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>when the spark session is not active</p>"},{"location":"api/ordeq_spark/hooks/spark_explain/","title":"spark_explain.py","text":""},{"location":"api/ordeq_spark/hooks/spark_explain/#ordeq_spark.hooks.spark_explain.SparkExplainHook","title":"<code>SparkExplainHook</code>","text":"<p>               Bases: <code>OutputHook[DataFrame]</code></p> <p>Hook to print the Spark execution plan before saving a DataFrame.</p>"},{"location":"api/ordeq_spark/hooks/spark_job_group/","title":"spark_job_group.py","text":""},{"location":"api/ordeq_spark/hooks/spark_job_group/#ordeq_spark.hooks.spark_job_group.SparkJobGroupHook","title":"<code>SparkJobGroupHook</code>","text":"<p>               Bases: <code>NodeHook</code></p> <p>Node hook that sets the Spark job group to the node name. Please make sure the Spark session is initialized before using this hook.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from ordeq import node, run\n&gt;&gt;&gt; from ordeq_spark import SparkHiveTable\n&gt;&gt;&gt; from pyspark.sql import DataFrame\n\n&gt;&gt;&gt; @node(\n...     inputs=SparkHiveTable(table=\"tables.a\"),\n...     outputs=SparkHiveTable(table=\"tables.b\"),\n... )\n... def append(a: DataFrame) -&gt; DataFrame:\n...     return a.union(a)\n\n&gt;&gt;&gt; run(append, hooks=[SparkJobGroupHook()]) # doctest: +SKIP\n</code></pre>"},{"location":"api/ordeq_spark/hooks/spark_job_group/#ordeq_spark.hooks.spark_job_group.SparkJobGroupHook.before_node_run","title":"<code>before_node_run(node)</code>","text":"<p>Sets the node name as the job group in the Spark context. This makes the history server a lot easier to use.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>the node</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the Spark session is not active</p>"},{"location":"api/ordeq_spark/io/dataframe/","title":"dataframe.py","text":""},{"location":"api/ordeq_spark/io/dataframe/#ordeq_spark.io.dataframe.SparkDataFrame","title":"<code>SparkDataFrame</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Input[DataFrame]</code></p> <p>Allows a Spark DataFrame to be hard-coded in python. This is suitable for small tables such as very simple dimension tables that are unlikely to change. It may also be useful in unit testing.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql.types import *\n&gt;&gt;&gt; from ordeq_spark import SparkDataFrame\n&gt;&gt;&gt; df = SparkDataFrame(\n...     schema=StructType([\n...         StructField(\"year\", IntegerType()),\n...         StructField(\"datafile\", StringType()),\n...     ]),\n...     data=(\n...         (2022, \"file_2022.xlsx\"),\n...         (2023, \"file_2023.xlsx\"),\n...         (2024, \"file_2023.xlsx\"),\n...     )\n... )\n</code></pre>"},{"location":"api/ordeq_spark/io/jdbc/","title":"jdbc.py","text":""},{"location":"api/ordeq_spark/io/jdbc/#ordeq_spark.io.jdbc.SparkJDBC","title":"<code>SparkJDBC</code>  <code>dataclass</code>","text":"<p>IO for loading from and saving to a database with Spark and JDBC.</p> <p>The JDBC driver need to be added to your Spark application. For instance, to connect to Microsoft SQL Server, pass <code>com.microsoft.sqlserver:mssql-jdbc:12.8.1.jre11</code> as extra JAR.</p> <p>When saving, the types of the DataFrame should be compatible with the target table. More info [1].</p> <p>[1] https://spark.apache.org/docs/4.0.0-preview1/sql-data-sources-jdbc.html</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from ordeq_spark import SparkJDBCTable\n&gt;&gt;&gt; SparkJDBCTable(\n...     table=\"schema.table\",\n...     driver=\"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n...     url=(\n...         \"jdbc:sqlserver://host:52840\"\n...         \"databaseName=tempdb;\"\n...         \"user=SA;\"\n...         \"password=1Secure*Password1;\"\n...     )\n...  ).load()  # doctest: +SKIP\n</code></pre> <p>Use <code>SparkJDBCQuery</code> to execute a custom query on load:</p> <pre><code>&gt;&gt;&gt; from ordeq_spark import SparkJDBCQuery\n&gt;&gt;&gt; SparkJDBCQuery(\n...     query=\"SELECT 1;\",\n...     driver=\"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n...     url=(\n...         \"jdbc:sqlserver://host:52840\"\n...         \"databaseName=tempdb;\"\n...         \"user=SA;\"\n...         \"password=1Secure*Password1;\"\n...     )\n...  ).load()  # doctest: +SKIP\n</code></pre>"},{"location":"api/ordeq_spark/io/session/","title":"session.py","text":""},{"location":"api/ordeq_spark/io/session/#ordeq_spark.io.session.SparkSession","title":"<code>SparkSession</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Input[SparkSession]</code></p> <p>Input representing the active Spark session. Useful for accessing the active Spark session in nodes.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from ordeq_spark.io.session import SparkSession\n&gt;&gt;&gt; spark_session = SparkSession()\n&gt;&gt;&gt; spark = spark_session.load()  # doctest: +SKIP\n&gt;&gt;&gt; print(spark.version)  # doctest: +SKIP\n3.3.1\n</code></pre> <p>Example in a node:</p> <pre><code>&gt;&gt;&gt; from ordeq import node\n&gt;&gt;&gt; from ordeq_common import Literal\n&gt;&gt;&gt; items = Literal({'id': [1, 2, 3], 'value': ['a', 'b', 'c']})\n&gt;&gt;&gt; @node(\n...     inputs=[items, spark_session],\n...     outputs=[],\n... )\n... def convert_to_df(\n...     data: dict, spark: pyspark.sql.SparkSession\n... ) -&gt; pyspark.sql.DataFrame:\n...     return spark.createDataFrame(data)\n</code></pre>"},{"location":"api/ordeq_spark/io/session/#ordeq_spark.io.session.SparkSession.load","title":"<code>load()</code>","text":"<p>Gets the active SparkSession</p> <p>Returns:</p> Type Description <code>SparkSession</code> <p>pyspark.sql.SparkSession: The Spark session.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there is no active Spark session.</p>"},{"location":"api/ordeq_spark/io/types/","title":"types.py","text":""},{"location":"api/ordeq_spark/io/files/csv/","title":"csv.py","text":""},{"location":"api/ordeq_spark/io/files/csv/#ordeq_spark.io.files.csv.SparkCSV","title":"<code>SparkCSV</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[DataFrame]</code></p> <p>IO for loading and saving CSV using Spark.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from ordeq_spark import SparkCSV\n&gt;&gt;&gt; csv = SparkCSV(\n...     path=\"to.csv\"\n... ).with_load_options(\n...     infer_schema=True\n... )\n</code></pre> <p>By default, Spark creates a directory on save. Use <code>single_file</code> if you want to write to a file instead:</p> <pre><code>&gt;&gt;&gt; from ordeq_spark import SparkCSV\n&gt;&gt;&gt; csv = SparkCSV(\n...     path=\"to.json\"\n... ).with_load_options(single_file=True)\n</code></pre>"},{"location":"api/ordeq_spark/io/files/json/","title":"json.py","text":""},{"location":"api/ordeq_spark/io/files/json/#ordeq_spark.io.files.json.SparkJSON","title":"<code>SparkJSON</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[DataFrame]</code></p> <p>IO for loading and saving JSON using Spark.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from ordeq_spark import SparkJSON\n&gt;&gt;&gt; json = SparkJSON(\n...     path=\"to.json\"\n... )\n</code></pre> <p>By default, Spark creates a directory on save. Use <code>single_file</code> if you want to write to a file instead:</p> <pre><code>&gt;&gt;&gt; from ordeq_spark import SparkJSON\n&gt;&gt;&gt; json = SparkJSON(\n...     path=\"to.json\"\n... ).with_load_options(single_file=True)\n</code></pre>"},{"location":"api/ordeq_spark/io/files/utils/","title":"utils.py","text":""},{"location":"api/ordeq_spark/io/tables/global_temp_view/","title":"global_temp_view.py","text":""},{"location":"api/ordeq_spark/io/tables/global_temp_view/#ordeq_spark.io.tables.global_temp_view.SparkGlobalTempView","title":"<code>SparkGlobalTempView</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[DataFrame]</code></p> <p>IO for reading from and writing to Spark global temporary views.</p> <p>Examples:</p> <p>Create and save a DataFrame to a global temp view:</p> <pre><code>&gt;&gt;&gt; from ordeq_spark import SparkGlobalTempView\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()  # doctest: +SKIP\n&gt;&gt;&gt; view = SparkGlobalTempView(table=\"my_temp_view\")\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     [(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"]\n... )  # doctest: +SKIP\n&gt;&gt;&gt; view.save(df, mode=\"createOrReplace\")  # doctest: +SKIP\n</code></pre> <p>Load the DataFrame from the global temp view:</p> <pre><code>&gt;&gt;&gt; loaded_df = view.load()  # doctest: +SKIP\n&gt;&gt;&gt; loaded_df.show()  # doctest: +SKIP\n</code></pre>"},{"location":"api/ordeq_spark/io/tables/hive/","title":"hive.py","text":""},{"location":"api/ordeq_spark/io/tables/hive/#ordeq_spark.io.tables.hive.SparkHiveTable","title":"<code>SparkHiveTable</code>  <code>dataclass</code>","text":"<p>               Bases: <code>SparkTable</code>, <code>IO[DataFrame]</code></p> <p>IO for reading from and writing to Hive tables in Spark.</p> <p>Examples:</p> <p>Save a DataFrame to a Hive table:</p> <pre><code>&gt;&gt;&gt; from ordeq_spark import SparkHiveTable\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; spark = SparkSession.builder.enableHiveSupport().getOrCreate()  # doctest: +SKIP\n&gt;&gt;&gt; table = SparkHiveTable(table=\"my_hive_table\")\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     [(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"]\n... )  # doctest: +SKIP\n&gt;&gt;&gt; table.save(df, format=\"parquet\", mode=\"overwrite\")  # doctest: +SKIP\n</code></pre> <p>If <code>schema</code> is provided, it will be applied before save and after load.</p>"},{"location":"api/ordeq_spark/io/tables/iceberg/","title":"iceberg.py","text":""},{"location":"api/ordeq_spark/io/tables/iceberg/#ordeq_spark.io.tables.iceberg.SparkIcebergTable","title":"<code>SparkIcebergTable</code>  <code>dataclass</code>","text":"<p>               Bases: <code>SparkTable</code>, <code>IO[DataFrame]</code></p> <p>IO used to load &amp; save Iceberg tables using Spark.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from ordeq_spark import (\n...     SparkIcebergTable\n... )\n&gt;&gt;&gt; from pyspark.sql.types import StructType, StructField, IntegerType\n&gt;&gt;&gt; MyTable = SparkIcebergTable(\n...     table=\"my.iceberg.table\",\n...     schema=StructType(\n...         fields=[\n...             StructField(\"id\", IntegerType()),\n...             StructField(\"amount\", IntegerType()),\n...         ]\n...     )\n... )\n\n&gt;&gt;&gt; import pyspark.sql.functions as F\n&gt;&gt;&gt; MyPartitionedTable = (\n...     SparkIcebergTable(\n...         table=\"my.iceberg.table\"\n...     ).with_save_options(\n...         mode=\"overwritePartitions\",\n...         partition_by=(\n...             (\"colour\",),\n...             (F.years, \"dt\"),\n...         )\n...     )\n... )\n</code></pre> <p>If <code>schema</code> is provided, it will be applied before save and after load.</p> <p>Saving is idempotent: if the target table does not exist, it is created with the configuration set in the save options.</p> <p>Table properties can be specified on the <code>properties</code> attribute. Currently, the properties will be taken into account on write only.</p> <pre><code>&gt;&gt;&gt; TableWithProperties = SparkIcebergTable(\n...     table=\"my.iceberg.table\",\n...     properties=(\n...         ('read.split.target-size', '268435456'),\n...         ('write.parquet.row-group-size-bytes', '268435456'),\n...     )\n... )\n</code></pre> <p>Currently only supports a subset of Iceberg writes. More info 1:</p>"},{"location":"api/ordeq_spark/io/tables/iceberg/#ordeq_spark.io.tables.iceberg.SparkIcebergTable.save","title":"<code>save(df, mode='overwritePartitions', partition_by=())</code>","text":"<p>Saves the DataFrame to the Iceberg table.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to save</p> required <code>mode</code> <code>SparkIcebergWriteMode</code> <p>write mode, one of: - \"create\" - create the table, fail if it exists - \"createOrReplace\" - create the table, replace if it exists - \"overwrite\" - overwrite the table - \"overwritePartitions\" - overwrite partitions of the table - \"append\" - append to the table</p> <code>'overwritePartitions'</code> <code>partition_by</code> <code>tuple[tuple[Callable[[str], Column], str] | tuple[str], ...]</code> <p>tuple of columns to partition by, each element can be - a tuple of a function and a column name, e.g. (F.years, \"dt\") - a single column name as a string, e.g. \"colour\"</p> <code>()</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>if partition_by is not a tuple of tuples</p> <code>ValueError</code> <p>if mode is not one of the supported modes</p> <code>RuntimeError</code> <p>if the Spark captured exception cannot be parsed</p> <code>CapturedException</code> <p>if there is an error during the write operation</p>"},{"location":"api/ordeq_spark/io/tables/table/","title":"table.py","text":""},{"location":"api/ordeq_spark/io/tables/temp_view/","title":"temp_view.py","text":""},{"location":"api/ordeq_spark/io/tables/temp_view/#ordeq_spark.io.tables.temp_view.SparkTempView","title":"<code>SparkTempView</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[DataFrame]</code></p> <p>IO for reading from and writing to Spark temporary views.</p> <p>Examples:</p> <p>Create and save a DataFrame to a temp view:</p> <pre><code>&gt;&gt;&gt; from ordeq_spark import SparkTempView\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()  # doctest: +SKIP\n&gt;&gt;&gt; view = SparkTempView(table=\"my_temp_view\")\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     [(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"]\n... )  # doctest: +SKIP\n&gt;&gt;&gt; view.save(df, mode=\"createOrReplace\")  # doctest: +SKIP\n</code></pre> <p>Load the DataFrame from the temp view:</p> <pre><code>&gt;&gt;&gt; loaded_df = view.load()  # doctest: +SKIP\n&gt;&gt;&gt; loaded_df.show()  # doctest: +SKIP\n</code></pre>"},{"location":"api/ordeq_test_utils/","title":"ordeq_test_utils","text":""},{"location":"api/ordeq_test_utils/snapshot/","title":"snapshot.py","text":""},{"location":"api/ordeq_test_utils/snapshot/#ordeq_test_utils.snapshot.append_packages_dir_to_sys_path","title":"<code>append_packages_dir_to_sys_path(packages_dir)</code>","text":"<p>Append the packages directory to <code>sys.path</code>.</p> <p>This allows us to import the example packages at test time. Cleanup is performed after use to ensure a clean state for each test.</p> <p>Parameters:</p> Name Type Description Default <code>packages_dir</code> <code>Path</code> <p>The path to the packages directory.</p> required"},{"location":"api/ordeq_test_utils/snapshot/#ordeq_test_utils.snapshot.capture_module","title":"<code>capture_module(file_path, caplog, capsys, recwarn)</code>","text":"<p>Capture the output, logging, errors, and typing feedback from running a Python module.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>The path to the Python file to run.</p> required <code>caplog</code> <code>LogCaptureFixture</code> <p>The pytest caplog fixture for capturing logs.</p> required <code>capsys</code> <code>CaptureFixture</code> <p>The pytest capsys fixture for capturing stdout/stderr.</p> required <code>recwarn</code> <code>WarningsRecorder</code> <p>The pytest recwarn fixture for capturing warnings.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The normalized captured output as a string.</p>"},{"location":"api/ordeq_test_utils/snapshot/#ordeq_test_utils.snapshot.compare","title":"<code>compare(captured, expected)</code>","text":"<p>Return a unified diff between captured and expected strings.</p> <p>Parameters:</p> Name Type Description Default <code>captured</code> <code>str</code> <p>The actual captured output.</p> required <code>expected</code> <code>str</code> <p>The expected output.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A unified diff string showing the differences.</p>"},{"location":"api/ordeq_test_utils/snapshot/#ordeq_test_utils.snapshot.compare_resources_against_snapshots","title":"<code>compare_resources_against_snapshots(file_path, snapshot_path, caplog, capsys, recwarn)</code>","text":"<p>Compare the output of a resource file against its snapshot, updating the snapshot if different.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>The path to the resource file to test.</p> required <code>snapshot_path</code> <code>Path</code> <p>The path to the snapshot file to compare against.</p> required <code>caplog</code> <code>LogCaptureFixture</code> <p>The pytest caplog fixture for capturing logs.</p> required <code>capsys</code> <code>CaptureFixture</code> <p>The pytest capsys fixture for capturing stdout/stderr.</p> required <code>recwarn</code> <code>WarningsRecorder</code> <p>The pytest recwarn fixture for capturing warnings.</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>A unified diff string if the outputs differ, otherwise None.</p>"},{"location":"api/ordeq_test_utils/snapshot/#ordeq_test_utils.snapshot.make_output_invariant","title":"<code>make_output_invariant(output)</code>","text":"<p>Normalize output to be invariant to UUIDs, object hashes, and OS-specific paths.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The captured output string to normalize.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The normalized output string.</p>"},{"location":"api/ordeq_test_utils/snapshot/#ordeq_test_utils.snapshot.replace_object_hashes","title":"<code>replace_object_hashes(text)</code>","text":"<p>Replace object hashes (e.g., 0x103308890) in the text with sequential placeholders.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input string to process.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The text with object hashes replaced by HASH1, HASH2, etc.</p>"},{"location":"api/ordeq_test_utils/snapshot/#ordeq_test_utils.snapshot.replace_uuid4","title":"<code>replace_uuid4(text)</code>","text":"<p>Replace UUID4 strings in the text with sequential placeholders.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input string to process.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The text with UUID4 strings replaced by ID1, ID2, etc.</p>"},{"location":"api/ordeq_test_utils/snapshot/#ordeq_test_utils.snapshot.run_module","title":"<code>run_module(file_path)</code>","text":"<p>Dynamically import and run a Python module from a file path.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>The path to the Python file to import and run.</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>None if the module runs successfully, otherwise a string describing</p> <code>str | None</code> <p>the exception.</p>"},{"location":"api/ordeq_toml/","title":"ordeq_toml","text":""},{"location":"api/ordeq_toml/toml/","title":"toml.py","text":""},{"location":"api/ordeq_toml/toml/#ordeq_toml.toml.TOML","title":"<code>TOML</code>  <code>dataclass</code>","text":"<p>               Bases: <code>TOMLInput</code>, <code>Output[dict[str, Any]]</code></p> <p>IO class for reading and writing TOML files.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from ordeq_toml import TOML\n&gt;&gt;&gt; toml_path = Path(\"config.toml\")\n&gt;&gt;&gt; io = TOML(path=toml_path)\n&gt;&gt;&gt; data = {\"key\": \"value\", \"number\": 42}\n&gt;&gt;&gt; io.save(data)  # doctest: +SKIP\n</code></pre>"},{"location":"api/ordeq_toml/toml/#ordeq_toml.toml.TOML.load","title":"<code>load(**load_options)</code>","text":"<p>Load and parse the TOML file specified by the path attribute.</p> <p>Parameters:</p> Name Type Description Default <code>**load_options</code> <code>Any</code> <p>Additional options to pass to the TOML loader.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary representing the contents of the TOML file.</p>"},{"location":"api/ordeq_toml/toml/#ordeq_toml.toml.TOML.save","title":"<code>save(data, **save_options)</code>","text":"<p>Serialize the given data to a TOML file at the path attribute.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>A dictionary to be serialized and saved as a TOML file.</p> required <code>**save_options</code> <code>Any</code> <p>Additional options to pass to the TOML dumper.</p> <code>{}</code>"},{"location":"api/ordeq_toml/toml/#ordeq_toml.toml.TOMLInput","title":"<code>TOMLInput</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Input[dict[str, Any]]</code></p> <p>IO class for reading TOML files.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from ordeq_toml import TOMLInput\n&gt;&gt;&gt; toml_path = Path(\"config.toml\")\n&gt;&gt;&gt; io = TOMLInput(path=toml_path)\n&gt;&gt;&gt; io.load()  # doctest: +SKIP\n</code></pre>"},{"location":"api/ordeq_toml/toml/#ordeq_toml.toml.TOMLInput.load","title":"<code>load(**load_options)</code>","text":"<p>Load and parse the TOML file specified by the path attribute.</p> <p>Parameters:</p> Name Type Description Default <code>**load_options</code> <code>Any</code> <p>Additional options to pass to the TOML loader.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary representing the contents of the TOML file.</p>"},{"location":"api/ordeq_viz/","title":"ordeq_viz","text":""},{"location":"api/ordeq_viz/api/","title":"api.py","text":""},{"location":"api/ordeq_viz/api/#ordeq_viz.api.viz","title":"<code>viz(*runnables, fmt, output=None, **options)</code>","text":"<pre><code>viz(\n    *runnables: Runnable,\n    fmt: Literal[\"kedro-viz\", \"mermaid\"],\n    output: Path,\n    **options: Any,\n) -&gt; None\n</code></pre><pre><code>viz(\n    *runnables: Runnable,\n    fmt: Literal[\"mermaid\"],\n    output: None = None,\n    **options: Any,\n) -&gt; str\n</code></pre> <p>Visualize the pipeline from the provided packages, modules, or nodes</p> <p>Parameters:</p> Name Type Description Default <code>runnables</code> <code>Runnable</code> <p>Package names, modules, or node callables from which to gather nodes from.</p> <code>()</code> <code>fmt</code> <code>Literal['kedro-viz', 'mermaid']</code> <p>Format of the output visualization, (\"kedro-viz\" or \"mermaid\").</p> required <code>output</code> <code>Path | None</code> <p>output file or directory where the viz will be saved.</p> <code>None</code> <code>options</code> <code>Any</code> <p>Additional options for the visualization functions.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str | None</code> <p>If <code>fmt</code> is 'mermaid' and <code>output</code> is not provided, returns the mermaid</p> <code>str | None</code> <p>diagram as a string. Otherwise, returns None.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>fmt</code> is 'kedro-viz' and <code>output</code> is not provided.</p>"},{"location":"api/ordeq_viz/graph/","title":"graph.py","text":""},{"location":"api/ordeq_viz/to_kedro_viz/","title":"to_kedro_viz.py","text":""},{"location":"api/ordeq_viz/to_kedro_viz/#ordeq_viz.to_kedro_viz.pipeline_to_kedro_viz","title":"<code>pipeline_to_kedro_viz(nodes, ios, output_directory)</code>","text":"<p>Convert a pipeline to a kedro-viz static pipeline directory</p> <p>Run with:</p> <pre><code>export KEDRO_DISABLE_TELEMETRY=true\nkedro viz run --load-file kedro-pipeline-example\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>set[Node]</code> <p>set of <code>ordeq.Node</code></p> required <code>ios</code> <code>Catalog</code> <p>dict of name and <code>ordeq.IO</code></p> required <code>output_directory</code> <code>Path</code> <p>path to write the output data to</p> required <p>Raises:</p> Type Description <code>FileExistsError</code> <p>if the output directory already exists</p>"},{"location":"api/ordeq_viz/to_mermaid/","title":"to_mermaid.py","text":""},{"location":"api/ordeq_viz/to_mermaid/#ordeq_viz.to_mermaid.pipeline_to_mermaid","title":"<code>pipeline_to_mermaid(nodes, ios, legend=True, use_dataset_styles=True, connect_wrapped_datasets=True, title=None, layout=None, theme=None, look=None, io_shape_template='[(\"{value}\")]', node_shape_template='([\"{value}\"])')</code>","text":"<p>Convert a pipeline to a mermaid diagram</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>set[Node]</code> <p>set of <code>ordeq.Node</code></p> required <code>ios</code> <code>Catalog</code> <p>dict of name and <code>ordeq.IO</code></p> required <code>legend</code> <code>bool</code> <p>if True, display a legend</p> <code>True</code> <code>use_dataset_styles</code> <code>bool</code> <p>if True, use a distinct color for each dataset type</p> <code>True</code> <code>connect_wrapped_datasets</code> <code>bool</code> <p>if True, connect wrapped datasets with a dashed line</p> <code>True</code> <code>title</code> <code>str | None</code> <p>Title of the mermaid diagram</p> <code>None</code> <code>layout</code> <code>str | None</code> <p>Layout type for the diagram (e.g., 'dagre')</p> <code>None</code> <code>theme</code> <code>str | None</code> <p>Theme for the diagram (e.g., 'neo')</p> <code>None</code> <code>look</code> <code>str | None</code> <p>Look and feel for the diagram (e.g., 'neo')</p> <code>None</code> <code>io_shape_template</code> <code>str</code> <p>Shape template for IO nodes, with <code>{value}</code> as placeholder for the name</p> <code>'[(\"{value}\")]'</code> <code>node_shape_template</code> <code>str</code> <p>Shape template for processing nodes, with <code>{value}</code> as placeholder for the name</p> <code>'([\"{value}\"])'</code> <p>Returns:</p> Type Description <code>str</code> <p>the pipeline rendered as mermaid diagram syntax</p>"},{"location":"api/ordeq_yaml/","title":"ordeq_yaml","text":""},{"location":"api/ordeq_yaml/yaml/","title":"yaml.py","text":""},{"location":"api/ordeq_yaml/yaml/#ordeq_yaml.yaml.YAML","title":"<code>YAML</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IO[dict]</code></p> <p>IO representing a YAML.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from ordeq_yaml import YAML\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; my_yaml = YAML(\n...     path=Path(\"path/to.yaml\")\n... )\n</code></pre>"},{"location":"contributing/acknowledgements/","title":"Acknowledgements","text":"<p>Ordeq has been developed at ING, with contributions from various individuals. We would like to acknowledge the following:</p> <ul> <li>Andra-Maria Acsintoae</li> <li>Soheil Moattar</li> <li>Jose Sanchez Viloria</li> <li>Barend Garvelink</li> <li>Guy Halperin</li> <li>Vlad Cozma</li> <li>Nathan Bijleveld</li> </ul> <p>The authors and maintainers of the project are Niels Neerhoff and Simon Brugman.</p>"},{"location":"contributing/documentation/","title":"Documentation","text":"<p>Ordeq uses MkDocs for documentation, and mkdocstrings for auto-generating the API documentation. The documentation is configured in <code>mkdocs.yml</code> at the root of the repository.</p> <p>To view the documentation locally, first install all dependencies:</p> <pre><code>just install\n</code></pre> <p>To spin up the documentation server, run:</p> <pre><code>just docs-serve\n</code></pre> <p>Now navigate to http://127.0.0.1:8000/ and enjoy the documentation!</p>"},{"location":"contributing/integrations/","title":"Adding integrations","text":"<p>Ordeq is designed to integrate seamlessly with the other tooling. The level of integration can vary from examples in the documentation to fully-fledged extensions that are maintained as part of the Ordeq repository.</p> <p>Here are the different levels of integrations, from least to most integrated:</p> <ol> <li> <p>User customizations: Implemented and maintained in the users' own codebases.</p> <ul> <li>Examples: custom IOs or hooks.</li> </ul> </li> <li> <p>Documented examples: Integrations that are documented in the Ordeq documentation.</p> <ul> <li>Examples: Streamlit and Docker integration guide.</li> </ul> </li> <li> <p>IO extensions: Integrations that extend the IO packages of Ordeq.</p> <ul> <li>Examples: <code>ordeq-pandas</code>, <code>ordeq-polars</code>. See the packages overview.</li> </ul> </li> <li> <p>Framework extensions: Integrations that extend framework functionality of Ordeq.</p> <ul> <li>Examples: <code>ordeq-viz</code> and <code>ordeq-cli-runner</code>. See the packages overview.</li> </ul> </li> <li> <p>Core framework: Reusable components that are added to the Ordeq core framework.</p> <ul> <li>Examples: hooks.</li> </ul> </li> </ol> <p>When deciding the level of integration, we evaluate the following criteria:</p> <ul> <li>Generality: Is the integration useful for a broad audience?</li> <li>Community interest: Is there a demand or interest from the community for this integration?</li> <li>Complexity: How much complexity does the integration add to the codebase?</li> <li>Maintenance: Can we maintain the integration over time?</li> </ul> <p>A favourable score on these criteria results in a higher level of integration.</p> <p>Not sure where to start?</p> <p>If you're unsure about the appropriate level of integration for your use case, please reach out to us on GitHub</p>"},{"location":"getting-started/introduction/","title":"Introduction","text":"<p>We have all been there. As your data project grows:</p> <ul> <li>Logic gets tightly coupled</li> <li>Repetitive tasks are duplicated</li> <li>Complexity increases</li> </ul> <p>Ordeq tackles these problems by providing a simple yet powerful way to define IO and transformations throughout your project.</p>"},{"location":"getting-started/introduction/#how-ordeq-helps","title":"How Ordeq helps","text":"<p>Let's see how Ordeq helps you develop data pipelines. We will start with a simple example. Here is how a simple data pipeline looks like without Ordeq:</p> __main__.py<pre><code>from pyspark.sql import SparkSession\n\nif __name__ == \"__main__\":\n    spark = SparkSession.builder.getOrCreate()\n    txs = spark.table(\"txs\")\n    clients = spark.table(\"clients\")\n    txs_and_clients = txs.join(clients, on=\"client_id\", how=\"left\")\n    txs_and_clients.write.mode(\"overwrite\").saveAsTable(\"txs_and_clients\")\n</code></pre> <p>Ordeq works with any data processing tool</p> <p>The example above uses PySpark, since it is a popular data processing tool, but the same principles apply to other tools. If you are not familiar with Spark, imagine the same logic implemented with Pandas, Dask, or another tool of your choice.</p> <p>Suppose we want to add a bit more logic, such as filtering the data by a certain date:</p> __main__.py<pre><code>from pyspark.sql import SparkSession\nfrom argparse import ArgumentParser\n\nif __name__ == \"__main__\":\n    spark = SparkSession.builder.getOrCreate()\n    parser = ArgumentParser()\n    parser.add_argument(\"--date\", type=str)\n    date = parser.parse_args().date\n    txs = spark.table(\"txs\")\n    txs = txs.filter(txs.date == date)\n    clients = spark.table(\"clients\")\n    txs_and_clients = txs.join(clients, on=\"client_id\", how=\"left\")\n    txs_and_clients.write.mode(\"overwrite\").saveAsTable(\"txs_and_clients\")\n</code></pre> <p>The code is getting more complex, and now you have to pass the date argument every time you run the script. Also, the logic is still tightly coupled, and you cannot easily reuse parts of it.</p>"},{"location":"getting-started/introduction/#improvements","title":"Improvements","text":"<p>Can we do better? Let's try to modularize the logic by splitting it into functions:</p> __main__.py<pre><code>from pyspark.sql import DataFrame, SparkSession\nfrom argparse import ArgumentParser\n\n\ndef load_table(spark: SparkSession, table: str) -&gt; DataFrame:\n    return spark.table(table)\n\n\ndef save_table(df: DataFrame, table: str):\n    df.write.mode(\"overwrite\").saveAsTable(table)\n\n\ndef parse_date() -&gt; str:\n    parser = ArgumentParser()\n    parser.add_argument(\"--date\", type=str)\n    return parser.parse_args().date\n\n\ndef join_txs_and_clients(txs: DataFrame, clients: DataFrame, date: str):\n    txs = txs.filter(txs.date == date)\n    return txs.join(clients, on=\"client_id\", how=\"left\")\n\n\nif __name__ == \"__main__\":\n    spark = SparkSession.builder.getOrCreate()\n    date = parse_date()\n    txs = load_table(spark, \"txs\")\n    clients = load_table(spark, \"clients\")\n    txs_and_clients = join_txs_and_clients(txs, clients, date)\n    save_table(txs_and_clients, \"txs_and_clients\")\n</code></pre> <p>This is much better! Each piece of logic can be tested in isolation. You can reuse the functions in other parts of your project. However, there are still some challenges. You still need to route <code>spark</code>, <code>date</code>, <code>txs</code> and <code>clients</code> through the functions. This couples the IO with the transformations.</p>"},{"location":"getting-started/introduction/#the-ordeq-solution","title":"The Ordeq solution","text":"<p>Ordeq dissolves the coupling by separating IO, like Spark tables and command line argument, from the transformations. While the transformations reside in <code>pipeline.py</code>, IOs are defined in a separate <code>catalog</code> module. Lastly, a <code>__main__.py</code> module takes care of running the job:</p> pipeline.pycatalog.py__main__.pyPipeline Diagram <pre><code>import catalog\nfrom ordeq import node\nfrom pyspark.sql import DataFrame\n\n\n@node(\n    inputs=[catalog.txs, catalog.clients, catalog.date],\n    outputs=catalog.txs_and_clients,\n)\ndef join_txs_and_clients(\n    txs: DataFrame, clients: DataFrame, date: str\n) -&gt; DataFrame:\n    txs = txs.filter(txs.date == date)\n    return txs.join(clients, on=\"client_id\", how=\"left\")\n</code></pre> <pre><code>from ordeq_args import CommandLineArg\nfrom ordeq_spark import SparkHiveTable\n\n# Input IOs\ndate = CommandLineArg(\"--date\", type=str)\ntxs = SparkHiveTable(table=\"txs\")\nclients = SparkHiveTable(table=\"clients\")\n\n# Output IOs\ntxs_and_clients = SparkHiveTable(table=\"txs_and_clients\")\n</code></pre> <pre><code>import logging\n\nfrom ordeq import run\nfrom pipeline import join_txs_and_clients\n\n# Enable logging\nlogging.basicConfig(level=logging.INFO)\n\nif __name__ == \"__main__\":\n    run(join_txs_and_clients)\n</code></pre> <pre><code>graph TB\n    subgraph legend[\"Legend\"]\n    direction TB\n    subgraph Objects\n        L0([\"Node\"]):::node\n        L1[(\"IO\")]:::io\n    end\n    subgraph IO Types\n        L00[(\"CommandLineArg\")]:::io0\n        L01[(\"SparkHiveTable\")]:::io1\n    end\nend\n\nIO0 --&gt; join_txs_and_clients\nIO1 --&gt; join_txs_and_clients\nIO2 --&gt; join_txs_and_clients\njoin_txs_and_clients --&gt; IO3\n\nsubgraph pipeline[\"Pipeline\"]\n    direction TB\n    join_txs_and_clients([\"join_txs_and_clients\"]):::node\n    IO0[(\"txs\")]:::io1\n    IO1[(\"clients\")]:::io1\n    IO2[(\"date\")]:::io0\n    IO3[(\"txs_and_clients\")]:::io1\nend\n\nclassDef node fill:#008AD7,color:#FFF\nclassDef io fill:#FFD43B\nclassDef io0 fill:#66c2a5\nclassDef io1 fill:#fc8d62\n</code></pre> <p>Visualising pipelines with Ordeq</p> <p>This diagram is auto-generated by Ordeq. See Run and Viz for more information.</p> <p>The idea behind the separation is that changes to the IO should not affect the transformations, and vice versa. Furthermore, the separation helps you keep your project organized.</p>"},{"location":"getting-started/introduction/#decoupling-in-practice","title":"Decoupling in practice","text":""},{"location":"getting-started/introduction/#changing-io","title":"Changing IO","text":"<p>Say you want to read the transactions from a Iceberg table instead of a Hive table, you only need to change <code>catalog.py</code>.</p> pipeline.pycatalog.py__main__.pyPipeline Diagram <pre><code>import catalog\nfrom ordeq import node\nfrom pyspark.sql import DataFrame\n\n\n@node(\n    inputs=[catalog.txs, catalog.clients, catalog.date],\n    outputs=catalog.txs_and_clients,\n)\ndef join_txs_and_clients(\n    txs: DataFrame, clients: DataFrame, date: str\n) -&gt; DataFrame:\n    txs = txs.filter(txs.date == date)\n    return txs.join(clients, on=\"client_id\", how=\"left\")\n</code></pre> <pre><code>from ordeq_args import CommandLineArg\nfrom ordeq_spark import SparkHiveTable\n\n# Input IOs\ndate = CommandLineArg(\"--date\", type=str)\ntxs = SparkIcebergTable(table=\"txs\")\nclients = SparkHiveTable(table=\"clients\")\n\n# Output IOs\ntxs_and_clients = SparkHiveTable(table=\"txs_and_clients\")\n</code></pre> <pre><code>import logging\n\nfrom ordeq import run\nfrom pipeline import join_txs_and_clients\n\n# Enable logging\nlogging.basicConfig(level=logging.INFO)\n\nif __name__ == \"__main__\":\n    run(join_txs_and_clients)\n</code></pre> <pre><code>graph TB\nsubgraph legend[\"Legend\"]\n    direction TB\n    subgraph Objects\n        L0([\"Node\"]):::node\n        L1[(\"IO\")]:::io\n    end\n    subgraph IO Types\n        L00[(\"CommandLineArg\")]:::io0\n        L01[(\"SparkHiveTable\")]:::io1\n        L02[(\"SparkIcebergTable\")]:::io2\n    end\nend\n\nIO0 --&gt; join_txs_and_clients\nIO1 --&gt; join_txs_and_clients\nIO2 --&gt; join_txs_and_clients\njoin_txs_and_clients --&gt; IO3\n\nsubgraph pipeline[\"Pipeline\"]\n    direction TB\n    join_txs_and_clients([\"join_txs_and_clients\"]):::node\n    IO0[(\"txs\")]:::io2\n    IO1[(\"clients\")]:::io1\n    IO2[(\"date\")]:::io0\n    IO3[(\"txs_and_clients\")]:::io1\nend\n\nclassDef node fill:#008AD7,color:#FFF\nclassDef io fill:#FFD43B\nclassDef io0 fill:#66c2a5\nclassDef io1 fill:#fc8d62\nclassDef io2 fill:#8da0cb\n</code></pre> <p>Or, maybe the date comes from an environment variable instead of a command line argument:</p> pipeline.pycatalog.py__main__.pyPipeline Diagram <pre><code>import catalog\nfrom ordeq import node\nfrom pyspark.sql import DataFrame\n\n\n@node(\n    inputs=[catalog.txs, catalog.clients, catalog.date],\n    outputs=catalog.txs_and_clients,\n)\ndef join_txs_and_clients(\n    txs: DataFrame, clients: DataFrame, date: str\n) -&gt; DataFrame:\n    txs = txs.filter(txs.date == date)\n    return txs.join(clients, on=\"client_id\", how=\"left\")\n</code></pre> <pre><code>from ordeq_args import EnvironmentVariable\nfrom ordeq_spark import SparkIcebergTable, SparkHiveTable\n\n# Input IOs\ndate = EnvironmentVariable(\"DATE\")\ntxs = SparkIcebergTable(table=\"txs\")\nclients = SparkHiveTable(table=\"clients\")\n\n# Output IOs\ntxs_and_clients = SparkHiveTable(table=\"txs_and_clients\")\n</code></pre> <pre><code>import logging\n\nfrom ordeq import run\nfrom pipeline import join_txs_and_clients\n\n# Enable logging\nlogging.basicConfig(level=logging.INFO)\n\nif __name__ == \"__main__\":\n    run(join_txs_and_clients)\n</code></pre> <pre><code>graph TB\nsubgraph legend[\"Legend\"]\n    direction TB\n    subgraph Objects\n        L0([\"Node\"]):::node\n        L1[(\"IO\")]:::io\n    end\n    subgraph IO Types\n        L00[(\"EnvironmentVariable\")]:::io0\n        L01[(\"SparkHiveTable\")]:::io1\n        L02[(\"SparkIcebergTable\")]:::io2\n    end\nend\n\nIO0 --&gt; join_txs_and_clients\nIO1 --&gt; join_txs_and_clients\nIO2 --&gt; join_txs_and_clients\njoin_txs_and_clients --&gt; IO3\n\nsubgraph pipeline[\"Pipeline\"]\n    direction TB\n    join_txs_and_clients([\"join_txs_and_clients\"]):::node\n    IO0[(\"txs\")]:::io2\n    IO1[(\"clients\")]:::io1\n    IO2[(\"date\")]:::io0\n    IO3[(\"txs_and_clients\")]:::io1\nend\n\nclassDef node fill:#008AD7,color:#FFF\nclassDef io fill:#FFD43B\nclassDef io0 fill:#66c2a5\nclassDef io1 fill:#fc8d62\nclassDef io2 fill:#8da0cb\n\n</code></pre> <p>Perhaps you want to append data to the <code>txs_and_clients</code> table instead of overwriting it:</p> pipeline.pycatalog.py__main__.pyPipeline Diagram <pre><code>import catalog\nfrom ordeq import node\nfrom pyspark.sql import DataFrame\n\n\n@node(\n    inputs=[catalog.txs, catalog.clients, catalog.date],\n    outputs=catalog.txs_and_clients,\n)\ndef join_txs_and_clients(\n    txs: DataFrame, clients: DataFrame, date: str\n) -&gt; DataFrame:\n    txs = txs.filter(txs.date == date)\n    return txs.join(clients, on=\"client_id\", how=\"left\")\n</code></pre> <pre><code>from ordeq_args import EnvironmentVariable\nfrom ordeq_spark import SparkIcebergTable, SparkHiveTable\n\n# Input IOs\ndate = EnvironmentVariable(\"DATE\")\ntxs = SparkIcebergTable(table=\"txs\")\nclients = SparkHiveTable(table=\"clients\")\n\n# Output IOs\ntxs_and_clients = SparkHiveTable(table=\"txs_and_clients\").with_save_options(\n    mode=\"append\"\n)\n</code></pre> <pre><code>import logging\n\nfrom ordeq import run\nfrom pipeline import join_txs_and_clients\n\n# Enable logging\nlogging.basicConfig(level=logging.INFO)\n\nif __name__ == \"__main__\":\n    run(join_txs_and_clients)\n</code></pre> <pre><code>graph TB\nsubgraph legend[\"Legend\"]\n    direction TB\n    subgraph Objects\n        L0([\"Node\"]):::node\n        L1[(\"IO\")]:::io\n    end\n    subgraph IO Types\n        L00[(\"EnvironmentVariable\")]:::io0\n        L01[(\"SparkHiveTable\")]:::io1\n        L02[(\"SparkIcebergTable\")]:::io2\n    end\nend\n\nIO0 --&gt; join_txs_and_clients\nIO1 --&gt; join_txs_and_clients\nIO2 --&gt; join_txs_and_clients\njoin_txs_and_clients --&gt; IO3\n\nsubgraph pipeline[\"Pipeline\"]\n    direction TB\n    join_txs_and_clients([\"join_txs_and_clients\"]):::node\n    IO0[(\"txs\")]:::io2\n    IO1[(\"clients\")]:::io1\n    IO2[(\"date\")]:::io0\n    IO3[(\"txs_and_clients\")]:::io1\nend\n\nclassDef node fill:#008AD7,color:#FFF\nclassDef io fill:#FFD43B\nclassDef io0 fill:#66c2a5\nclassDef io1 fill:#fc8d62\nclassDef io2 fill:#8da0cb\n</code></pre> <p>All changes above require only amendments to <code>catalog.py</code>. The transformations in <code>pipeline.py</code> remain unchanged. Furthermore, each IO is defined once and can be reused throughout your project.</p>"},{"location":"getting-started/introduction/#changing-transformations","title":"Changing transformations","text":"<p>Vice versa, if you want to change the logic of how transactions and clients are joined, you only need to change <code>pipeline.py</code>. For example, you might want to filter out inactive clients and transactions with a non-positive amount:</p> pipeline.pycatalog.py__main__.pyPipeline Diagram <pre><code>from ordeq import node\nimport catalog\n\n\n@node(\n    inputs=[catalog.txs, catalog.clients, catalog.date],\n    outputs=catalog.txs_and_clients,\n)\ndef join_txs_and_clients(\n    txs: DataFrame, clients: DataFrame, date: str\n) -&gt; DataFrame:\n    txs = txs.filter(txs.date == date)\n    txs_and_clients = txs.join(clients, on=\"client_id\", how=\"inner\")\n    return txs_and_clients.where(\n        (txs_and_clients.amount &gt; 0) &amp; (txs_and_clients.status == \"active\")\n    )\n</code></pre> <pre><code>from ordeq_args import EnvironmentVariable\nfrom ordeq_spark import SparkHiveTable, SparkIcebergTable\n\n# Input IOs\ndate = EnvironmentVariable(\"DATE\")\ntxs = SparkIcebergTable(table=\"txs\")\nclients = SparkHiveTable(table=\"clients\")\n\n# Output IOs\ntxs_and_clients = SparkHiveTable(table=\"txs_and_clients\").with_save_options(\n    mode=\"append\"\n)\n</code></pre> <pre><code>import logging\n\nfrom ordeq import run\nfrom pipeline import join_txs_and_clients\n\n# Enable logging\nlogging.basicConfig(level=logging.INFO)\n\nif __name__ == \"__main__\":\n    run(join_txs_and_clients)\n</code></pre> <pre><code>graph TB\nsubgraph legend[\"Legend\"]\n    direction TB\n    subgraph Objects\n        L0([\"Node\"]):::node\n        L1[(\"IO\")]:::io\n    end\n    subgraph IO Types\n        L00[(\"EnvironmentVariable\")]:::io0\n        L01[(\"SparkHiveTable\")]:::io1\n        L02[(\"SparkIcebergTable\")]:::io2\n    end\nend\n\nIO0 --&gt; join_txs_and_clients\nIO1 --&gt; join_txs_and_clients\nIO2 --&gt; join_txs_and_clients\njoin_txs_and_clients --&gt; IO3\n\nsubgraph pipeline[\"Pipeline\"]\n    direction TB\n    join_txs_and_clients([\"join_txs_and_clients\"]):::node\n    IO0[(\"txs\")]:::io2\n    IO1[(\"clients\")]:::io1\n    IO2[(\"date\")]:::io0\n    IO3[(\"txs_and_clients\")]:::io1\nend\n\nclassDef node fill:#008AD7,color:#FFF\nclassDef io fill:#FFD43B\nclassDef io0 fill:#66c2a5\nclassDef io1 fill:#fc8d62\nclassDef io2 fill:#8da0cb\n</code></pre> <p>The example above only changes the <code>join_txs_and_clients</code> node. What if we want to add more nodes? For example, we might want to add a node that aggregates the transaction amount by client:</p> pipeline.pycatalog.py__main__.pyPipeline Diagram <pre><code>from ordeq import node\nimport catalog\n\n\n@node(\n    inputs=[catalog.txs, catalog.clients, catalog.date],\n    outputs=catalog.txs_and_clients,\n)\ndef join_txs_and_clients(\n    txs: DataFrame, clients: DataFrame, date: str\n) -&gt; DataFrame:\n    txs = txs.filter(txs.date == date)\n    txs_and_clients = txs.join(clients, on=\"client_id\", how=\"inner\")\n    return txs_and_clients.where(\n        (txs_and_clients.amount &gt; 0) &amp; (txs_and_clients.status == \"active\")\n    )\n\n\n@node(inputs=catalog.txs_and_clients, outputs=catalog.aggregated_txs)\ndef aggregate_txs(txs_and_clients: DataFrame) -&gt; DataFrame:\n    return txs_and_clients.groupBy(\"client_id\").sum(\"amount\")\n</code></pre> <pre><code>from ordeq_args import EnvironmentVariable\nfrom ordeq_spark import SparkIcebergTable, SparkHiveTable\n\n# Input IOs\ndate = EnvironmentVariable(\"DATE\")\ntxs = SparkIcebergTable(table=\"txs\")\nclients = SparkHiveTable(table=\"clients\")\n\n# Intermediate IOs\ntxs_and_clients = SparkHiveTable(table=\"txs_and_clients\").with_save_options(\n    mode=\"append\"\n)\n\n# Output IOs\naggregated_txs = SparkHiveTable(table=\"aggregated_txs\").with_save_options(\n    partition_by=\"date\"\n)\n</code></pre> <pre><code>import logging\n\nfrom ordeq import run\nimport pipeline\n\n# Enable logging\nlogging.basicConfig(level=logging.INFO)\n\nif __name__ == \"__main__\":\n    run(pipeline)\n    # This is equivalent to running:\n    # from pipeline import join_txs_and_clients, aggregate_txs\n    # &gt;&gt;&gt; run(join_txs_and_clients, aggregate_txs)\n</code></pre> <pre><code>graph TB\nsubgraph legend[\"Legend\"]\n    direction TB\n    subgraph Objects\n        L0([\"Node\"]):::node\n        L1[(\"IO\")]:::io\n    end\n    subgraph IO Types\n        L00[(\"EnvironmentVariable\")]:::io0\n        L01[(\"SparkHiveTable\")]:::io1\n        L02[(\"SparkIcebergTable\")]:::io2\n    end\nend\n\nIO0 --&gt; join_txs_and_clients\nIO1 --&gt; join_txs_and_clients\nIO2 --&gt; join_txs_and_clients\njoin_txs_and_clients --&gt; IO3\nIO3 --&gt; aggregate_txs\naggregate_txs --&gt; IO4\n\nsubgraph pipeline[\"Pipeline\"]\n    direction TB\n    join_txs_and_clients([\"join_txs_and_clients\"]):::node\n    aggregate_txs([\"aggregate_txs\"]):::node\n    IO0[(\"txs\")]:::io2\n    IO1[(\"clients\")]:::io1\n    IO2[(\"date\")]:::io0\n    IO3[(\"txs_and_clients\")]:::io1\n    IO4[(\"aggregated_txs\")]:::io1\nend\n\nclassDef node fill:#008AD7,color:#FFF\nclassDef io fill:#FFD43B\nclassDef io0 fill:#66c2a5\nclassDef io1 fill:#fc8d62\nclassDef io2 fill:#8da0cb\n</code></pre> <p>This example shows how easy it is to grow and maintain your project with Ordeq. You can add new nodes and IO without changing existing code. Each transformation is modular and isolated.</p>"},{"location":"getting-started/introduction/#running-the-pipeline","title":"Running the pipeline","text":"<p>Meanwhile, the <code>run</code> function takes care of loading the inputs and saving the outputs of each node. You no longer need to route the inputs and outputs of each transformation through the functions. Dependencies between nodes are automatically resolved.</p> <p>Where to go from here?</p> <ul> <li>Learn more about Ordeqs core concepts</li> </ul>"},{"location":"getting-started/concepts/catalogs/","title":"Catalogs","text":"<p>A catalog is collection of IOs. From experience, we find that separating these IO from transformations (nodes) keeps your code clean and maintainable. Catalogs help you organize and swap IOs across different environments \u2014 such as local development, production, or testing \u2014 without changing your nodes. This makes it easy to run the same pipeline with different data sources or destinations.</p>"},{"location":"getting-started/concepts/catalogs/#defining-catalogs","title":"Defining catalogs","text":"<p>To create a catalog, make a Python module where each IO is a variable. For example, a local catalog might look like this:</p> catalogs/local.py<pre><code>from ordeq_spark import SparkCSV\n\niris = SparkCSV(path=\"path/to/local/iris.csv\")\npredictions = SparkCSV(path=\"path/to/local/predictions.csv\")\n</code></pre> <p>Here, <code>iris</code> and <code>predictions</code> are IOs pointing to local CSV files.</p> <p>IO naming convention</p> <p>Following PEP-8, IO instances should be lowercased. For instance, use <code>iris</code> instead of <code>Iris</code>. IO classes, on the other hand, should be in <code>PascalCase</code>, such as <code>SparkCSV</code>.</p> <p>For production, you might want to use different IOs, such as tables in a data lake:</p> catalogs/production.py<pre><code>from ordeq_spark import SparkIcebergTable\n\niris = SparkIcebergTable(table=\"iris\")\npredictions = SparkIcebergTable(table=\"predictions\")\n</code></pre> <p>Notice that both catalogs use the same variable names (<code>iris</code>, <code>predictions</code>), but the IOs themselves are different: one uses CSV files, the other uses Iceberg tables.</p> <p>Now you can use the IOs in your nodes by importing the catalog:</p> nodes.py<pre><code>from ordeq import node\nfrom pyspark.sql import DataFrame\n\nfrom catalogs import local  # or 'production'\n\n\n@node(inputs=local.iris, outputs=local.predictions)\ndef predict(iris: DataFrame) -&gt; DataFrame:\n    # Your prediction logic here\n    ...\n</code></pre> <p>Avoid individual IO imports</p> <p>It is best practice to import the catalog entirely, rather than individual IOs. This keeps the import statements clean, and makes it easier to switch catalogs. It also avoids name clashes between IOs and function arguments in your code.</p>"},{"location":"getting-started/concepts/catalogs/#switching-between-catalogs","title":"Switching between catalogs","text":"<p>You can select which catalog to use based on the environment that your code runs in. An easy way to do this is by using an environment variable. Say you want to switch between the local and production catalogs based on an environment variable called <code>ENV</code>. You can do so as follows:</p> catalogs/__init__.py<pre><code>import os\nfrom catalogs import local, production\n\ncatalog = local if os.getenv(\"ENV\") == \"local\" else production\n</code></pre> <p>Now, you can use the resolved catalog in your nodes:</p> nodes.py<pre><code>from ordeq import node\nfrom pyspark.sql import DataFrame\n\nfrom catalogs import catalog  # resolves to 'local' or 'production'\n\n\n@node(inputs=catalog.iris, outputs=catalog.predictions)\ndef predict(iris: DataFrame) -&gt; DataFrame:\n    # Your prediction logic here\n    ...\n</code></pre> <p>When the environment variable <code>ENV=local</code> it uses the local catalog. For any other value, it uses the production catalog.</p>"},{"location":"getting-started/concepts/catalogs/#ensuring-consistency","title":"Ensuring consistency","text":"<p>It is important that all your catalogs define the same IOs (that is, the same variable names). This prevents errors when switching between environments.</p> <p>You can check catalog consistency using the <code>check_catalogs_are_consistent</code> function. Here is how you would adapt the previous script:</p> catalogs/__init__.py<pre><code>import os\nfrom ordeq import check_catalogs_are_consistent\n\nfrom catalogs import local, production\n\ncheck_catalogs_are_consistent(local, production)\ncatalog = local if os.getenv(\"ENV\") == \"local\" else production\n</code></pre> <p>If the catalogs have different variable names, this function will raise an error, helping you catch mistakes early.</p> <p>When (not) to use catalogs</p> <p>Creating separate modules for different environments makes most sense if each module contains a different set of IOs that cannot be otherwise resolved at run-time.</p> <p>For instance, if the only difference between your local and production environments is the namespace, you can use environment variables or configuration file to set the table names dynamically, rather than creating separate catalogs:</p> catalog.py<pre><code>from ordeq_spark import SparkCSV\n\nns = os.getenv(\"NAMESPACE\", \"default\")\niris = SparkIcebergTable(table=f\"{ns}.iris\")\npredictions = SparkIcebergTable(table=f\"{ns}.predictions\")\n</code></pre> <p>This approach is simpler and avoids the overhaead of multiple catalog modules.</p>"},{"location":"getting-started/concepts/catalogs/#extending-catalogs","title":"Extending catalogs","text":"<p>Often we do not want to create a new catalog from scratch, but rather extend an existing one. For example, you might want to create a <code>staging</code> catalog that is similar to <code>production</code>, but with a few differences. You can do this by importing the base catalog and overriding specific IOs:</p> catalogs/staging.py<pre><code>from ordeq_spark import SparkCSV\n\nfrom catalogs.production import *\n\niris = SparkCSV(path=\"path/to/staging/iris.csv\")  # Override iris IO\n</code></pre> <p>This way, the <code>staging</code> catalog inherits all IOs from <code>production</code>, except for <code>iris</code>, which is replaced with a CSV.</p> <p>You can also extend catalogs, as follows:</p> catalogs/staging.py<pre><code>from ordeq_spark import SparkCSV\n\nfrom catalogs.production import *\n\niris_large = SparkCSV(\n    path=\"path/to/staging/iris_large.csv\"\n)  # Extends with a new IO\n</code></pre> <p>Note that the extended catalog is not consistent with the <code>production</code> catalog, since it defines a new IO.</p>"},{"location":"getting-started/concepts/catalogs/#using-catalogs-in-tests","title":"Using catalogs in tests","text":"<p>Catalogs are useful for testing, because you can easily swap to test IOs. First, define a test catalog with test IOs:</p> <pre><code>from ordeq_spark import SparkCSV\n\niris = SparkCSV(path=\"path/to/test/iris.csv\")\npredictions = SparkCSV(path=\"path/to/test/predictions.csv\")\n</code></pre> <p>You can place the catalog in the source package, with the other catalogs. In that case, you can import it in your nodes as shown above.</p> <p>If you place the catalog in another package, say the <code>tests</code> package, the easiest way to use the test catalog is to patch:</p> test_nodes.py<pre><code>from ordeq import run\nfrom unittest import patch\n\nimport nodes\nimport tests\n\n\n@patch(\"nodes.catalog\", new=tests.catalog)\ndef test_it_predicts(catalog):\n    actual = run(predict)\n    assert predictions.load() == ...  # do your assertions here\n</code></pre> <p>Limitation</p> <p>Patching a catalog only works if the catalog is a plain module, not a package.</p> <p>If you do not want to create a new catalog, you can run nodes with alternative IOs directly:</p> test_nodes.py<pre><code>from ordeq import run\nfrom ordeq_spark import SparkCSV\n\nfrom catalogs import catalog\n\n\ndef test_it_predicts():\n    actual = run(predict, io={catalog.iris: SparkCSV(path=\"path/to/test.csv\")})\n    assert catalog.predictions.load() == ...  # do your assertions here\n</code></pre> <p>This is especially useful for unit tests. Checkout the node testing guide for more details.</p>"},{"location":"getting-started/concepts/hooks/","title":"Hooks","text":"<p>Hooks can be used to run code at certain phases in the run. With hooks, you can execute custom functionality such as data quality checks, logging and timing, without affecting data transformations in the nodes.</p>"},{"location":"getting-started/concepts/hooks/#why-hooks-are-useful","title":"Why hooks are useful","text":"<p>To see how hooks can be useful, let's consider the following example:</p> nodes.pycatalog.py <pre><code>import catalog\nfrom ordeq import node\n\n\n@node(inputs=catalog.names, outputs=catalog.greetings)\ndef greet(names: tuple[str, ...]) -&gt; list[str]:\n    \"\"\"Returns a greeting for each person.\"\"\"\n    greetings = []\n    for name in names:\n        greetings.append(f\"Hello, {name}!\")\n    return greetings\n</code></pre> <pre><code>from pathlib import Path\n\nfrom ordeq_files import CSV, Text\n\nnames = CSV(path=Path(\"names.csv\"))\ngreetings = Text(path=Path(\"greetings.txt\"))\n</code></pre> <p>As we learned earlier, running the node <code>greet</code> is roughly equivalent to:</p> <pre><code>&gt;&gt;&gt; import catalog\n&gt;&gt;&gt; names = catalog.names.load()\n&gt;&gt;&gt; greetings = greet(names)\n&gt;&gt;&gt; catalog.greetings.save(greetings)\n</code></pre> <p>With hooks, you can inject custom logic around each step. For example, you might want to log the time taken to load the input <code>names</code>:</p> <pre><code>&gt;&gt;&gt; import catalog\n&gt;&gt;&gt; import time\n&gt;&gt;&gt; start_time = time.time()\n&gt;&gt;&gt; names = catalog.names.load()\n&gt;&gt;&gt; end_time = time.time()\n&gt;&gt;&gt; print(f\"Loading names took {end_time - start_time} seconds\")\n&gt;&gt;&gt; greetings = greet(names)\n&gt;&gt;&gt; catalog.greetings.save(greetings)\n</code></pre> <p>As you can see, this custom logic adds a lot of boilerplate code around loading the input. If you want to do this for every input, it quickly becomes tedious and error-prone.</p> <p>Instead, you can use a hook to run this logic automatically around every input loading:</p> time_hook.py<pre><code>import time\n\nfrom ordeq import Input, InputHook\n\n\nclass TimeHook(InputHook):\n    def before_input_load(self, io: Input) -&gt; None:\n        self.start_time = time.time()\n\n    def after_input_load(self, io: Input, data) -&gt; None:\n        end_time = time.time()\n        print(f\"Loading {io} took {end_time - self.start_time} seconds\")\n</code></pre> <p>To ensure the hook is executed, it needs to be attached to the input:</p> catalog.py <pre><code>from pathlib import Path\n\nfrom ordeq_files import CSV, Text\nfrom time_hook import TimeHook\n\nnames = CSV(path=Path(\"names.csv\")).with_input_hooks(TimeHook())\ngreetings = Text(path=Path(\"greetings.txt\"))\n</code></pre>"},{"location":"getting-started/concepts/hooks/#types-of-hooks","title":"Types of hooks","text":"<p>Ordeq provides three types of hooks:</p> <ul> <li><code>RunHook</code>: called around a set of nodes</li> <li><code>NodeHook</code>: called around running a node</li> <li><code>InputHook</code>: called around the loading of inputs</li> <li><code>OutputHook</code>: called around the saving of outputs</li> </ul> <p>The following diagram depicts how the hooks are applied by Ordeq:</p> <pre><code>graph LR\n    subgraph Run\n        Z[Start run] --&gt; Z1[RunHook.before_run]\n        Z1 --&gt; Z2{{For each node}}\n        Z2 --&gt; A\n        subgraph Node\n            A[Start node run] --&gt; B[NodeHook.before_node_run]\n            B --&gt; C1{{For each input}}\n            subgraph Input\n                C1 --&gt; C2[InputHook.before_input_load]\n                C2 --&gt; C3[Load input]\n                C3 --&gt; C4[InputHook.after_input_load]\n                C4 --&gt; D1{{Next input}}\n                D1 -- More? --&gt; C2\n\n            end\n            D1 -- All done --&gt; E[Call node function]\n            E --&gt;|Success| F1{{For each output}}\n            subgraph Output\n                F1 --&gt; F2[OutputHook.before_output_save]\n                F2 --&gt; F3[Save output]\n                F3 --&gt; F4[OutputHook.after_output_save]\n                F4 --&gt; G1{{Next output}}\n                G1 -- More? --&gt; F2\n            end\n            G1 -- All done --&gt; H[NodeHook.after_node_run]\n            H --&gt; I[End node run]\n            E --&gt;|Error| X[NodeHook.on_node_call_error]\n            X --&gt; I\n\n        end\n        I --&gt; Z3[RunHook.after_run]\n        Z3 --&gt; End[End run]\n    end</code></pre> <p>This page demonstrated the concept of hooks and discussed an elementary examples. For a more elaborate guide, including details on how to implement your own hooks, see Creating custom hooks.</p> <p>Where to go from here?</p> <ul> <li>See how to create custom hooks in the guide</li> <li>Check out the guide on testing nodes</li> </ul>"},{"location":"getting-started/concepts/io/","title":"IO","text":"<p>IO refers to the loading and saving of data. For example, you might have an IO that loads a CSV file. In Ordeq, an IO is represented by an <code>IO</code> class.</p>"},{"location":"getting-started/concepts/io/#why-use-ios","title":"Why use IOs?","text":"<p>To understand why IOs are useful, let's look at a simple example. Suppose we want to load a simple CSV file. We could write a function that loads the CSV file directly:</p> <pre><code>import csv\nfrom pathlib import Path\n\n\ndef load_csv(path: Path) -&gt; list[list[str]]:\n    with path.open(mode=\"r\") as f:\n        reader = csv.reader(f)\n        return list(reader)\n</code></pre> <p>The main downside of this approach is that it immediately loads the data when the IO details, like the path to the file, are defined:</p> <pre><code>&gt;&gt;&gt; load_csv(Path(\"to/data.csv\"))\n[(1, \"kiwis\", 7.2), (2, \"grapefruits\", 1.4)]\n</code></pre> <p>Instead, we can use the <code>CSV</code> IO from <code>ordeq_files</code> to represent the CSV file:</p> <pre><code>from pathlib import Path\n\nfrom ordeq_files import CSV\n\nio = CSV(path=Path(\"to/data.csv\"))\n</code></pre> <p>Defining the IO does not load the data yet, until we tell it to:</p> <pre><code>&gt;&gt;&gt; io.load()\n[(1, \"kiwis\", 7.2), (2, \"grapefruits\", 1.4)]\n</code></pre> <p>IOs do not hold any data</p> <p>IOs do not hold any data themselves, they just know how to load and save data.</p> <p>This means IOs can be defined separately from when they are used. It also means IOs can be easily reused in different places.</p> <p>The same IO can be used to save data as well:</p> <pre><code>&gt;&gt;&gt; data_to_save = [(1, \"apples\", 3.5), (2, \"bananas\", 4.0)]\n&gt;&gt;&gt; io.save(data_to_save)\n</code></pre> <p>Lastly, IOs serve as convenient and lightweight representations of data in your project:</p> <pre><code>&gt;&gt;&gt; print(io)\nCSV(path=PosixPath('to/data.csv'))\n</code></pre> <p>More complex IOs</p> <p>A key feature of IOs is that they abstract the loading and saving behaviour from the user. IOs are typically used to handle the interaction with file systems, cloud storage, APIs, databases and other data sources. Unlike the example above, these more complex IOs manage everything from authentication to (de)serialization.</p> <p>Ordeq offers many off-the-shelf IOs for common data formats, such as CSV, Excel, JSON, Parquet, and SQL databases. Refer to the API documentation for a full list of available IOs.</p>"},{"location":"getting-started/concepts/io/#using-ios","title":"Using IOs","text":"<p>IOs can be used stand-alone, for instance when exploring data in a Jupyter notebook. Suppose you just received an Excel file from a colleague and want to take a look at it. You can use the <code>PandasExcel</code> IO from <code>ordeq_pandas</code> to load and inspect the data:</p> <pre><code>&gt;&gt;&gt; from ordeq_pandas import PandasExcel\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; fruit_sales = PandasExcel(path=Path(\"fruit_sales.xlsx\"))\n&gt;&gt;&gt; df = fruit_sales.load()\n&gt;&gt;&gt; df.head(2)\nFruit       Quantity (kg)  Price   Store\n0   apples          '3.5'   1.2     A\n1  bananas          '4.0'   0.8     B\n&gt;&gt;&gt; df.dtypes\nFruit           object\nQuantity (kg)   object\nPrice           float64\nStore           object\ndtype: object\n</code></pre> <p>Unfortunately the data types are not quite right. We want to convert the <code>Quantity (kg)</code> column to <code>float</code> and rename the columns to be more convenient. Furthermore, we want to drop the <code>Store</code> column as we don't need it.</p>"},{"location":"getting-started/concepts/io/#load-save-options","title":"Load &amp; save options","text":"<p>You can alter the loading behaviour of an IO through its load options:</p> <pre><code>&gt;&gt;&gt; fruit_sales = fruit_sales.with_load_options(\n...     dtype={\"Quantity (kg)\": float},\n...     usecols=\"A:C\",\n...     names=[\"fruit\", \"quantity_kg\", \"price\"],\n... )\n&gt;&gt;&gt; df = fruit_sales.load()\n&gt;&gt;&gt; df.dtypes\nfruit           object\nquantity_kg     float64\nprice           float64\ndtype: object\n</code></pre> <p>Here, the load options are used to specify the data types, select specific columns, and rename a column. Under the hood, these options are passed to <code>pandas.read_excel</code>.</p> <p>Building IO load and save options</p> <p>The <code>with_load_options</code> and <code>with_save_options</code> methods return a new IO instance with the updated options. The original IO instance remains unchanged.</p> <p>Similarly, you can alter the saving behaviour of an IO through its save options:</p> <pre><code>&gt;&gt;&gt; fruit_sales = fruit_sales.with_save_options(index=False)\n&gt;&gt;&gt; fruit_sales.save(df)\n</code></pre> <p>For more information on the available load and save options, refer to the documentation of the specific IO you are using.</p> <p>IOs should not apply transformations</p> <p>IOs should only be concerned with loading and saving data. Therefore, IOs should not apply any transformation on load or save. Some load or save options do incur what can be considered a transformation, like the casting or renaming done above. As a rule of thumb:</p> <ul> <li>if the option is specific to your use case, it should be done outside the IO</li> <li>if the option refers to an operation that is likely to be useful to others, it might be appropriate as a load/save option.</li> <li>if the option is closely tied to the IO implementation, it is likely appropriate as a load/save option.</li> </ul> <p>Where to go from here?</p> <ul> <li>See how IOs are used in nodes</li> <li>Find out how to organise IOs with catalogs</li> <li>See how to extend inject custom logic with IO hooks</li> <li>Check out the guide on creating custom IOs</li> </ul>"},{"location":"getting-started/concepts/nodes/","title":"Nodes","text":"<p>Nodes are functions representing a piece of data pipeline logic. Typically, a node takes some data as input, transforms it, and returns the transformed data as output. Nodes can be created by decorating a function with the <code>@node</code> decorator:</p> nodes.py <pre><code>from collections.abc import Iterable\n\nfrom ordeq import node\n\n\n@node\ndef greet(names: Iterable[str]) -&gt; None:\n    \"\"\"Prints a greeting for each person.\"\"\"\n    for name in names:\n        print(f\"Hello, {name}!\")\n</code></pre> <p>Like functions, each node can have no, one, or more than one input(s), and return no, one, or more than one output(s). In data pipelines, the inputs and outputs usually represent data that is processed by the pipeline.</p>"},{"location":"getting-started/concepts/nodes/#specifying-inputs-and-outputs","title":"Specifying inputs and outputs","text":"<p>Suppose we want to greet a list of people whose names are provided in a CSV. Let's use the CSV IO discussed in the IO section. First we define the CSV IO in <code>catalog.py</code>. Next, we modify the node in <code>nodes.py</code>:</p> nodes.pycatalog.py <pre><code>import catalog\nfrom ordeq import node\n\n\n@node(inputs=catalog.names)\ndef greet(names: tuple[str, ...]):\n    \"\"\"Prints a greeting for each person.\"\"\"\n    for name in names:\n        print(f\"Hello, {name}!\")\n</code></pre> <pre><code>from pathlib import Path\n\nfrom ordeq_files import CSV\n\nnames = CSV(path=Path(\"names.csv\"))\n</code></pre> <p>By specifying <code>names</code> as the input, we inform Ordeq that the <code>greet</code> node should use the data from <code>names.csv</code> when the node is run.</p> <p>Where to define IOs</p> <p>Although you can define IOs anywhere in your project, it is best practice to define them in a separate module. Such a module is often referred to as a \"catalog\" and is discussed in more detail in the catalogs section.</p> <p>Similarly, we can add a <code>greetings</code> IO and specify it as output to the <code>greet</code> node:</p> nodes.pycatalog.py <pre><code>import catalog\nfrom ordeq import node\n\n\n@node(inputs=catalog.names, outputs=catalog.greetings)\ndef greet(names: tuple[str, ...]) -&gt; list[str]:\n    \"\"\"Returns a greeting for each person.\"\"\"\n    greetings = []\n    for name in names:\n        greetings.append(f\"Hello, {name}!\")\n    return greetings\n</code></pre> <pre><code>from pathlib import Path\n\nfrom ordeq_files import CSV, Text\n\nnames = CSV(path=Path(\"names.csv\"))\ngreetings = Text(path=Path(\"greetings.txt\"))\n</code></pre> <p>Nodes behave like plain functions</p> <p>The <code>@node</code> decorator only registers the function as a node, it does not change the function's behavior:</p> <pre><code>&gt;&gt;&gt; type(greet)\nfunction\n&gt;&gt;&gt; greet([\"Alice\", \"Bob\"])\n[\"Hello, Alice!\", \"Hello, Bob!\"]\n&gt;&gt;&gt; greet(greet([\"Alice\"]))\n\"Hello, Hello, Alice!!\"\n&gt;&gt;&gt; greet.__doc__\n\"Prints a greeting for each person.\"\n&gt;&gt;&gt; greet.__annotations__\n{'names': typing.Iterable[str], 'return': list[str]}\n</code></pre> <p>This also means the node can be unit tested like any other function.</p>"},{"location":"getting-started/concepts/nodes/#running-a-node","title":"Running a node","text":"<p>Nodes can be run as follows:</p> <pre><code>&gt;&gt;&gt; from ordeq import run\n&gt;&gt;&gt; run(greet)\n</code></pre> <p>Let's break down what happens when a node is run:</p> <ul> <li>the node inputs (<code>names</code>) are loaded and passed to the function <code>greet</code></li> <li>the function <code>greet</code> is executed</li> <li>the values returned by <code>greet</code> are saved to the node outputs (<code>greetings</code>).</li> </ul> <p>Running <code>greet</code> is therefore roughly equivalent to:</p> <pre><code>&gt;&gt;&gt; names = catalog.names.load()\n&gt;&gt;&gt; greetings = greet(names)\n&gt;&gt;&gt; catalog.greetings.save(greetings)\n</code></pre> <p>Because Ordeq handles the loading and saving of the inputs and outputs, you can focus on the transformation in the node.</p>"},{"location":"getting-started/concepts/nodes/#running-multiple-nodes","title":"Running multiple nodes","text":"<p>Even the simplest data pipelines consist of multiple steps. Usually, one step depends on the output of another step. Let's extend our example with another node that parses the name to greet from a YAML file:</p> nodes.pycatalog.py <pre><code>import catalog\nfrom ordeq import node\n\n\n@node(inputs=catalog.invitees, outputs=catalog.names)\ndef parse_names(invitees: dict) -&gt; list[str]:\n    \"\"\"Parse the names from the invitees data.\"\"\"\n    return [invitee[\"name\"] for invitee in invitees]\n\n\n@node(inputs=catalog.names, outputs=catalog.greetings)\ndef greet(names: tuple[str, ...]) -&gt; list[str]:\n    \"\"\"Returns a greeting for each person.\"\"\"\n    greetings = []\n    for name in names:\n        greetings.append(f\"Hello, {name}!\")\n    return greetings\n</code></pre> <pre><code>from ordeq_files import CSV, Text\nfrom ordeq_yaml import YAML\nfrom pathlib import Path\n\ninvitees = YAML(path=Path(\"invitees.yaml\"))\nnames = CSV(path=Path(\"names.csv\"))\ngreetings = Text(path=Path(\"greetings.txt\"))\n</code></pre> <p>Note that <code>parse_names</code> outputs the <code>names</code> IO, which is input to the <code>greet</code> node. When we run the two nodes together, Ordeq will automatically pass the output of <code>parse_names</code> to <code>greet</code>:</p> <pre><code>&gt;&gt;&gt; run(parse_names, greet)\n</code></pre> <p>This is roughly equivalent to:</p> <pre><code>&gt;&gt;&gt; invitees = catalog.invitees.load()\n&gt;&gt;&gt; names = parse_names(invitees)\n&gt;&gt;&gt; catalog.names.save(names)\n&gt;&gt;&gt; greetings = greet(parsed_names)\n&gt;&gt;&gt; catalog.greetings.save(greetings)\n</code></pre> <p>As before, Ordeq handles the loading and saving of inputs and outputs. But now, it also takes care of passing the outputs of one node as inputs to another.</p> <p>Dependency resolution</p> <p>Ordeq resolves the DAG (Directed Acyclic Graph) of the nodes that are run, ensuring that each node is run in the correct order based on its dependencies. This also means an IO cannot be outputted by more than one node.</p>"},{"location":"getting-started/concepts/nodes/#retrieving-results","title":"Retrieving results","text":"<p>The result of <code>run</code> is a dictionary containing the data that was loaded or saved by each IO:</p> <pre><code>&gt;&gt;&gt; result = run(parse_names, greet)\n&gt;&gt;&gt; result[catalog.names]\n[\"Abraham\", \"Adam\", \"Azul\", ...]\n&gt;&gt;&gt; result[catalog.greetings]\n[\"Hello, Abraham!\", \"Hello, Adam!\", \"Hello, Azul!\", ...]\n</code></pre> <p>This works much like a cache of the processed data for the duration of the run. Of course, you can also load the data directly from the IOs:</p> <pre><code>&gt;&gt;&gt; greetings.load()\n[\"Hello, Abraham!\", \"Hello, Adam!\", \"Hello, Azul!\", ...]\n</code></pre> <p>This has the overhead of loading the data from storage, but it can be useful if you want to access the data after the run has completed.</p> <p>Where to go from here?</p> <ul> <li>See how to extend inject custom logic with node hooks</li> <li>Check out the guide on testing nodes</li> </ul>"},{"location":"getting-started/concepts/views/","title":"Views","text":"<p>Views are a special type of node. They are ideal to reflect common transformations, such as:</p> <ul> <li>filtering, parsing, casting or selecting data</li> <li>converting inputs from one type to another (say, from Pandas to Spark DataFrame)</li> <li>selecting a device for model training (say, CPU vs GPU)</li> </ul> <p>Often, we don't want to save the result of these transformations. Instead, we want to pass along the result directly to a subsequent transformation. That's exactly what views achieve.</p>"},{"location":"getting-started/concepts/views/#example","title":"Example","text":"<p>We will slightly adapt the running example from the [introduction]. Suppose we're tasked to:</p> <ul> <li>filter a Spark DataFrame <code>txs</code> by date</li> <li>join the filtered DataFrame with <code>clients</code>, and output the result to a Hive table</li> </ul> <p>A first attempt at completing this using Ordeq would look roughly like this:</p> pipeline.pycatalog.py__main__.py <pre><code>import catalog\nfrom ordeq import node\nfrom pyspark.sql import DataFrame\n\n\n@node(inputs=[catalog.txs, catalog.date], outputs=[catalog.txs_filtered])\ndef filter_by_date(txs: DataFrame, date: str) -&gt; DataFrame:\n    return txs.filter(txs.date == date)\n\n\n@node(\n    inputs=[catalog.txs, catalog.clients, catalog.date],\n    outputs=catalog.txs_and_clients,\n)\ndef join_txs_and_clients(\n    txs: DataFrame, clients: DataFrame, date: str\n) -&gt; DataFrame:\n    return txs.join(clients, on=\"client_id\", how=\"left\")\n</code></pre> <pre><code>from ordeq import IO\nfrom ordeq_args import CommandLineArg\nfrom ordeq_spark import SparkHiveTable\n\ndate = CommandLineArg(\"--date\", type=str)\ntxs = SparkHiveTable(table=\"txs\")\ntxs_filtered = IO()  # Placeholder IO\nclients = SparkHiveTable(table=\"clients\")\ntxs_and_clients = SparkHiveTable(table=\"txs_and_clients\")\n</code></pre> <pre><code>from nodes import filter_by_date, join_txs_and_clients\nfrom ordeq import run\n\nif __name__ == \"__main__\":\n    run(filter_by_date, join_txs_and_clients)\n</code></pre> <p>Here's what we have done:</p> <ul> <li>to keep the transformations modular, we created two nodes: <code>filter_by_date</code> and <code>join_txs_and_clients</code>.</li> <li>we defined a placeholder IO <code>txs_filtered</code> in <code>catalog.py</code>.     This IO is not associated with any actual data source.     It merely serves to pass the output from one node to the other.</li> </ul> <p>This approach has a couple of downsides:</p> <ul> <li>the separate placeholder IO seems like overhead</li> <li>we have to remember to run both the <code>filter_by_date</code> and <code>join_txs_with_clients</code>, even though <code>filter_by_date</code> will never be run individually</li> <li>it's not obvious how to reuse <code>filter_by_date</code> with different IO, while it does seem a very common transformation</li> </ul>"},{"location":"getting-started/concepts/views/#using-views","title":"Using views","text":"<p>Using views, we can edit the example above to as follows:</p> pipeline.pycatalog.py__main__.py <pre><code>import catalog\nfrom ordeq import node\nfrom pyspark.sql import DataFrame\n\n\n@node(inputs=[catalog.txs, catalog.date])\ndef txs_filtered(txs: DataFrame, date: str) -&gt; DataFrame:\n    return txs.filter(txs.date == date)\n\n\n@node(inputs=[txs_filtered, catalog.clients], outputs=catalog.txs_and_clients)\ndef join_txs_and_clients(\n    txs: DataFrame, clients: DataFrame, date: str\n) -&gt; DataFrame:\n    return txs.join(clients, on=\"client_id\", how=\"left\")\n</code></pre> <pre><code>from ordeq import IO\nfrom ordeq_args import CommandLineArg\nfrom ordeq_spark import SparkHiveTable\n\ndate = CommandLineArg(\"--date\", type=str)\ntxs = SparkHiveTable(table=\"txs\")\nclients = SparkHiveTable(table=\"clients\")\ntxs_and_clients = SparkHiveTable(table=\"txs_and_clients\")\n</code></pre> <pre><code>from nodes import join_txs_and_clients\nfrom ordeq import run\n\nif __name__ == \"__main__\":\n    run(join_txs_and_clients)\n</code></pre> <p>Let's break this down:</p> <ul> <li>we have defined a view <code>txs_filtered</code>. Views have no outputs, but may take inputs. In this case, the inputs are the same as those of the previous <code>filter_by_date</code> node.</li> <li>we pass the view <code>txs_filtered</code> directly as input to the node <code>join_txs_with_clients</code></li> </ul> <p>By setting the view as input to the node, Ordeq will pass along the result of <code>txs_filtered</code> to <code>join_txs_with_clients</code>, without the need for a placeholder IO. In addition, it will automatically run <code>txs_filtered</code> once <code>join_txs_with_clients</code> is run. This greatly simplifies the run command.</p>"},{"location":"guides/cloud_storage/","title":"Cloud storage","text":"<p>Ordeq IO has been designed to work seamlessly with cloud storage services like Amazon S3, Google Cloud Storage and Azure Blob Storage. Interacting with these services involves certain complexities, like authentication and network communication. Ordeq IO aims to abstract these complexities from the user, while requiring little code.</p>"},{"location":"guides/cloud_storage/#leveraging-client-libraries","title":"Leveraging client libraries","text":"<p>Many IOs offered by Ordeq leverage client libraries with built-in cloud storage support. For instance, you can load and save a CSV to S3 using Pandas as follows:</p> <p>Install the <code>ordeq-pandas</code> package</p> <p>To follow the example below, you need to install the <code>ordeq-pandas</code> package.</p> <pre><code>&gt;&gt;&gt; from ordeq_pandas import PandasCSV\n&gt;&gt;&gt; csv = PandasCSV(path=\"gs://my-bucket/my-data.csv\")\n&gt;&gt;&gt; csv.load()  # this loads from Google Cloud Storage\n   id    name      date   amount\n0   1   Alice 2024-01-01     100\n1   2     Bob 2024-01-02     150\n...\n</code></pre> <p>Other popular libraries that offer built-in support for cloud storage are DuckDB, Polars and Spark. Ordeq provides IOs for these libraries in <code>ordeq-duckdb</code>, <code>ordeq-polars</code> and <code>ordeq-spark</code>. Have a look at the API reference for more information.</p>"},{"location":"guides/cloud_storage/#cloudpath","title":"CloudPath","text":"<p>Another way IOs support cloud storage when built-in support is not present, is by accepting <code>cloudpathlib</code>'s <code>CloudPath</code>. CloudPaths abstract away the complexities of interacting with cloud storage like Amazon S3, Google Cloud Storage and Azure Blob Storage.</p> <p>Install <code>cloudpathlib</code> to use <code>CloudPath</code></p> <p>To follow the example below, you need to install the <code>cloudpathlib</code> package.</p> <p>The following example loads a JSON from Azure Blob Storage:</p> <pre><code>&gt;&gt;&gt; from cloudpathlib import CloudPath\n&gt;&gt;&gt; json = JSON(path=CloudPath(\"az://my-container/txs.json\"))\n&gt;&gt;&gt; json.load()  # this loads from Azure Blob Storage\n{'transactions': [{'amount': 100, 'date': '2023-01-01'}, ...]}\n</code></pre> <p>Check out the cloudpathlib documentation for more information and examples.</p> <p>Many file-like IOs accept a CloudPath object as path. Examples include <code>ordeq_yaml.YAML</code>, <code>ordeq_matplotlib.MatplotlibFigure</code> and <code>ordeq_altair.AltairChart</code>. Check the API reference for more details.</p>"},{"location":"guides/custom_io/","title":"Creating a IO class","text":"<p>This guide will help you create a new IO class by extending the base classes provided by Ordeq. IO classes are basic building block in <code>ordeq</code> to abstract IO operations from data transformations.</p> <p>Frequently used IO implementations are offered out-of-the-box as <code>ordeq</code> packages. For instance, there is support for JSON, YAML, Pandas, NumPy, Polars and many more. These can be used where applicable and serve as reference implementation for new IO classes.</p>"},{"location":"guides/custom_io/#understanding-the-io-base-class","title":"Understanding the IO Base Class","text":"<p>The <code>IO</code> class is an abstract base class that defines the structure for loading and saving data. It includes the following key methods:</p> <ul> <li><code>load()</code>: Method to be implemented by subclasses for loading data.</li> <li><code>save(data)</code>: Method to be implemented by subclasses for saving data.</li> </ul>"},{"location":"guides/custom_io/#example-faissindex","title":"Example: FaissIndex","text":"<p>Before creating a custom IO class, first consider the following implementation using the Faiss library. The <code>FaissIndex</code> class extends the <code>IO</code> class and implements the <code>load</code> and <code>save</code> methods:</p> <pre><code>from dataclasses import dataclass\nfrom pathlib import Path\n\nimport faiss\nfrom ordeq import IO\n\n\n@dataclass(frozen=True, kw_only=True)\nclass FaissIndex(IO[faiss.Index]):\n    path: Path\n\n    def load(self) -&gt; faiss.Index:\n        return faiss.read_index(str(self.path))\n\n    def save(self, index: faiss.Index) -&gt; None:\n        faiss.write_index(index, str(self.path))\n</code></pre>"},{"location":"guides/custom_io/#creating-your-own-io-class","title":"Creating Your Own IO Class","text":"<p>In this section, we will go step-by-step through the creation of a simple text-based file dataset.</p>"},{"location":"guides/custom_io/#define-your-io-class","title":"Define Your IO Class","text":"<p>Create a new class that extends the <code>IO</code> class and implement the <code>load</code> and <code>save</code> methods.</p> <pre><code>from dataclasses import dataclass\nfrom pathlib import Path\n\nfrom ordeq import IO\n\n\n@dataclass(frozen=True, kw_only=True)\nclass CustomIO(IO):\n    path: Path\n\n    def load(self):\n        pass\n\n    def save(self, data):\n        pass\n</code></pre>"},{"location":"guides/custom_io/#implement-the-load-method","title":"Implement the <code>load</code> Method","text":"<p>This method should contain the logic for loading your data. For example:</p> <pre><code>def load(self):\n    return self.path.read_text()\n</code></pre>"},{"location":"guides/custom_io/#implement-the-save-method","title":"Implement the <code>save</code> Method","text":"<p>This method should contain the logic for saving your data. For example:</p> <pre><code>def save(self, data):\n    self.path.write_text(data)\n</code></pre>"},{"location":"guides/custom_io/#load-or-save-arguments","title":"Load- or save arguments","text":"<p>The <code>path</code> attribute is used by both the <code>load</code> and <code>save</code> method. It's also possible to provide parameters to the individual methods. For instance, we could let the user control the newline character used by <code>write_text</code>:</p> <pre><code>from dataclasses import dataclass\nfrom pathlib import Path\n\nfrom ordeq import IO\n\n\n@dataclass(frozen=True, kw_only=True)\nclass CustomIO(IO):\n    path: Path\n\n    def load(self):\n        return self.path.read_text()\n\n    def save(self, data, newline: str = \"\\n\"):\n        self.path.write_text(data, newline=newline)\n</code></pre> <p>A common pattern when using third party functionality is to delegate keyword arguments to another function.</p> <p>Below is an example of this for a Pandas CSV IO class:</p> <pre><code>from dataclasses import dataclass\nfrom pathlib import Path\n\nimport pandas as pd\nfrom ordeq import IO\n\n\n@dataclass(frozen=True, kw_only=True)\nclass PandasCSV(IO[pd.DataFrame]):\n    path: Path\n\n    def load(self, **load_args) -&gt; pd.DataFrame:\n        return pd.read_csv(self.path, **load_args)\n\n    def save(self, data: pd.DataFrame, **save_args):\n        data.write_csv(self.path, **save_args)\n</code></pre>"},{"location":"guides/custom_io/#providing-type-information","title":"Providing type information","text":"<p>We can provide the <code>str</code> argument to <code>IO</code> to indicate that <code>CustomIO</code> class loads and saves strings. This type should match the return type of the <code>load</code> method and the first parameter of the <code>save</code> method.</p> <pre><code>from dataclasses import dataclass\nfrom pathlib import Path\n\nfrom ordeq import IO\n\n\n@dataclass(frozen=True, kw_only=True)\nclass CustomIO(IO[str]):  # IO operates on type `str`\n    path: Path\n\n    def load(self) -&gt; str:  # and thus returns a `str` on load\n        return self.path.read_text()\n\n    def save(\n        self, data: str\n    ) -&gt; None:  # and takes a `str` as first argument to `save`\n        self.path.write_text(data)\n</code></pre>"},{"location":"guides/custom_io/#read-only-and-write-only-classes","title":"Read-only and write-only classes","text":"<p>While most data need to be loaded and saved alike, this is not always the case. If in our code one of these operations is not necessary, then we can choose to not implement them.</p> <p>Practical examples are:</p> <ul> <li>Read-only: When loading machine learning models from a third party registry where we have only read permissions (e.g. HuggingFace).</li> <li>Write-only: when a Matplotlib plot is rendered to a PNG file, we cannot load the <code>Figure</code> back from the PNG data.</li> </ul>"},{"location":"guides/custom_io/#creating-a-read-only-class-using-input","title":"Creating a Read-only class using <code>Input</code>","text":"<p>For a practical example of a class that is Read-only, we will consider generating of synthetic sensor data. The <code>SensorDataGenerator</code> class will extend the <code>Input</code> class, meaning it will only have to implement the <code>load</code> method.</p> <pre><code>import random\nfrom dataclasses import dataclass\nfrom typing import Any\n\nfrom ordeq import Input\n\n\n@dataclass(frozen=True, kw_only=True)\nclass SensorDataGenerator(Input[dict[str, Any]]):\n    \"\"\"Example Input class to generate synthetic sensor data\n\n    Example usage:\n\n    ```pycon\n    &gt;&gt;&gt; generator = SensorDataGenerator(sensor_id=\"sensor_3\")\n    &gt;&gt;&gt; data = generator.load()\n    {'sensor_id': 'sensor_3', 'temperature': 22.001252691230633, 'humidity': 35.2674852725557}\n    ```\n\n    \"\"\"\n\n    sensor_id: str\n\n    def load(self) -&gt; dict[str, Any]:\n        \"\"\"Simulate reading data from a sensor\"\"\"\n        return {\n            \"sensor_id\": self.sensor_id,\n            \"temperature\": random.uniform(20.0, 30.0),\n            \"humidity\": random.uniform(30.0, 50.0),\n        }\n</code></pre> <p>Saving data using this dataset would raise a <code>ordeq.IOException</code> explaining the save method is not implemented.</p> <p>Similarly, you can inherit from the <code>Output</code> class for IO that only require to implement the <code>save</code> method. The <code>ordeq-matplotlib</code> package contains an example of this in <code>MatplotlibFigure</code>.</p>"},{"location":"guides/kedro/","title":"Coming from Kedro","text":"<p>This guide is for users familiar with Kedro who want to get started with Ordeq. Because the frameworks are conceptually close, it's easy to transition and leverage your existing knowledge. If you are new to Ordeq, start with the introduction.</p> <p>Ordeq and Kedro share several core abstractions:</p> <ul> <li>Nodes for modular data pipelines</li> <li>Catalogs for defining and managing IO</li> </ul> <p>Ordeq offers several advantages over Kedro:</p> <ul> <li>Lighter weight and Python-first (no YAML required)</li> <li>Adding new IOs requires a fraction of the code</li> <li>Suitable for heavy data engineering and resource configuration</li> <li>Custom IOs are first-class citizens</li> </ul> <p>In this guide, we will compare a starter Kedro project with its Ordeq equivalent. You may use this as a reference when transitioning from Kedro to Ordeq.</p>"},{"location":"guides/kedro/#spaceflight-starter-project","title":"Spaceflight starter project","text":"<p>We will use the spaceflights-pandas starter project as an example. Below is the directory structure of the Kedro project, and the Ordeq equivalent.</p> <p>How to try it out yourself</p> <p>If you would like to follow this guide step-by-step:</p> <ul> <li>clone the spaceflights-pandas starter project</li> <li>create another, empty project, with the Ordeq layout described below</li> </ul> <p>You can also download the completed Ordeq project here.</p> KedroOrdeq <pre><code>conf/\n\u2514\u2500\u2500 base\n    \u2514\u2500\u2500 catalog.yml\nsrc/\n\u251c\u2500\u2500 pipeline_registry.py\n\u251c\u2500\u2500 settings.py\n\u251c\u2500\u2500 __main__.py\n\u2514\u2500\u2500 spaceflights\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 pipeline.py\n    \u2514\u2500\u2500 nodes.py\n</code></pre> <pre><code>src/\n\u251c\u2500\u2500 __main__.py\n\u251c\u2500\u2500 catalog.py\n\u2514\u2500\u2500 spaceflights\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 pipeline.py\n</code></pre>"},{"location":"guides/kedro/#migrating-the-catalog","title":"Migrating the catalog","text":"<p>Ordeq defines a catalog in code, while Kedro's catalog is YAML-based. In Kedro, catalogs entries are called datasets, while Ordeq uses IO. This section will show how to migrate each dataset in the Kedro catalog to an IO in Ordeq catalog.</p> <p>Ordeq also supports layered catalogs</p> <p>For simplicity, we assume the Kedro catalog consists of only one YAML file. Ordeq supports multiple, layered, catalogs too. For more information, see catalogs.</p> conf/base/catalog.yml (Kedro)src/catalog.py (Ordeq) <pre><code>companies:\n    type: pandas.CSVDataset\n    filepath: data/01_raw/companies.csv\n\nshuttles:\n    type: pandas.ExcelDataset\n    filepath: data/01_raw/shuttles.xlsx\n    load_args:\n        engine: openpyxl\n\npreprocessed_companies:\n    type: pandas.ParquetDataset\n    filepath: data/02_intermediate/preprocessed_companies.parquet\n\npreprocessed_shuttles:\n    type: pandas.ParquetDataset\n    filepath: data/02_intermediate/preprocessed_shuttles.parquet\n</code></pre> <pre><code>from pathlib import Path\n\nfrom ordeq_pandas import PandasCSV, PandasExcel, PandasParquet\n\ncompanies = PandasCSV(path=Path(\"data/01_raw/companies.csv\"))\n\nshuttles = PandasExcel(\n    path=Path(\"data/01_raw/shuttles.xlsx\")\n).with_load_options(engine=\"openpyxl\")\n\npreprocessed_companies = PandasParquet(\n    path=Path(\"data/02_intermediate/preprocessed_companies.parquet\")\n)\n\npreprocessed_shuttles = PandasParquet(\n    path=Path(\"data/02_intermediate/preprocessed_shuttles.parquet\")\n)\n</code></pre> <p>Switch the tabs above to see the Kedro catalog and its Ordeq equivalent. For each dataset in the Kedro catalog, we have defined an equivalent Ordeq IO:</p> <ul> <li><code>companies</code> is a <code>pandas.CSVDataset</code>, so we use the <code>ordeq_pandas.PandasCSV</code> IO</li> <li><code>shuttles</code> is a <code>pandas.ExcelDataset</code>, so we use the <code>ordeq_pandas.PandasExcel</code> IO<ul> <li>The <code>load_args</code> in Kedro are translated to <code>with_load_options</code> in Ordeq</li> </ul> </li> <li><code>preprocessed_companies</code> and <code>preprocessed_shuttles</code> are <code>pandas.ParquetDataset</code>, so we use the     <code>ordeq_pandas.PandasParquet</code> IO</li> </ul> <p>User IOs</p> <p>Ordeq provides many IOs for popular data processing libraries out-of-the-box, such as <code>PandasCSV</code> and <code>PandasParquet</code>. You can use or extend these IOs directly. Creating your own IOs is a first-class feature of Ordeq, designed to be simple and flexible. You are always in control of how data is loaded and saved. For more information, see the guide on creating user IOs.</p>"},{"location":"guides/kedro/#migrating-the-nodes-and-pipeline","title":"Migrating the nodes and pipeline","text":"<p>Next we are going to migrate the nodes and the pipeline. First, let's cover a couple of differences between Kedro and Ordeq pipelines:</p> <ul> <li>Each Kedro pipeline needs to be defined in a <code>pipeline.py</code> file</li> <li>Kedro pipelines are created using a <code>create_pipeline</code> function</li> <li>Kedro uses a string to reference the data</li> </ul> <p>In contrast:</p> <ul> <li>Ordeq pipelines can be defined anywhere</li> <li>Ordeq pipelines are Python files (or, modules)</li> <li>Ordeq uses the actual IO object to reference the data</li> </ul> <p>Below is the nodes and pipeline definition for the Kedro spaceflights project:</p> src/spaceflights/pipeline.py (Kedro)src/spaceflights/nodes.py (Kedro) <pre><code>from kedro.pipeline import Pipeline, node, pipeline\nfrom nodes import preprocess_companies, preprocess_shuttles\n\n\ndef create_pipeline(**kwargs) -&gt; Pipeline:\n    return pipeline([\n        node(\n            func=preprocess_companies,\n            inputs=\"companies\",\n            outputs=\"preprocessed_companies\",\n            name=\"preprocess_companies_node\",\n        ),\n        node(\n            func=preprocess_shuttles,\n            inputs=\"shuttles\",\n            outputs=\"preprocessed_shuttles\",\n            name=\"preprocess_shuttles_node\",\n        ),\n    ])\n</code></pre> <pre><code>import pandas as pd\n\n# ... utility methods omitted for brevity\n\n\ndef preprocess_companies(companies: pd.DataFrame) -&gt; pd.DataFrame:\n    companies[\"iata_approved\"] = _is_true(companies[\"iata_approved\"])\n    companies[\"company_rating\"] = _parse_percentage(\n        companies[\"company_rating\"]\n    )\n    return companies\n\n\ndef preprocess_shuttles(shuttles: pd.DataFrame) -&gt; pd.DataFrame:\n    shuttles[\"d_check_complete\"] = _is_true(shuttles[\"d_check_complete\"])\n    shuttles[\"moon_clearance_complete\"] = _is_true(\n        shuttles[\"moon_clearance_complete\"]\n    )\n    shuttles[\"price\"] = _parse_money(shuttles[\"price\"])\n    return shuttles\n</code></pre> <p>In Kedro, the name of the pipeline is implicitly assigned based on the folder name. In this case, the pipeline is called <code>spaceflights</code>. The datasets are bound to the nodes in the pipeline definition, using strings.</p> <p>Next, let's have a look at the Ordeq equivalent:</p> src/spaceflights/pipeline.py (Ordeq) <pre><code>import catalog\nimport pandas as pd\nfrom ordeq import node\n\n# ... utility methods omitted for brevity\n\n\n@node(inputs=catalog.companies, outputs=catalog.preprocessed_companies)\ndef preprocess_companies(companies: pd.DataFrame) -&gt; pd.DataFrame:\n    companies[\"iata_approved\"] = _is_true(companies[\"iata_approved\"])\n    companies[\"company_rating\"] = _parse_percentage(\n        companies[\"company_rating\"]\n    )\n    return companies\n\n\n@node(inputs=catalog.shuttles, outputs=catalog.preprocessed_shuttles)\ndef preprocess_shuttles(shuttles: pd.DataFrame) -&gt; pd.DataFrame:\n    shuttles[\"d_check_complete\"] = _is_true(shuttles[\"d_check_complete\"])\n    shuttles[\"moon_clearance_complete\"] = _is_true(\n        shuttles[\"moon_clearance_complete\"]\n    )\n    shuttles[\"price\"] = _parse_money(shuttles[\"price\"])\n    return shuttles\n</code></pre> <p>In Ordeq, the pipeline is defined by the module itself, so there is no need for an additional file. The IOs are bound to the nodes in the node definition, using the actual IO objects instead of strings. Note that the node functions themselves are identical in both frameworks.</p>"},{"location":"guides/kedro/#migrating-the-runner","title":"Migrating the runner","text":"<p>Running a Kedro project is done through the Kedro CLI. Ordeq projects can be run both programmatically, or using a CLI. We will first show how to set up a CLI entry point similar to Kedro's. Here's the <code>src/__main__.py</code> file for both Kedro and Ordeq:</p> src/__main__.py (Kedro)src/__main__.py (Ordeq CLI) <pre><code>import sys\nfrom pathlib import Path\nfrom typing import Any\n\nfrom kedro.framework.cli.utils import find_run_command\nfrom kedro.framework.project import configure_project\n\n\ndef main(*args, **kwargs) -&gt; Any:\n    package_name = Path(__file__).parent.name\n    configure_project(package_name)\n\n    interactive = hasattr(sys, \"ps1\")\n    kwargs[\"standalone_mode\"] = not interactive\n\n    run = find_run_command(package_name)\n    return run(*args, **kwargs)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>from ordeq_cli_runner import main\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Install the <code>ordeq-cli-runner</code> package</p> <p>To run your Ordeq project through the CLI, make sure to install the <code>ordeq-cli-runner</code> package.</p> <p>To run the Ordeq project through the CLI, you can now run:</p> <pre><code>python src/__main__.py run spaceflights.pipeline\n</code></pre> <p>Alternatively, you can run the pipeline programmatically, as follows:</p> src/__main__.py (Ordeq programmatic)<pre><code>from ordeq import run\nfrom spaceflights import pipeline\n\nrun(pipeline)\n</code></pre> <p>More info about running Ordeq projects can be found in the guide.</p>"},{"location":"guides/kedro/#other-components","title":"Other components","text":"<p>Kedro projects also have a settings file and a pipeline registry. Ordeq does not have these concepts, so there is no need to migrate them:</p> <ul> <li>Ordeq pipelines are referred to by the name of the module, so there is no need for a registry</li> <li>The settings file typically contains settings specific to the YAML-based catalog, which is not used by Ordeq</li> </ul> <p>Need help?</p> <p>You might use Kedro's more advanced features, such as parameters or hooks. Ordeq supports these features too, although the implementation might differ. If you have any questions or run into any issues, please open an issue on GitHub.</p>"},{"location":"guides/lazy_io/","title":"Lazily dataset processing","text":"<p>Loading multiple files in memory for processing at the same time can be problematic. When processing batches of files, such as partitioned parquet files in Polars or a directory of PDF files, we may run out of memory, and generally consume more resources than required.</p> <p>The <code>Iterate</code> dataset can help executing the same logic to multiple datasets, without the need to load them all in memory first.</p> <p>Instead, <code>Iterate</code> makes use of Python generators to process the datasets one by one.</p> <pre><code>from collections.abc import Iterable\nfrom pathlib import Path\n\nfrom ordeq import node\nfrom ordeq_common import Iterate\nfrom ordeq_files import JSON, Text\n\npaths = [Path(\"hello.txt\"), Path(\"world.txt\")]\ntext_dataset = Iterate(*(Text(path=path) for path in paths))\njson_dataset = Iterate(\n    *(JSON(path=path.with_suffix(\".json\")) for path in paths)\n)\n\n\n@node(inputs=text_dataset, outputs=json_dataset)\ndef generate_json_contents(\n    contents: Iterable[str],\n) -&gt; Iterable[dict[str, str]]:\n    \"\"\"Wrap the text-contents in a JSON structure.\"\"\"\n    for content in contents:\n        yield {\"content\": content}\n</code></pre>"},{"location":"guides/node_parameters/","title":"Node parameters","text":"<p>Passing parameters to nodes is a powerful way to customize their behavior without modifying the node's code. In Ordeq, parameters are just native IOs and need not be treated differently than regular inputs and outputs.</p> <p>Commonly used parameter types that are supported out of the box include:</p> <ul> <li>Python built-in types: <code>str</code>, <code>int</code>, <code>float</code>, <code>bool</code></li> <li>Configuration files (TOML, YAML, JSON, INI, etc.)</li> <li>Pydantic models or dataclass instances</li> <li>CLI arguments or environment variables</li> </ul>"},{"location":"guides/node_parameters/#using-ios-instead-of-variables-in-the-global-scope","title":"Using IOs instead of variables in the global scope","text":"<p>Although it is possible to use variables in the global scope as parameters, it is not recommended. Using IOs has several advantages:</p> <ul> <li>Clarity: It is clear which parameters a node depends on. The parameters are included in <code>run</code> and <code>viz</code> outputs.</li> <li>Maintainability: Configuration and transformations are separated, making it easier to change one without affecting the other. For example, we could easily change a parameter to be read from a configuration file or CLI argument.</li> <li>Reproducibility: When the behavior of a node is fully determined by its inputs, this makes it easier to reproduce results and avoid unintended side effects. Moreover, this means Ordeq can avoid recomputation when the inputs have not changed.</li> </ul> <p>Example using global variables (not recommended):</p> <pre><code>from ordeq import IO, node\n\nname_str = IO()\ngreeting = IO()\nexcited = False\n\n\n@node(inputs=name_str, outputs=greeting)\ndef greet(name: str) -&gt; str:\n    message = f\"Hello, {name}\"\n    if excited:\n        message += \"!\"\n    return message\n</code></pre> <p>Instead, use an IO for the <code>excited</code> parameter:</p> <pre><code>from ordeq import IO, node\nfrom ordeq_common import Literal\n\nname_str = IO()\ngreeting = IO()\nis_excited = Literal(False)\n\n\n@node(inputs=[name_str, is_excited], outputs=greeting)\ndef greet(name: str, excited: bool) -&gt; str:\n    message = f\"Hello, {name}\"\n    if excited:\n        message += \"!\"\n    return message\n</code></pre> <p>This way, the <code>greet</code> node is fully defined by its inputs and outputs, for instance making it possible to avoid recomputation when the inputs have not changed.</p>"},{"location":"guides/node_parameters/#reading-from-a-pyprojecttoml-section","title":"Reading from a <code>pyproject.toml</code> section","text":"<p>The pyproject.toml format is the standard way to configure Python projects. The <code>[tool]</code> table is intended for tool-specific configuration. Ordeq can read parameters from a <code>[tool.your_tool_name]</code> section in <code>pyproject.toml</code> and pass it to one or more nodes using little code.</p> <pre><code>from pathlib import Path\n\nfrom ordeq import IO, node\nfrom ordeq_pyproject import Pyproject\n\nname = IO()\nlanguage = Pyproject(\n    path=Path(\"pyproject.toml\"), section=\"tool.my_tool.language\"\n)\ngreeting = IO()\n\n\n@node(inputs=[name, language], outputs=greeting)\ndef greet(name: str, language: str) -&gt; str:\n    if language == \"en\":\n        return f\"Hello, {name}\"\n\n    if language == \"es\":\n        return f\"Hola, {name}\"\n\n    raise ValueError(\"Language not supported\")\n</code></pre> pyproject.toml<pre><code>[tool.my_tool]\nlanguage = \"en\"\n</code></pre> <p>The example above reads the <code>language</code> parameter from the <code>pyproject.toml</code> file and uses it to customize the greeting message. The data is passed as a dictionary to the node.</p> <p>For more information on the pyproject.toml format, see writing your pyproject.toml.</p>"},{"location":"guides/node_parameters/#using-pydantic-models","title":"Using Pydantic models","text":"<p>Pydantic models and dataclasses are a great way to define structured parameters with validation. The following example a Pydantic model that is read from a YAML file, validated and passed to a node:</p> <pre><code>from pathlib import Path\n\nfrom ordeq import IO, node\nfrom ordeq_pydantic import PydanticModel\nfrom ordeq_yaml import YAML\nfrom pydantic import BaseModel\n\n\nclass GreetingConfig(BaseModel):\n    language: str = \"en\"\n    excited: bool = False\n\n\nname = IO()\nconfig = PydanticModel(\n    io=YAML(path=Path(\"path/to/file.yml\")), model_type=GreetingConfig\n)\ngreeting = IO()\n\n\n@node(inputs=[name, config], outputs=greeting)\ndef greet(name: str, cfg: GreetingConfig) -&gt; str:\n    if cfg.language == \"en\":\n        return f\"Hello, {name}\"\n    if cfg.language == \"es\":\n        return f\"Hola, {name}\"\n\n    raise ValueError(\"Language not supported\")\n</code></pre>"},{"location":"guides/parametrized_io/","title":"Parametrizing IOs","text":"<p>A parameterized IO is an IO that uses a parameter to configure its load or save behavior. Typical examples of parameters are environment variables, command line arguments, or configuration files. These parameters can configure various aspects of the IO, such as file paths and connection strings.</p> <p>For example, you can parametrize an IO to:</p> <ul> <li>save a JSON file to a path based on a command line argument</li> <li>load an Excel sheet, where the sheet name is taken from a configuration file</li> <li>make an API request to a URL that is determined from an environment variable</li> </ul>"},{"location":"guides/parametrized_io/#approach","title":"Approach","text":"<p>To create a parameter for an IO, we need:</p> <ul> <li>a custom IO class (see the guide on custom IOs)</li> <li>the parameter IO (the environment variable, command line argument, configuration file, etc.)</li> </ul> <p>The approach is as follows:</p> <ul> <li>The custom IO class will take the parameter IO as an attribute</li> <li>The <code>load</code> and <code>save</code> methods load the parameter IO</li> <li>The <code>load</code> and <code>save</code> methods use the loaded parameter to load or save the data</li> </ul>"},{"location":"guides/parametrized_io/#examples","title":"Examples","text":""},{"location":"guides/parametrized_io/#saving-a-file-based-on-a-command-line-argument","title":"Saving a file based on a command line argument","text":"<p>The following example shows how to load a JSON file based on a command line argument. First, we create a custom class <code>JSONWithParameter</code>.</p> <p>Install <code>ordeq-args</code> and <code>ordeq-files</code></p> <p>To follow the example below, you need to install the <code>ordeq-args</code> and <code>ordeq-files</code> package.</p> json_with_parameter.py<pre><code>import json\nfrom pathlib import Path\nfrom functools import cached_property\nfrom dataclasses import dataclass\n\nfrom ordeq import Input, IO\nfrom ordeq_args import CommandLineArg\nfrom ordeq_files import JSON\n\n\n@dataclass(frozen=True)\nclass JSONWithParameter(JSON):\n    path_io: CommandLineArg[Path]\n\n    @cached_property\n    def path(self) -&gt; Path:\n        return self.path_io.load()\n</code></pre> <p>This IO inherits from the <code>JSON</code> IO in <code>ordeq-files</code>, but instead of taking a static <code>path</code> argument, it takes a <code>path_io</code> argument, which is a <code>CommandLineArg</code> IO that loads a <code>Path</code>. On load and save, the <code>path</code> property will return the path from the command line argument.</p> <p>We can use the parametrized IO as follows:</p> main.py<pre><code>from ordeq import node, run\nfrom ordeq_files import JSON\nfrom ordeq_yaml import YAML\nfrom ordeq_args import CommandLineArg\nfrom pathlib import Path\nfrom json_with_parameter import JSONWithParameter\n\nsource = YAML(path=Path(\"to/source.yaml\"))\ntarget = JSONWithParameter(path_io=CommandLineArg(name=\"--target\", type=Path))\n\n\n@node(inputs=source, outputs=target)\ndef convert_yaml_to_json(data: dict) -&gt; dict:\n    return data\n\n\nif __name__ == \"__main__\":\n    run(convert_yaml_to_json)\n</code></pre> <p>You can now run <code>main.py</code> from the command line, and specify the target path as argument:</p> <pre><code>python src/main.py --target output.json\n</code></pre> <p>This will copy the contents of <code>source.yaml</code> to <code>output.json</code>.</p>"},{"location":"guides/parametrized_io/#loading-an-excel-sheet-based-on-a-configuration-file","title":"Loading an Excel sheet based on a configuration file","text":"<p>The following example shows how to load an Excel sheet based on a configuration file. In the example we will assume the configuration is stored in a YAML.</p> <p>First, we create a custom class <code>PandasExcel</code>. Note that this class resembles the <code>PandasExcel</code> offered in <code>ordeq-pandas</code>, but with a parameter IO <code>config</code>.</p> <p>Install <code>ordeq-files</code> and <code>pandas</code></p> <p>To follow the example below, you need to install the <code>ordeq-files</code> and <code>pandas</code> packages.</p> pandas_excel.py<pre><code>from ordeq import IO, Input\nimport pandas as pd\nfrom pathlib import Path\n\n\nclass PandasExcel(Input[pd.DataFrame]):\n    def __init__(self, path: Path, config: IO[dict]):\n        self.path = path\n        self.config = config\n        super().__init__()\n\n    def load(self) -&gt; pd.DataFrame:\n        config = self.config.load()  # e.g. {\"sheet_name\": \"Sheet1\"}\n        return pd.read_excel(self.path, **config)\n</code></pre> <p>On load, the <code>PandasExcel</code> IO will load the configuration dictionary from the <code>config</code> IO, and pass the configuration as keyword arguments to <code>pd.read_excel</code>.</p> <p>We can initialize <code>PandasExcel</code> with any IO that loads a dictionary. For example, we can use the <code>YAML</code> IO from <code>ordeq-yaml</code> as a parameter:</p> <pre><code>&gt;&gt;&gt; from pandas_excel import PandasExcel\n&gt;&gt;&gt; PandasExcel(\n...     path=Path(\"data.xlsx\"),\n...     config=YAML(path=Path(\"config.yaml\"))\n... )\n</code></pre> <p>Suppose the configuration file <code>config.yaml</code> contains:</p> <pre><code>sheet_name: Sheet8\n</code></pre> <p>When we do <code>xlsx.load()</code>, it will load the Excel file <code>data.xlsx</code> using the sheet name <code>Sheet8</code>.</p> <p>Because the configuration is loaded from a file, we can change the configuration without changing the code. For instance, we can easily add more configuration to the file, such as <code>header</code> and <code>usecols</code>:</p> <pre><code>sheet_name: Sheet8\nheader: 0\nusecols: A:C\n</code></pre> <p>New keys in the configuration file will be passed as keyword arguments to <code>pd.read_excel</code>.</p> <p>Keep parameters simple</p> <p>IOs are used to load and save data, and should not perform any transformations. If you find yourself applying transformations on load or save, consider creating a node instead.</p>"},{"location":"guides/parametrized_io/#make-a-request-to-an-endpoint-from-an-environment-variable","title":"Make a request to an endpoint from an environment variable","text":"<p>Next, we will create an IO that makes a request to an endpoint, where the endpoint is created from an environment variable.</p> <p>Install <code>ordeq-args</code> and <code>requests</code></p> <p>To follow the example below, you need to install the <code>ordeq-args</code> and <code>requests</code> package.</p> user_request.py<pre><code>from ordeq_args import EnvironmentVariable\nfrom ordeq import IO\nimport requests\n\n\nclass UsersRequest(IO[requests.Response]):\n    def __init__(self, idx: IO[str]):\n        self.base_url = \"https://jsonplaceholder.typicode.com/users/\"\n        self.idx = idx\n        super().__init__()\n\n    def load(self) -&gt; requests.Response:\n        idx = self.idx.load()\n        url = f\"{self.base_url}/{idx}\"\n        return requests.get(url)\n</code></pre> <p>This IO requests users from the JSON placeholder API. You can also navigate to the URL in your browser to see the response. The environment variable contains the user ID to request.</p> <p>The <code>UsersRequest</code> IO can then be used as follows:</p> <pre><code>&gt;&gt;&gt; from user_request import UsersRequest\n&gt;&gt;&gt; from ordeq_args import EnvironmentVariable\n&gt;&gt;&gt; idx = EnvironmentVariable(name=\"USER_ID\")\n&gt;&gt;&gt; response = UsersRequest(idx=idx)\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; os.environ['USER_ID'] = '1'\n&gt;&gt;&gt; response.load()\nb'{\"id\":1,\"name\":\"Leanne Graham\", ...}'\n&gt;&gt;&gt; os.environ['USER_ID'] = '2'\n&gt;&gt;&gt; response.load()\nb'{\"id\":2,\"name\":\"Ervin Howell\", ...}'\n&gt;&gt;&gt; ...\n</code></pre> <p>Environment variables are typically used to configure sensitive information, like authentication details. The example above extends to these types of parameters as well.</p>"},{"location":"guides/parametrized_io/#best-practices","title":"Best practices","text":"<p>Parameters are a powerful way to make your IOs more flexible and reusable. By composing an IO with a parameter IO, you can easily change the behavior of the IO without changing code.</p> <p>When adding parameters to your IOs, consider the following best practices:</p> <ul> <li> <p>Keep parameters simple:     IOs should not perform any transformations.     If the load or save logic is getting complex, consider creating a node instead.</p> </li> <li> <p>Caching:     The approach above loads the parameter IO every time the <code>load</code> or <code>save</code> method is called.     You can implement caching to avoid loading the parameter multiple times.</p> </li> <li> <p>Manage reuse     If you find yourself reusing the same parameter IO across different IOs, consider creating a node instead.</p> </li> </ul> <p>Open a GitHub issue</p> <p>If you have any questions or suggestions, feel free to open an issue on GitHub!</p>"},{"location":"guides/parametrized_io/#faq","title":"FAQ","text":""},{"location":"guides/parametrized_io/#why-do-i-have-to-create-a-custom-class-to-parametrize-an-io","title":"Why do I have to create a custom class to parametrize an IO?","text":"<p>The IOs offer by Ordeq packages do not support parametrization out of the box. We recommend custom IOs for parametrization, as this approach provides most flexibility.</p>"},{"location":"guides/parametrized_io/#what-if-i-want-to-fall-back-to-other-parameters","title":"What if I want to fall back to other parameters?","text":"<p>If you have multiple parameters providing you the same information, you can implement a fallback mechanism with a custom IO. For instance, you can create a custom IO that first checks if a command line argument is provided, and if not, falls back to the environment variable.</p>"},{"location":"guides/run_and_viz/","title":"Running and visualizing nodes","text":"<p>The <code>run</code> and the <code>viz</code> functions are companion tools that allow you to execute and visualize the nodes defined in your code.</p>"},{"location":"guides/run_and_viz/#run","title":"Run","text":"<p>Running nodes with Ordeq is as simple as calling the <code>run</code> function from the <code>ordeq</code> package:</p> main.py<pre><code>from ordeq import run, node\n\n\n@node\ndef my_node():\n    print(\"Hello, Ordeq!\")\n\n\nif __name__ == \"__main__\":\n    run(my_node)\n</code></pre> <p>This function accepts any number of functions, modules, or packages as input and will execute the nodes defined within them. This allows you to adapt the structure of your codebase to the complexity of your project. For small projects, you might keep everything in a single script. To run all nodes defined in that script, simply pass the module itself to <code>run</code>:</p> main.py<pre><code>from ordeq import run, node\n\n\n@node\ndef my_node():\n    print(\"Hello, Ordeq!\")\n\n\nif __name__ == \"__main__\":\n    run(__name__)\n</code></pre> <p>For larger projects, you can organize your nodes into separate modules or packages. For example, separating nodes into a different module <code>nodes.py</code> with identical functionality:</p> main.py<pre><code>from ordeq import run\nimport nodes\n\nif __name__ == \"__main__\":\n    run(nodes)\n</code></pre> nodes.py<pre><code>from ordeq import node\n\n\n@node\ndef my_node():\n    print(\"Hello, Ordeq!\")\n</code></pre>"},{"location":"guides/run_and_viz/#logging","title":"Logging","text":"<p>By default, Ordeq logs execution progress using the <code>INFO</code> level. The default Python logging level is <code>WARNING</code>, so to see Ordeq logs, you need to set the logging level to <code>INFO</code> or lower:</p> main.py<pre><code>import logging\nfrom ordeq import run\nimport nodes\n\nlogging.basicConfig(level=logging.INFO)\n\nif __name__ == \"__main__\":\n    run(nodes)\n</code></pre>"},{"location":"guides/run_and_viz/#viz","title":"Viz","text":"<p>The <code>viz</code> function from the <code>ordeq_viz</code> package allows you to visualize the nodes and their dependencies in a graph format:</p> main.py<pre><code>from pathlib import Path\nfrom ordeq_viz import viz\nimport nodes\n\nif __name__ == \"__main__\":\n    viz(nodes, fmt=\"mermaid\", output=Path(\"pipeline.mermaid\"))\n</code></pre> <p>Just as <code>run</code>, the <code>viz</code> function accepts functions, modules, or packages as input and will generate a visual representation of the nodes and their dependencies.</p>"},{"location":"guides/run_and_viz/#notebooks","title":"Notebooks","text":"<p>In notebook environments, you can directly visualize the graph without saving it to a file:</p> notebook.ipynb<pre><code>from ordeq_viz import viz\nfrom IPython.display import display, Markdown\nimport nodes\n\ndiagram = viz(nodes, fmt=\"mermaid\")\ndisplay(Markdown(diagram))\n</code></pre> <p>Jupyter supports this since version 7.1.</p> <p>Similarly for Marimo notebooks, you can display the diagram directly:</p> notebook.py<pre><code>from ordeq_viz import viz\nimport nodes\nimport marimo as mo\n\ndiagram = viz(nodes, fmt=\"mermaid\")\nmo.mermaid(diagram)\n</code></pre>"},{"location":"guides/run_and_viz/#combining-run-and-viz","title":"Combining run and viz","text":"<p>You can also combine both <code>run</code> and <code>viz</code> in a single script to execute the nodes and visualize the workflow:</p> main.py<pre><code>from pathlib import Path\nfrom ordeq import run\nfrom ordeq_viz import viz\nimport nodes\n\nif __name__ == \"__main__\":\n    run(nodes)\n    viz(nodes, fmt=\"mermaid\", output=Path(\"pipeline.mermaid\"))\n</code></pre> <p>This is particularly powerful for debugging and understanding complex workflows.</p> <p>Split screen development</p> <p>Split screen view in your IDE is very handy for working with source code, <code>run</code> and <code>viz</code> outputs side by side.</p>"},{"location":"guides/starting_a_new_project/","title":"Starting a new project","text":"<p>This guide is meant to help users set up a new project with Ordeq. It assumes you are familiar with Ordeq's core concepts, such as IO and nodes. We will discuss several common setups for projects that use Ordeq, and their pro's and cons.</p> <p>We use uv for package and project management throughout this guide. Check out the uv documentation for installation and usage instructions.</p>"},{"location":"guides/starting_a_new_project/#single-file-projects","title":"Single-file projects","text":"<p>Single-file projects are a great option if you want to run a quick test or experiment. As the name implies, a single-file project contains only one Python module.</p> <p>Using notebooks</p> <p>For users interested in using Ordeq with notebooks, we will create a dedicated guide.</p> <p>To create a single-file project, navigate to the desired project directory and run:</p> <pre><code>uv init\n</code></pre> <p>This will create the following project structure:</p> <pre><code>project\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 main.py\n\u2514\u2500\u2500 pyproject.toml\n</code></pre> <p>The <code>README.md</code> contains documentation on the project, while the <code>pyproject.toml</code> defines the project's dependencies.</p> <p>Next, install Ordeq from the PyPi registry:</p> <pre><code>uv add ordeq\n</code></pre> <p>This will add the <code>ordeq</code> package to the <code>pyproject.toml</code> , so that you can use Ordeq in your code.</p> <p>Running scripts with uv</p> <p>uv also supports running scripts with without <code>pyproject.toml</code>. More info can be found here.</p>"},{"location":"guides/starting_a_new_project/#defining-the-pipeline","title":"Defining the pipeline","text":"<p>Now you can start editing <code>main.py</code> and create your pipeline. The following example pipeline loads data from an API endpoint, parses it, and saves the result to a YAML:</p> <p>Install <code>ordeq-yaml</code> and <code>ordeq-requests</code></p> <p>To run the example below yourself, make sure to install <code>ordeq-yaml</code> and <code>ordeq-packages</code>.</p> project/main.py<pre><code>from ordeq import node, run\nfrom ordeq_requests import ResponseJSON\nfrom ordeq_yaml import YAML\nfrom pathlib import Path\n\nuser = ResponseJSON(url=\"https://jsonplaceholder.typicode.com/users/1\")\nyaml = YAML(path=Path(\"users.yml\"))\n\n\n@node(inputs=user, outputs=yaml)\ndef parse_users(user: dict) -&gt; dict:\n    return {\n        \"id\": user[\"id\"],\n        \"address\": f\"{user[\"address\"][\"street\"]} {user[\"address\"][\"zipcode\"]}\"\n    }\n\nif __name__ == \"__main__\":\n    run(parse_users)\n</code></pre> <p>The nodes and IOs are defined in the same file, together with the run script. When you're ready to run this pipeline, use:</p> <pre><code>uv run main.py\n</code></pre> <p>No need to activate the virtual environment</p> <p>uv automatically activates the virtual environment when running commands like <code>uv run</code>. You do not need to manually activate the virtual environment.</p> <p>The single file starter project can be downloaded here.</p>"},{"location":"guides/starting_a_new_project/#multiple-files-packaging","title":"Multiple files: packaging","text":"<p>Single files are convenient for simple pipelines. Most pipelines are more complex, and therefore it's better to divide your project into multiple files. This allows you to maintain, test, document, and (re)use parts individually. For instance, we recommend defining your IOs in a catalog, separately from the nodes.</p> <p>In Python, a collection of files is called a package. A packaged project may look as follows:</p> <pre><code>project\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500 package\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 __main__.py\n        \u251c\u2500\u2500 catalog.py\n        \u2514\u2500\u2500 pipeline.py\n</code></pre> <p>As before, we have the <code>README.md</code> and <code>pyproject.toml</code>. In addition:</p> <ul> <li>the pipeline has been moved to the <code>pipeline</code> module</li> <li>we've added a catalog</li> <li><code>__main__.py</code> will be used to run the project</li> </ul> <p>src-layout vs flat-layout</p> <p>There are two common setups for multi-file projects: the src-layout and the flat layout. The example above uses a src-layout. There are pro's and cons to both options. See this discussion for more details. We will use a src-layout in the remainder of this guide.</p> <p>Click on the tabs below to see how the example pipeline looks in the package layout:</p> src/package/__main__.pysrc/package/catalog.pysrc/package/pipeline.pysrc/package/__init__.py <pre><code>import pipeline\nfrom ordeq import run\n\nif __name__ == \"__main__\":\n    run(pipeline)\n</code></pre> <pre><code>from pathlib import Path\n\nfrom ordeq_requests import ResponseJSON\nfrom ordeq_yaml import YAML\n\nuser = ResponseJSON(url=\"https://jsonplaceholder.typicode.com/users/1\")\nyaml = YAML(path=Path(\"users.yml\"))\n</code></pre> <pre><code>import catalog\nfrom ordeq import node\n\n\n@node(inputs=catalog.user, outputs=catalog.yaml)\ndef parse_users(user: dict) -&gt; dict:\n    return {\n        \"id\": user[\"id\"],\n        \"address\": f\"{user['address']['street']} {user['address']['zipcode']}\",\n    }\n</code></pre> <pre><code># This file can be empty: it's only needed to allow imports of this package\n</code></pre> <p>The package can be downloaded here.</p>"},{"location":"guides/starting_a_new_project/#sub-pipelines","title":"Sub-pipelines","text":"<p>As your project grows, it's a good idea to modularize even further. A common way to do this is to break up the pipeline into sub-pipelines. Each sub-pipeline is represented by its own module or package.</p> <p>Suppose you are working on a machine learning pipeline that consists of pre-processing, actual processing, and post-processing. Your project structure may look as follows:</p> <pre><code>project\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500 package\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 __main__.py\n        \u251c\u2500\u2500 catalog.py\n        \u2514\u2500\u2500 ml\n            \u251c\u2500\u2500 __init__.py\n            \u251c\u2500\u2500 inference.py\n            \u251c\u2500\u2500 postprocessing.py\n            \u2514\u2500\u2500 preprocessing.py\n</code></pre> <p>The <code>ml</code> package contains all nodes of the pipeline, but the nodes are now distributed across modules. This makes your project easier to maintain. Each module represents a sub-pipeline, which you can run and visualize individually. More info can be found here.</p> <p>For more complex projects, it makes sense to bring even more structure. For instance, you might work on a project with two pipelines: one which processes data for The Netherlands, and another for data for the UK. Suppose these pipelines share the same source data, but require different transformations.</p> <p>This project can be structured with one package for <code>nl</code> and one for <code>usa</code>:</p> <pre><code>project\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500 package\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 __main__.py\n        \u251c\u2500\u2500 catalog.py\n        \u251c\u2500\u2500 nl\n        \u2502   \u251c\u2500\u2500 __init__.py\n        \u2502   \u251c\u2500\u2500 inference.py\n        \u2502   \u251c\u2500\u2500 postprocessing.py\n        \u2502   \u2514\u2500\u2500 preprocessing.py\n        \u2514\u2500\u2500 usa\n            \u251c\u2500\u2500 __init__.py\n            \u251c\u2500\u2500 inference.py\n            \u251c\u2500\u2500 postprocessing.py\n            \u2514\u2500\u2500 preprocessing.py\n</code></pre> <p>You can download the example package above here.</p> <p>Catalogs and multi-package projects</p> <p>One reason to use catalogs is to ease reuse of IOs across your project. If the IOs are shared by multiple packages, it makes sense to create one <code>catalog.py</code> under <code>src</code> directly. You can also create a catalog in the packages themselves, but this has the downside that it's less easy to import.</p>"},{"location":"guides/starting_a_new_project/#nested-pipelines","title":"Nested pipelines","text":"<p>You can also nest packages to reflect nested sub-pipelines:</p> <pre><code>project/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500 package\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 __main__.py\n        \u251c\u2500\u2500 catalog.py\n        \u2514\u2500\u2500 nl                          # pipeline (package)\n            \u251c\u2500\u2500 __init__.py\n            \u251c\u2500\u2500 inference.py            # sub-pipeline (module)\n            \u251c\u2500\u2500 postprocessing.py       # sub-pipeline (module)\n            \u2514\u2500\u2500 preprocessing           # sub-pipeline (package)\n                \u251c\u2500\u2500 __init__.py\n                \u251c\u2500\u2500 cleaning.py         # sub-sub-pipeline (module)\n                \u2514\u2500\u2500 standardization.py  # sub-sub-pipeline (module)\n</code></pre> <p>The complete example can be downloaded here.</p> <p>Pipelines in the same source folder should have shared dependencies</p> <p>Pipelines that reside in the same source folder should have shared dependencies, like the catalog and libraries. If the dependencies do not overlap, it makes more sense to create a separate project. Projects with different dependencies can still share the same codebase. More info here.</p>"},{"location":"guides/starting_a_new_project/#best-practices","title":"Best practices","text":"<p>Here are some tips to get you started:</p> <ul> <li>Start simple, with a single file, or a package with just a catalog and a pipeline</li> <li>Incrementally add nodes to the pipeline</li> <li>Identify the (sub-)pipelines in your project, and create a module or package for each</li> </ul> <p>If you're unsure how to set up your project with Ordeq, feel free to create an issue on GitHub.</p>"},{"location":"guides/starting_a_new_project/#developing-libraries","title":"Developing libraries","text":"<p>The examples above concern executable projects that can run one or more pipelines. A type of project that we have not covered yet is a library. Library projects are typically not directly executed, but are meant to be installed and reused in another project.</p> <p>Consider, for example:</p> <ul> <li>a machine learning framework that runs an Ordeq pipeline under the hood</li> <li>a catalog library, containing IOs for reuse across different projects and codebases</li> <li>an extension of Ordeq, like <code>ordeq-pandas</code> or <code>ordeq-viz</code></li> </ul> <p>Interested in extending Ordeq?</p> <p>If you would like to extend Ordeq, have a look at the contribution guide.</p> <p>Suppose you want to create a library that uses Ordeq. To initialize your library project, run:</p> <pre><code>uv init --lib\n</code></pre> <p>The layout of the created project is similar to the package layout, but:</p> <ul> <li>the project does not contain a <code>__main__.py</code> entrypoint</li> <li>the <code>pyproject.toml</code> contains dependencies needed to build and distribute your library</li> </ul>"},{"location":"guides/testing_nodes/","title":"Testing nodes","text":"<p>Because nodes behave like plain Python functions, they can be tested using any Python testing framework. Let's reconsider the <code>greet</code> node from the node concepts section:</p> nodes.pycatalog.py <pre><code>import catalog\nfrom ordeq import node\n\n\n@node(inputs=catalog.names, outputs=catalog.greetings)\ndef greet(names: tuple[str, ...]) -&gt; list[str]:\n    \"\"\"Returns a greeting for each person.\"\"\"\n    greetings = []\n    for name in names:\n        greetings.append(f\"Hello, {name}!\")\n    return greetings\n</code></pre> <pre><code>from pathlib import Path\n\nfrom ordeq_files import CSV, Text\n\nnames = CSV(path=Path(\"names.csv\"))\ngreetings = Text(path=Path(\"greetings.txt\"))\n</code></pre> <p>This node can be unit-tested as follows:</p> <pre><code>from nodes import greet\n\n\ndef test_greet_empty():\n    assert greet() == []\n\n\ndef test_greet_one_name():\n    assert greet([\"Alice\"]) == [\"Hello, Alice!\"]\n\n\ndef test_greet_two_names():\n    assert greet([\"Alice\", \"Bob\"]) == [\"Hello, Alice!\", \"Hello, Bob!\"]\n\n\ndef test_greet_special_chars():\n    assert greet([\"A$i%*c\"]) == [\"Hello, A$i%*c!\"]\n</code></pre> <p>These tests only test the transformations. They do not load or save any data, and do not use any hooks. This is a good practice for unit tests, as it keeps them fast and isolated.</p>"},{"location":"guides/testing_nodes/#running-nodes-in-tests","title":"Running nodes in tests","text":"<p>Alternatively, you can test nodes by running them. This will load the data from the node inputs, and save the returned data to the node outputs:</p> <pre><code>from catalog import greetings\nfrom nodes import greet\nfrom ordeq import run\n\n\ndef test_run_greet():\n    run(greet)\n    assert greetings.load() == [\n        \"Hello, Abraham!\",\n        \"Hello, Adam!\",\n        \"Hello, Azul!\",\n        ...,\n    ]\n</code></pre> <p>In contrast to the unit tests, this test depends on the content of the CSV file used as input to <code>greet</code>.</p>"},{"location":"guides/testing_nodes/#running-nodes-with-alternative-io","title":"Running nodes with alternative IO","text":"<p>Many times we do not want to connect to a real file system or database when testing. This can be because connecting to the real data is slow, or because we do not want the tests to change the actual data. Instead, we want to test the logic with some seed data, often stored locally.</p> <p>Suppose reading from <code>greetings</code> is very expensive, because it is a large file. We can use a local file with the same structure to test the node:</p> <pre><code>from pathlib import Path\n\nfrom catalog import greetings, names\nfrom nodes import greet\nfrom ordeq import run\nfrom ordeq_files import CSV, Text\n\n\ndef test_run_greet():\n    local_names = CSV(path=Path(\"to/local/names.csv\"))\n    local_greetings = Text(path=Path(\"to/local/greetings.txt\"))\n    run(greet, io={names: local_names, greetings: local_greetings})\n    assert local_greetings.load() == [\n        \"Hello, Abraham!\",\n        \"Hello, Adam!\",\n        \"Hello, Azul!\",\n        ...,\n    ]\n</code></pre> <p>When <code>greet</code> is run, Ordeq will use the <code>local_names</code> and <code>local_greetings</code> IOs as replacements of the <code>names</code> and <code>greetings</code> defined in the catalog.</p>"},{"location":"guides/testing_nodes/#io-fixtures","title":"IO fixtures","text":"<p>You can also use the <code>io</code> argument to <code>run</code> as a fixture in your tests. This allows you to define the IOs once and reuse them multiple times.</p> <pre><code>from pathlib import Path\n\nimport pytest\nfrom catalog import greetings, names\nfrom ordeq import IO, Input, Output\nfrom ordeq_files import CSV, Text\n\n\n@pytest.fixture(scope=\"session\")\ndef io() -&gt; dict[IO | Input | Output, IO | Input | Output]:\n    \"\"\"Mapping of node inputs and outputs to the inputs and outputs used throughout tests.\"\"\"\n    return {\n        names: CSV(path=Path(\"to/local/names.csv\")),\n        greetings: Text(path=Path(\"to/local/greetings.txt\")),\n    }\n</code></pre> <p>Now we can use the <code>io</code> fixture in our tests:</p> <pre><code>import catalog\nfrom nodes import greet\nfrom ordeq import run\n\n\ndef test_run_greet(io):\n    run(greet, io=io)\n    assert io[catalog.greetings].load() == [\n        \"Hello, Abraham!\",\n        \"Hello, Adam!\",\n        \"Hello, Azul!\",\n        ...,\n    ]\n</code></pre> <p>For more information on the fixture scope, refer to the <code>pytest</code> documentation.</p>"},{"location":"guides/examples/benchmark_node_runtime/","title":"Benchmarking node runtime","text":"<p>Node hooks can be used to keep track of node execution durations. Using hooks to achieve this has the advantage that the timing does not add cognitive complexity to the dataset transformations. It also avoids repetitive boilerplate code inside the nodes.</p> timer_hook.py<pre><code>from time import perf_counter\n\nfrom ordeq import Node, NodeHook\n\n\nclass TimerHook(NodeHook):\n    \"\"\"Hook to time nodes.\"\"\"\n\n    def __init__(self):\n        self.node_timing = {}\n\n    def before_node_run(self, node: Node) -&gt; None:\n        self.node_timing[hash(node)] = perf_counter()\n\n    def after_node_run(self, node: Node) -&gt; None:\n        start = self.node_timing[hash(node)]\n        end = perf_counter()\n        function_name = node.func.__name__\n        print(f\"Executing `{function_name}` took {end - start:.2f}s\")\n</code></pre> <p>Usage:</p> <pre><code>from ordeq import node, run\nfrom timer_hook import TimerHook\n\n\n@node()\ndef my_transformation_function(data): ...\n\n\nrun(my_transformation_function, hooks=[TimerHook()])  # doctest: +SKIP\n</code></pre>"},{"location":"guides/integrations/docker/","title":"Docker","text":"<p>Docker is a popular tool for packaging applications and their dependencies into containers. Containers ensure your application runs consistently across different systems. Ordeq applications can be easily packaged and run in Docker containers. This guide shows how to package a simple hello-world pipeline in a Docker container and run it.</p>"},{"location":"guides/integrations/docker/#making-your-project-runnable","title":"Making your project runnable","text":"<p>First, make your project runnable by creating an entrypoint. In Python, this is typically a <code>__main__.py</code> script at the source root. For example, your project structure might look like:</p> <pre><code>src\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 __main__.py\n\u2514\u2500\u2500 pipeline.py\n</code></pre> <p>For this guide, <code>pipeline.py</code> contains a simple node:</p> src/pipeline.py<pre><code>from ordeq import node\n\n@node\ndef hello_world() -&gt; None:\n    print(\"Hello, World!\")\n</code></pre> <p>The content of <code>__main__.py</code> depends on how you want to run your application. Ordeq supports two main approaches:</p> <ul> <li>Using the command line interface (<code>ordeq-cli-runner</code> package)</li> <li>Running programmatically (using the <code>ordeq</code> package)</li> </ul> <p>Use the CLI if you need to choose pipelines dynamically. If your application always runs the same pipeline, running programmatically is simpler.</p> <p>Reference implementations:</p> src/__main__.py (Ordeq CLI)src/__main__.py (Programmatic) <pre><code>from ordeq_cli_runner import main\n\nif __name__ == \"__main__\":\n    # Invoke the CLI of ordeq-cli-runner\n    main()\n</code></pre> <pre><code>import pipeline\nfrom ordeq import run\n\nif __name__ == \"__main__\":\n    # Run the pipeline programmatically\n    run(pipeline)\n</code></pre> <p>This entrypoint makes your project runnable in any environment, not just containers. For more details, see this guide.</p>"},{"location":"guides/integrations/docker/#packaging-your-application","title":"Packaging your application","text":"<p>Next, package your application in a Docker container. Start by choosing a base image. We recommend following the uv Docker guide for setting up your base image and dependencies.</p> <p>Here\u2019s an example Dockerfile using a uv base image:</p> <pre><code># Use a Python image with uv pre-installed\nFROM ghcr.io/astral-sh/uv:python3.12-bookworm-slim\n\n# Copy the source code into the image\nADD . /app\n\n# Place executables in the environment at the front of the path\nENV PATH=\"/app/.venv/bin:$PATH\"\n\n# Sync the project into a new environment\nWORKDIR app\nRUN uv sync\n\n# Set the entrypoint to run the Ordeq application\nENTRYPOINT [\"uv\", \"run\", \"src/app/__main__.py\", \"run\"]\n</code></pre> <p>You can inspect the example project here.</p> <p>Build the Docker image with:</p> <pre><code>docker build -t app .\n</code></pre> <p>This creates a Docker image named <code>app</code> containing your Ordeq application. The user only needs the image to run the application; they don\u2019t need to know anything about Ordeq or Python.</p>"},{"location":"guides/integrations/docker/#running-your-application","title":"Running your application","text":"<p>To run the application in a container:</p> <pre><code>docker run app pipeline:hello_world\n</code></pre> <p>This command starts a container from the <code>app</code> image and runs the <code>hello_world</code> node in the <code>pipeline</code> pipeline. You can replace <code>pipeline:hello_world</code> with any arguments your entrypoint script accepts. The output for our example should be:</p> <pre><code>Hello, World!\n</code></pre> <p>Docker configuration</p> <p>Real-world applications often require additional configuration when running in Docker containers, such as networking, configuration, and secrets. Most of these are specific to your application, not to Ordeq. See the Docker documentation for more details.</p> <p>Questions, issues, or remarks</p> <p>If you have any questions, issues or remarks on integrating Ordeq with Docker, please reach out by opening an issue on GitHub.</p>"},{"location":"guides/integrations/marimo/","title":"Marimo","text":"<p>Marimo is an open-source reactive Python notebook. An improved version of Jupyter notebooks, Marimo allows you to build data applications with interactive widgets and reactive cells.</p> <p>This example demonstrates how to use Ordeq within a Marimo notebook to create an interactive data application.</p> <p>Even though this example is written for Marimo, you can run similar code in a Jupyter notebook as well to run Ordeq in the interactive environment of your choice.</p> <p>Try it out directly in the embedded Marimo notebook below:</p> <p></p> Source code for <code>examples/integration-marimo/src/marimo_example.py</code> <p>Tip: paste this code into an empty cell, and the marimo editor will create cells for you</p> <pre><code># ruff: noqa\nimport marimo\n\n__generated_with = \"0.17.0\"\napp = marimo.App(width=\"medium\")\n\n\n@app.cell\ndef _():\n    import marimo as mo\n\n    return (mo,)\n\n\n@app.cell\ndef _(mo):\n    mo.md(\n        r\"\"\"\n    ## Ordeq\n\n    Ordeq is a framework for developing data pipelines. It simplifies IO and modularizes pipeline logic.\n    \"\"\"\n    )\n\n\n@app.cell\ndef _(mo):\n    mo.md(\n        r\"\"\"\n    Ordeq allows you to focus only on the business logic required for your pipeline and not on the I/O operations.\n\n    First, you define what is the Input data your pipeline needs as well as the what Output data that your pipeline produces. These are your **IO objects**.\n\n    Typically you'd put them in a file called `catalog.py`\n    \"\"\"\n    )\n\n\n@app.cell\ndef _():\n    # catalog.py\n    from pathlib import Path\n\n    from ordeq_polars import PolarsEagerCSV\n\n    data_dir = Path(__file__).parent / \"data\"\n    data_dir.mkdir(exist_ok=True)\n\n    # Input data\n    user_data = PolarsEagerCSV(path=data_dir / \"users.csv\")\n\n    # Output data\n    clean_users_data = PolarsEagerCSV(path=data_dir / \"clean_users.csv\")\n    user_metrics = PolarsEagerCSV(path=data_dir / \"user_metrics.csv\")\n    return clean_users_data, user_data, user_metrics\n\n\n@app.cell\ndef _(mo):\n    mo.md(\n        r\"\"\"Let's first create a Polars DataFrame containing our user data, then use the `PolarsEagerCSV` IO to write it to a CSV file.\"\"\"\n    )\n\n\n@app.cell\ndef _():\n    import polars as pl\n\n    user_data_df = pl.DataFrame([\n        {\n            \"Name\": \"John Doe\",\n            \"Email\": \"john@example.com\",\n            \"Phone\": \"555-123-4567\",\n        },\n        {\n            \"Name\": \"Jane Smith\",\n            \"Email\": \"jane@example.com\",\n            \"Phone\": \"555-987-6543\",\n        },\n        {\n            \"Name\": \"Peter Jones\",\n            \"Email\": \"peter@sample.com\",\n            \"Phone\": \"555-555-1212\",\n        },\n        {\n            \"Name\": \"Rachel Adams\",\n            \"Email\": \"rachel.adams@test.com\",\n            \"Phone\": \"555-444-3333\",\n        },\n        {\n            \"Name\": \"Emily Davis\",\n            \"Email\": \"emily.davis@sample.com\",\n            \"Phone\": \"555-333-2222\",\n        },\n        {\n            \"Name\": \"Michael Brown\",\n            \"Email\": \"michael.brown@example2.com\",\n            \"Phone\": \"555-222-1111\",\n        },\n        {\n            \"Name\": \"Jane L. Smith\",\n            \"Email\": \"jane@example.com\",\n            \"Phone\": \"555-987-6543\",\n        },\n    ])\n    user_data_df\n    return pl, user_data_df\n\n\n@app.cell\ndef _(user_data, user_data_df):\n    # Write the data to a file using the IO object's .save() method:\n    user_data.save(user_data_df)\n\n\n@app.cell\ndef _(user_data):\n    # You can also load the data from using the IO object's .load() method:\n\n    user_data.load()\n\n\n@app.cell\ndef _(mo):\n    mo.md(\n        r\"\"\"\n    **Nodes** are Python functions decorated with `@node`, which implement the business logic of your pipeline.\n\n    Ordeq automatically loads and passes the `IO` objects that you mark as `inputs` of the node to the function and saves the data returned by the function to the `IO` objects marked as `outputs`. This way you don't need to worry about the inputs and outputs of your transformations and only focus on the business logic of your application.\n    \"\"\"\n    )\n\n\n@app.cell\ndef _(clean_users_data, pl, user_data, user_metrics):\n    # nodes.py\n    from ordeq import node\n    from polars import DataFrame\n\n    @node(inputs=[user_data], outputs=[clean_users_data])\n    def clean_users(user_data_df: DataFrame) -&gt; DataFrame:\n        return user_data_df.select(\n            pl.col(\"Name\").alias(\"name\"),\n            pl.col(\"Email\").alias(\"email\"),\n            pl.col(\"Phone\").alias(\"phone\"),\n            pl.col(\"Email\").str.split(\"@\").list.get(-1).alias(\"email_domain\"),\n        )\n\n    @node(inputs=[clean_users_data], outputs=[user_metrics])\n    def extract_user_metrics(clean_users_df: DataFrame) -&gt; DataFrame:\n        return clean_users_df.select(\n            pl.len().alias(\"user_cnt\"),\n            pl.col(\"email\").n_unique().alias(\"unique_users\"),\n            pl.col(\"email_domain\").n_unique().alias(\"unique_email_domains\"),\n            pl.col(\"phone\").n_unique().alias(\"unique_phone_numbers\"),\n        )\n\n    return clean_users, extract_user_metrics\n\n\n@app.cell\ndef _(mo):\n    mo.md(\n        r\"\"\"You can visualize the pipeline you've built by using the `ordeq-viz` package\"\"\"\n    )\n\n\n@app.cell\ndef _(clean_users, extract_user_metrics, mo):\n    from ordeq_viz import viz\n\n    diagram = viz(clean_users, extract_user_metrics, fmt=\"mermaid\")\n    mo.mermaid(diagram)\n\n\n@app.cell\ndef _(mo):\n    mo.md(\n        r\"\"\"Then run the pipeline by using `ordeq.run`, which takes the same arguments as `viz`\"\"\"\n    )\n\n\n@app.cell\ndef _(clean_users, extract_user_metrics):\n    from ordeq import run\n\n    run(clean_users, extract_user_metrics)\n\n\n@app.cell\ndef _(mo):\n    mo.md(r\"\"\"Inspect the saved outputs\"\"\")\n\n\n@app.cell\ndef _(clean_users_data):\n    clean_users_data.load()\n\n\n@app.cell\ndef _(user_metrics):\n    user_metrics.load()\n\n\n@app.cell\ndef _():\n    return\n\n\nif __name__ == \"__main__\":\n    app.run()\n</code></pre>"},{"location":"guides/integrations/streamlit/","title":"Streamlit","text":"<p>Streamlit is a lightweight framework for building interactive web apps with Python. It is ideal for quickly sharing tools, data workflows, and prototypes with users, stakeholders, or collaborators. As this guide will show, Ordeq can be integrated into Streamlit apps with minimal effort.</p> <p>Streamlit docs</p> <p>If you are new to Streamlit, consider checking out the Streamlit documentation for more information.</p>"},{"location":"guides/integrations/streamlit/#example-project","title":"Example project","text":"<p>We will cover the example project located here. This project does the following:</p> <ul> <li>Creates a Streamlit app with three widgets: a checkbox, a slider, and a button</li> <li>When the button is clicked, an Ordeq pipeline is triggered</li> <li>The pipeline contains a single node that prints the values of the checkbox and slider</li> </ul> <p>Of course, your project can be more complex, with multiple nodes and data flows. For instance, you could have a slider that sets a parameter for a machine learning model, or a checkbox that toggles certain features in the pipeline.</p> <p>Here's the example project structure:</p> <pre><code>integration_streamlit\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500 example\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 app.py\n        \u251c\u2500\u2500 catalog.py\n        \u251c\u2500\u2500 element.py\n        \u2514\u2500\u2500 pipeline.py\n</code></pre> <p>The <code>app.py</code> file contains the Streamlit app code, while <code>pipeline.py</code> defines the Ordeq pipeline. The project catalog is contained in <code>catalog.py</code>. Lastly, <code>element.py</code> defines a user IO that retrieves values from Streamlit widgets. Click on the tables below to see the contents of each file:</p> src/example/app.pysrc/example/element.pysrc/example/catalog.pysrc/example/pipeline.py <pre><code>import pipeline\nimport streamlit as st\nfrom ordeq import run\n\nst.checkbox(\"Checkbox\", key=\"checkbox\")\nst.slider(\"Slider\", 0, 100, key=\"slider\")\nst.button(\"Run pipeline\", on_click=lambda: run(pipeline))\n</code></pre> <pre><code>from dataclasses import dataclass\nfrom typing import TypeVar\n\nimport streamlit as st\nfrom ordeq import Input\n\nT = TypeVar(\"T\")\n\n\n@dataclass(frozen=True)\nclass StreamlitElement(Input[T]):\n    key: str | int\n\n\ndef load(self) -&gt; T:\n    \"\"\"Loads the value from the Streamlit session state.\n\n    Returns:\n        The value associated with the specified key in the session state.\n\n    Raises:\n        StreamlitAPIException:\n            If the specified key does not exist in the session state.\n    \"\"\"\n    return st.session_state[self.key]\n</code></pre> <pre><code>from element import StreamlitElement\n\nslider = StreamlitElement(key=\"slider\")\ncheckbox = StreamlitElement(key=\"checkbox\")\n</code></pre> <pre><code>import catalog\nfrom ordeq import node\n\n\n@node(inputs=[catalog.checkbox, catalog.slider])\ndef display_values(checkbox: bool, slider: int) -&gt; None:\n    # Simply print the values to the console.\n    # (Put your own logic here.)\n    print(f\"Checkbox is {checkbox}\")\n    print(f\"Slider value is {slider}\")\n</code></pre> <p>Why create an IO for Streamlit elements?</p> <p>You might wonder why we use an IO for Streamlit elements, instead of directly accessing <code>st.session_state</code> in the node. By using an IO, we isolate the transformation from retrieving values from Streamlit widgets. We can test the node logic independently of the Streamlit app, improving modularity and testability. You can easily swap out the data source or mock the inputs during testing without modifying the node logic.</p>"},{"location":"guides/integrations/streamlit/#running-the-app","title":"Running the app","text":"<p>To install the dependencies for the example project:</p> <pre><code>uv sync\n</code></pre> <p>New to uv?</p> <p>If you are unfamiliar with uv, check out the installation guide.</p> <p>Next, you can launch the example application as follows:</p> <pre><code>uv run streamlit run src/example/app.py\n</code></pre> <p>Open your browser at <code>http://localhost:8501</code> to view the app. This will show the following:</p> <p></p> <p>Clicking the button triggers an Ordeq pipeline. The pipeline prints the configured values of the checkbox and slider to the console, for example:</p> <pre><code>Checkbox is False\nSlider value is 0\n</code></pre> <p>Try playing around with the checkbox and slider, and click the button to see how the output changes!</p> <p>Questions?</p> <p>If you have any questions or need further assistance, feel free to reach out on GitHub</p>"}]}